{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fc89c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Index\n",
    "1. [Data Management Function](#1.-Data-Management-Function)\n",
    "2. [Read in Datasets and Annotate](#2.-Read-in-Datasets-and-Annotate)\n",
    "3. [Investigating gnomAD Sample Filters](#3.-Investigating-gnomAD-sample-filters)\n",
    "4. [Plotting Results of gnomAD Sample Filter Investigation](#-4.-Plotting-results-of-gnomAD-sample-filter-investigation)\n",
    "5. [Pre-QC Plots](#5.-Pre-QC-Plots)\n",
    "    1. [Number of SNPs](#5a-Number-of-SNPs)\n",
    "    2. [Mean Coverage](#5b-Mean-Coverage)\n",
    "    3. [Freemix](#5c-Freemix)\n",
    "6. [Post-QC Plots](#6.-Post-QC-Plots)\n",
    "    1. [Number of SNPs](#6a-Number-of-SNPs)\n",
    "    2. [Mean Coverage](#6b-Mean-Coverage)\n",
    "    3. [Freemix](#6c-Freemix)\n",
    "    4. [Site Frequency Spectrum](#6d-Site-Frequency-Spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a5426",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# General Overview\n",
    "The purpose of this script is to merge metadata components needed for the HGDP+1kGP dataset and then run  QC filters on that resulting dataset. The metadata included sample and variant information such as geographic region, and which samples/variants passed QC were initially located in different datasets. The QC filters were run using sample/variant flags from the metadata datasets. These flags were generated as a result of the dataset being run through the gnomAD QC pipeline. More information on the gnomAD QC pipeline can be found [here](https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#sample-and-variant-quality-control). To see how these filters were updated as a result of our analyses, see [gnomAD sample filters](#3.-Investigating-gnomAD-sample-filters) and the resulting gnomAD [minor release.](https://gnomad.broadinstitute.org/news/2021-10-gnomad-v3-1-2-minor-release/#improvements-to-the-hgdp--1kg-subset-release)\n",
    "\n",
    "**This script contains information on how to**:\n",
    "- Annotate new fields onto a matrix table from another matrix table or hail table\n",
    "- Unflatten a hail matrix table\n",
    "- Harmonize datasets to prevent merge conflicts\n",
    "- Use plots to identify which gnomAD QC filters are removing populations entirely (fail_n_snp_residual used as an example)\n",
    "- Retrieve populations being unduly removed by filters (mostly AFR and OCE populations)\n",
    "- Filter matrix tables using a field within the matrix table\n",
    "- Filter samples using a hardcoded list of samples to remove\n",
    "- Plot certain fields from the matrix table:\n",
    "    - Number of SNPs\n",
    "    - Coverage\n",
    "    - Site Frequency \n",
    "    - Freemix\n",
    "    - Number of samples which failed a filter\n",
    "\n",
    "**Datasets merged are**:\n",
    "- sample_meta: sample metadata table which contains harmonized metadata for the HGDP_1kGP dataset\n",
    "- sample_qc_meta: gnomad v3.1 sample qc metadata from for the hgdp_1kg subset which contains flags to denote which samples failed gnomAD QC filters\n",
    "- dense_mt: densified hgdp_1kg matrix table with a field of flags (mt.filters) to denote which variants passed or failed gnomAD qc filters\n",
    "\n",
    "Authors: Zan Koenig & Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fd0022",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "\n",
    "# for renaming purposes\n",
    "import re\n",
    "\n",
    "# the import statements below allow for plotting in hail\n",
    "from hail.ggplot import *\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d670566",
   "metadata": {},
   "source": [
    "## Set Requester Pays Bucket\n",
    "Running through these tutorials, users must specify which project is to be billed. To change which project is billed, set the `GCP_PROJECT_NAME` variable to your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting requester pays bucket to use throughout tutorial\n",
    "GCP_PROJECT_NAME = \"diverse-pop-seq-ref\" # change this to your project name\n",
    "hl.init(spark_conf={\n",
    "    'spark.hadoop.fs.gs.requester.pays.mode': 'CUSTOM',\n",
    "    'spark.hadoop.fs.gs.requester.pays.buckets': 'hgdp_tgp,gcp-public-data--gnomad',\n",
    "    'spark.hadoop.fs.gs.requester.pays.project.id': GCP_PROJECT_NAME\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba907f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Data Management Function\n",
    "This function serves the purpose of reading in the dataset of different stages throughout the tutorial and given certain flags, will allow the user to specify which filters they would like run on the dataset. This function helps to reduce the amount of times data needs to be written out, overall decreasing the computational and monetary cost of running the tutorials. \n",
    "\n",
    "<br>\n",
    "<details><summary>Click <u><span style=\"color:blue\">here</span></u> for more information on the function arguments.</summary> \n",
    "    \n",
    "<br> \n",
    "Click on each argument name to learn more!\n",
    "\n",
    "<ul>    \n",
    "<li><details><summary><u>\n",
    "<span style=\"color:blue\">default</span></u></summary>\n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a pre-QC matrix table</p></details></li>\n",
    "\n",
    "<li><details><summary><u>\n",
    "<span style=\"color:blue\">post_qc</span></u></summary> \n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a matrix table which has the following conducted:</p>\n",
    "<ul> \n",
    "    <li>sample_qc filtering</li>\n",
    "    <li>variant_qc filtering</li>\n",
    "    <li>outlier removal</li>\n",
    "    <li>duplicate removal</li>  \n",
    "    </ul></details></li>\n",
    "\n",
    "\n",
    "<li><details><summary><u>  \n",
    "<span style=\"color:blue\">sample_qc</span></u></summary>     \n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a matrix table with gnomad's sampleQC filters run on the dataset. For more information on gnomAD's sample QC steps click <a href=\"https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#sample-qc-hard-filtering\"> here.</a></p></details></li>    \n",
    "    \n",
    "<li><details><summary><u>  \n",
    "<span style=\"color:blue\">variant_qc</span></u></summary>     \n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a matrix table with gnomad's variant quality control filters run on the dataset. For more information on gnomAD's variant QC steps click <a href=\"https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#variant-qc\"> here.</a></p></details></li>        \n",
    "\n",
    "<li><details><summary><u> \n",
    "<span style=\"color:blue\">duplicate</span></u></summary>     \n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a matrix table with any duplicates in the dataset removed. By default there are no duplicates in the dataset, but was included as it is a useful QC step to demonstrate</p></details></li> \n",
    "  \n",
    "<li><details><summary><u>\n",
    "<span style=\"color:blue\">outlier_removal</span></u></summary>     \n",
    "    \n",
    "<p>when <i><b>True</b></i>, will return a matrix table with pca outliers removed. These outliers were determined by running pc_relate. More information on how we created the outlier list can be found <a href=\"https://nbviewer.org/github/atgu/hgdp_tgp/blob/master/tutorials/nb4.ipynb#5.-Outlier-Removal\"> here.</a></p></details></li> \n",
    "    \n",
    "<li><details><summary><u>  \n",
    "<span style=\"color:blue\">ld_pruning</span></u></summary>     \n",
    "   \n",
    "<p>when <i><b>True</b></i>, will return a matrix table which has the following conducted:</p>\n",
    "<ul> \n",
    "    <li>sample_qc filtering</li>\n",
    "    <li>variant_qc filtering</li>\n",
    "    <li>outlier removal</li>\n",
    "    <li>duplicate removal</li>\n",
    "    <li>call rate filter to variants whose call rate is > 0.999</li>\n",
    "    <li>allele frequency filter on variants to only keep variants with 0.05 < AF < 0.95</li>\n",
    "    </ul></details></li>   \n",
    "    \n",
    "<li><details><summary><u>\n",
    "<span style=\"color:blue\">rel_unrel</span></u></summary>  \n",
    "\n",
    "<p>when <i><b>default</b></i>, will return the same matrix table which would be returned when ld_pruning=True</p>\n",
    "\n",
    "<p>when <i><b>related</b></i>, will return a matrix table with only related samples</p>\n",
    "\n",
    "<p>when <i><b>unrelated</b></i> will return matrix table with only unrelated samples</p></details></li></ul>\n",
    "\n",
    "</details> \n",
    "    \n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9aa0ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "\n",
    "def read_qc(\n",
    "        default: bool = False,\n",
    "        post_qc:bool = False,\n",
    "        sample_qc: bool = False,\n",
    "        variant_qc: bool = False,\n",
    "        duplicate: bool = False,\n",
    "        outlier_removal: bool = False,\n",
    "        ld_pruning: bool = False,\n",
    "        rel_unrel: str = 'default',\n",
    "        n_partitions: int = 0) -> hl.MatrixTable:\n",
    "    \"\"\"\n",
    "    Wrapper function to get HGDP+1kGP data as Matrix Table at different stages of QC/filtering.\n",
    "    By default, returns pre QC MatrixTable with qc filters annotated but not filtered.\n",
    "\n",
    "    :param bool default: if True will preQC version of the dataset\n",
    "    :param bool post_qc: if True will return a post QC matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - outlier removal\n",
    "    :param bool sample_qc: if True will return a post sample QC matrix table\n",
    "    :param bool variant_qc: if True will return a post variant QC matrix table\n",
    "    :param bool duplicate: if True will return a matrix table with duplicate samples removed\n",
    "    :param bool outlier_removal: if True will return a matrix table with PCA outliers and duplicate samples removed\n",
    "    :param bool ld_pruning: if True will return a matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - LD pruning\n",
    "        - additional variant filtering\n",
    "    :param bool rel_unrel: default will return same mt as ld pruned above\n",
    "        if 'all' will return the same matrix table as if ld_pruning is True\n",
    "        if 'related_pre_outlier' will return a matrix table with only related samples pre pca outlier removal\n",
    "        if 'unrelated_pre_outlier' will return a matrix table with only unrelated samples pre pca outlier removal\n",
    "        if 'related_post_outlier' will return a matrix table with only related samples post pca outlier removal\n",
    "        if 'unrelated_post_outlier' wil return a matrix table with only unrelated samples post pca outlier removal\n",
    "    :param int n_partitions: if specified, will read in dataset with given number of partitions for the following arguments:\n",
    "        - ld_pruning\n",
    "        - rel_unrel\n",
    "    \"\"\"\n",
    "    # Reading in all the tables and matrix tables needed to generate the pre_qc matrix table\n",
    "    sample_meta = hl.import_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv')\n",
    "    sample_qc_meta = hl.read_table('gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht')\n",
    "    dense_mt = hl.read_matrix_table(\n",
    "        'gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_dense.mt')\n",
    "    \n",
    "    dense_mt = dense_mt.naive_coalesce(5000)\n",
    "\n",
    "\n",
    "    # Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "    def dict_to_struct(d):\n",
    "        fields = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                v = dict_to_struct(v)\n",
    "            fields[k] = v\n",
    "        return hl.struct(**fields)\n",
    "\n",
    "    # un-flattening a hail table with nested structure\n",
    "    # dict to hold struct names as well as nested field names\n",
    "    d = {}\n",
    "\n",
    "    # Getting the row field names\n",
    "    row = sample_meta.row_value\n",
    "\n",
    "    # returns a dict with the struct names as keys and their inner field names as values\n",
    "    for name in row:\n",
    "        def recur(dict_ref, split_name):\n",
    "            if len(split_name) == 1:\n",
    "                dict_ref[split_name[0]] = row[name]\n",
    "                return\n",
    "            existing = dict_ref.get(split_name[0])\n",
    "            if existing is not None:\n",
    "                assert isinstance(existing, dict), existing\n",
    "                recur(existing, split_name[1:])\n",
    "            else:\n",
    "                existing = {}\n",
    "                dict_ref[split_name[0]] = existing\n",
    "                recur(existing, split_name[1:])\n",
    "        recur(d, name.split('.'))\n",
    "\n",
    "    # using the dict created from flattened struct, creating new structs now un-flattened\n",
    "    sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "    sample_meta = sample_meta.key_by('s')\n",
    "\n",
    "    # grabbing the columns needed from HGDP metadata\n",
    "    new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "    # creating a table with gnomAD sample metadata and HGDP metadata\n",
    "    ht = sample_qc_meta.annotate(**new_meta[sample_qc_meta.s])\n",
    "\n",
    "    # stripping 'v3.1::' from the names to match with the densified MT\n",
    "    ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))\n",
    "\n",
    "    # Using hl.annotate_cols() method to annotate the gnomAD variant QC metadata onto the matrix table\n",
    "    mt = dense_mt.annotate_cols(**ht[dense_mt.s])\n",
    "\n",
    "    print(f\"sample_qc: {sample_qc}\\nvariant_qc: {variant_qc}\\nduplicate: {duplicate}\" \\\n",
    "          f\"\\noutlier_removal: { outlier_removal}\\nld_pruning: {ld_pruning}\\nrel_unrel: {rel_unrel}\")\n",
    "    \n",
    "    if default:\n",
    "        print(\"Returning default preQC matrix table\")\n",
    "        # returns preQC dataset\n",
    "        return mt\n",
    "    \n",
    "    if post_qc:\n",
    "        print(\"Returning post sample and variant QC matrix table with duplicates and PCA outliers removed\")\n",
    "        sample_qc = True\n",
    "        variant_qc = True\n",
    "        duplicate = True\n",
    "        outlier_removal = True\n",
    "    \n",
    "    if sample_qc:\n",
    "        print(\"Running sample QC\")\n",
    "        # run data through sample QC\n",
    "        # filtering samples to those who should pass gnomADs sample QC\n",
    "        # this filters to only samples that passed gnomad sample QC hard filters\n",
    "        mt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "    if variant_qc:\n",
    "        print(\"Running variant QC\")\n",
    "        # run data through variant QC\n",
    "        # Subsetting the variants in the dataset to only PASS variants (those which passed gnomAD's variant QC)\n",
    "        # PASS variants are variants which have an entry in the filters field.\n",
    "        # This field contains an array which contains a bool if any variant qc filter was failed\n",
    "        # This is the last step in the QC process\n",
    "        mt = mt.filter_rows(hl.len(mt.filters) != 0, keep=False)\n",
    "\n",
    "    if duplicate:\n",
    "        print(\"Removing any duplicate samples\")\n",
    "        # Removing any duplicates in the dataset using hl.distinct_by_col() which removes\n",
    "        # columns with a duplicate column key. It keeps one column for each unique key.\n",
    "        # after updating to the new dense_mt, this step is no longer necessary to run\n",
    "        mt = mt.distinct_by_col()\n",
    "\n",
    "    if outlier_removal:\n",
    "        print(\"Removing PCA outliers\")\n",
    "        # remove PCA outliers and duplicates\n",
    "        # reading in the PCA outlier list\n",
    "        # To read in the PCA outlier list, first need to read the file in as a list\n",
    "        # using hl.hadoop_open here which allows one to read in files into hail from Google cloud storage\n",
    "        pca_outlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "        with hl.utils.hadoop_open(pca_outlier_path) as file:\n",
    "            outliers = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "        # Using hl.literal here to convert the list from a python object to a hail expression so that it can be used\n",
    "        # to filter out samples\n",
    "        outliers_list = hl.literal(outliers)\n",
    "\n",
    "        # Using the list of PCA outliers, using the ~ operator which is a negation operator and obtains the compliment\n",
    "        # In this case the compliment is samples which are not contained in the pca outlier list\n",
    "        mt = mt.filter_cols(~outliers_list.contains(mt['s']))\n",
    "\n",
    "    if ld_pruning:\n",
    "        print(\"Returning ld pruned post variant and sample QC matrix table pre PCA outlier removal \")\n",
    "        # read in dataset which has additional variant filtering and ld pruning run\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        if n_partitions != 0:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt',\n",
    "            _n_partitions = n_partitions)\n",
    "        else:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt')\n",
    "         \n",
    "\n",
    "    if rel_unrel == \"default\":\n",
    "        # do nothing\n",
    "        # created a default value because there are multiple options for rel/unrel datasets\n",
    "        mt = mt\n",
    "\n",
    "    elif rel_unrel == 'related_pre_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate \n",
    "        #   - filter to only related individuals   \n",
    "        if n_partitions != 0:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/rel_updated.mt',\n",
    "            _n_partitions = n_partitions)\n",
    "        else:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/rel_updated.mt')\n",
    "        \n",
    "    elif rel_unrel == 'unrelated_pre_outlier':\n",
    "        print(\"Returning post QC matrix table with only unrelated individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate \n",
    "        #   - filter to only unrelated individuals\n",
    "        if n_partitions != 0:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt',\n",
    "            _n_partitions = n_partitions)\n",
    "        else:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt')\n",
    "\n",
    "    elif rel_unrel == 'related_post_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate \n",
    "        #   - filter to only related individuals\n",
    "        #   - PCA outlier removal\n",
    "        if n_partitions != 0:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt',\n",
    "            _n_partitions = n_partitions)\n",
    "        else:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt')\n",
    "\n",
    "    elif rel_unrel == 'unrelated_pst_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate \n",
    "        #   - filter to only unrelated individuals\n",
    "        #   - PCA outlier removal\n",
    "        if n_partitions != 0:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt',\n",
    "            _n_partitions = n_partitions)\n",
    "        else:\n",
    "            mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt')\n",
    "        \n",
    "    # Calculating both variant and sample_qc metrics on the mt before returning\n",
    "    # so the stats are up to date with the version being written out\n",
    "    mt = hl.sample_qc(mt)\n",
    "    mt = hl.variant_qc(mt)\n",
    "    \n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c2534",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Read in Datasets and Annotate\n",
    "<br>\n",
    "<details><summary>Click <u><span style=\"color:blue\">here</span></u> for more information about the input dataset.</summary>\n",
    "\n",
    "This input matrix table is a combination of 3 datasets: a harmonized sample metadata for the HGDP+1KG dataset, a >gnomAD v3.1 sample qc metadata with samples that failed gnomAD QC filters flagged, and a densified HGDP+1KG matrix table.\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/impex.html#hail.methods.read_matrix_table\"> More on  <i> read_matrix_table() </i></a></li>\n",
    "    \n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.count\"> More on  <i> count() </i></a></li>\n",
    "    \n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/impex.html#hail.methods.read_table\"> More on  <i> read_table() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.annotate_rows\"> More on  <i> annotate_rows() </i></a></li>\n",
    "    \n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.describe\"> More on  <i> describe() </i></a></li>\n",
    "    </ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf08e53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read-in the matrix table (shortened as mt)\n",
    "mt = read_qc(default=True)\n",
    "\n",
    "# how many snps and samples are there? counts \n",
    "print('Num of snps and samples prior to any analysis = ' + str(mt.count())) # 211358784 snps & 4151 samples \n",
    "\n",
    "# explore combined mt \n",
    "mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea10d07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Investigating gnomAD sample filters\n",
    "\n",
    "<br>\n",
    "<details><summary>Click <u><span style=\"color:blue\">here</span></u> to learn why we are doing this.</summary>\n",
    "    \n",
    "9 out of the 28 gnomAD sample filters were dropping huge numbers of ancestrally diverse individuals (mostly African > (AFR) and Oceanian (OCE) populations). The filters use gnomAD’s principal component analysis (PCA) which is obtained from other samples to residualize the distribution of values from different populations and identify outliers. If there is an error and outliers are identified, the sample fails the filter. \n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.filter_cols\"> More on  <i> filter_cols() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.expr.SetExpression.html#hail.expr.SetExpression.difference\"> More on  <i> difference() </i></a></li>\n",
    "\n",
    "<li><a href=\" https://hail.is/docs/0.2/hail.expr.CollectionExpression.html#hail.expr.CollectionExpression.length\"> More on  <i> length() </i></a></li>\n",
    "    </ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a856b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# put the gnomAD qc filters in a set \n",
    "all_sample_filters = set(mt['sample_filters']) \n",
    "\n",
    "# select out the filters that are removing whole populations despite them passing all other gnomAD filters\n",
    "# if a filter name starts with 'fail_', add it to a new set after removing 'fail_' from the name  \n",
    "bad_sample_filters = {re.sub('fail_', '', x) for x in all_sample_filters if x.startswith('fail_')} \n",
    "\n",
    "# filter out the samples that passed all gnomad QC filters OR only failed the filters that were removing population wholly\n",
    "mt_filt = mt.filter_cols(mt['sample_filters']['qc_metrics_filters'].difference(bad_sample_filters).length() == 0)\n",
    "\n",
    "# how many samples were removed by the initial QC?\n",
    "print('Num of samples before initial QC = ' + str(mt.count()[1])) # 4151\n",
    "print('Num of samples after initial QC = ' + str(mt_filt.count()[1])) # 4120\n",
    "print('Samples removed = ' + str(mt.count()[1] - mt_filt.count()[1])) # 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f6358",
   "metadata": {},
   "source": [
    "# 4. Plotting results of gnomAD sample filter investigation\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a72248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "filepath = \"gs://hgdp-1kg/hgdp_tgp/intermediate_files/failed_filters_population_level.csv\"\n",
    "filters = hl.import_table(filepath, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23dea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab only \"sample_filters.fail_n_snp_residual\"\n",
    "n_snp_resid = filters.annotate(population = filters['\"population\"'][1:hl.len(filters['\"population\"'])-1], \\\n",
    "                                   num_samples = hl.int(filters['\"num_of_samples\"']), \\\n",
    "                                   fail_n_snp_resid = hl.int(filters['\"sample_filters.fail_n_snp_residual\"']),\\\n",
    "                                 fail_gnomAD = hl.str(filters['\"failed_gnomAD\"']))\n",
    "\n",
    "#manipulate all strings to remove the extraneous quotation marks\n",
    "n_snp_resid = n_snp_resid.select(\"population\", \"num_samples\", \"fail_n_snp_resid\", \"fail_gnomAD\")\n",
    "\n",
    "# calculate the ratio between the number of samples that failed and the total number of samples in the population. \n",
    "n_snp_resid = n_snp_resid.annotate(fail_ratio = n_snp_resid.fail_n_snp_resid/n_snp_resid.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_snp_resid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate scatter plots of ratios for each filter column across all populations colored by gnomAD failure \n",
    "plot_n_snp_resid = hl.ggplot.ggplot(n_snp_resid, hl.ggplot.aes(x=n_snp_resid.population, y=n_snp_resid.fail_ratio, \\\n",
    "                                                color=n_snp_resid.fail_gnomAD)) + \\\n",
    "    hl.ggplot.geom_point() +\\\n",
    "    hl.ggplot.ylab(\"Ratio of failed samples/total samples\") + \\\n",
    "    hl.ggplot.ggtitle(\"Failure of gnomAD n_snp_resids filter by population\")+\\\n",
    "    hl.ggplot.scale_x_discrete(breaks=list(range(78)))\n",
    "\n",
    "plot_n_snp_resid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c994",
   "metadata": {},
   "source": [
    "# 5. Pre-QC Plots\n",
    "When conducting quality control, it is often a good idea to create plots of things such as the number of SNPS and coverage, so that after removing samples or variants you get a visual representation of changes in the dataset and can potentially see if anything requires further investigation. \n",
    "\n",
    "The following plots show the dataset prior to running any sample QC filters.\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c917af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict that maps color for plotting to region name for both pre and post QC plots\n",
    "newnames = {'AMR':\"#E41A1C\",'AFR':\"#984EA3\", 'OCE':\"#999999\", 'CSA':\"#FF7F00\", \n",
    "            'EAS':\"#4DAF4A\", 'EUR':\"#377EB8\", 'MID':\"#A65628\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08144a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using func to get pre_qc version of dataset\n",
    "pre_qc = read_qc(default=True)\n",
    "# As of hail v. 0.2.82, ggplot only takes in tables as input\n",
    "# Making a table of samples for plotting\n",
    "pre_qc_col = pre_qc.cols()\n",
    "pre_qc_row = pre_qc.rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40c388",
   "metadata": {},
   "source": [
    "#### 5a. Number of SNPs - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed623c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram of number of SNPS for each individual within each global region\n",
    "p = hl.ggplot.ggplot(pre_qc_col, hl.ggplot.aes(x = pre_qc_col.sample_qc.n_snp)) + \\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill = pre_qc_col.hgdp_tgp_meta.Genetic.region), min_val = 5000000, \n",
    "                             max_val = 7500000, bins = 200, position=\"identity\", alpha = .7) + \\\n",
    "    hl.ggplot.xlab(\"Number of SNPs\")+ \\\n",
    "    hl.ggplot.ggtitle(\"Number of SNPs, Pre-QC\")+ \\\n",
    "    hl.ggplot.coord_cartesian(ylim = (0,260))\n",
    "\n",
    "\n",
    "# Update colors\n",
    "p = p.to_plotly()\n",
    "\n",
    "p.for_each_trace(\n",
    "    lambda trace: trace.update(marker=dict(color = newnames[trace.name]))\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0e9ae",
   "metadata": {},
   "source": [
    "#### 5b. Mean Coverage - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot of mean coverage per individual\n",
    "p = hl.ggplot.ggplot(pre_qc_col, hl.ggplot.aes(x = pre_qc_col.bam_metrics.mean_coverage)) + \\\n",
    "    hl.ggplot.geom_density(hl.ggplot.aes(fill=pre_qc_col.project_meta.title),\n",
    "                             alpha = .7) + \\\n",
    "    hl.ggplot.xlab(\"Coverage (x)\")+ \\\n",
    "    hl.ggplot.ggtitle(\"Mean coverage, Pre-QC\")\n",
    "\n",
    "\n",
    "# Show plot\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8789af",
   "metadata": {},
   "source": [
    "#### 5c. Freemix - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aad088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting freemix colored by population \n",
    "freemix_pre_qc_pop = hl.ggplot.ggplot(pre_qc_col, hl.ggplot.aes(x = pre_qc_col.bam_metrics.freemix)) +\\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill=pre_qc_col.hgdp_tgp_meta.Genetic.region), bins = 140) + \\\n",
    "    hl.ggplot.scale_y_log10(\"Count (log scale)\") +\\\n",
    "    hl.ggplot.xlab(\"Freemix\") + \\\n",
    "    hl.ggplot.ggtitle(\"Bam metrics: Freemix, Pre-QC\")+ \\\n",
    "    hl.ggplot.coord_cartesian(xlim = (0,.5))\n",
    "\n",
    "# #update legends\n",
    "# #update colors\n",
    "freemix_pre_qc_pop = freemix_pre_qc_pop.to_plotly()\n",
    "freemix_pre_qc_pop.for_each_trace(lambda trace: trace.update(marker=dict(color = newnames[trace.name])))\n",
    "\n",
    "#show plot\n",
    "freemix_pre_qc_pop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89562794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting freemix colored by project\n",
    "freemix_pre_qc_proj = hl.ggplot.ggplot(pre_qc_col, hl.ggplot.aes(x = pre_qc_col.bam_metrics.freemix)) +\\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill=pre_qc_col.project_meta.title), position=\"identity\", bins = 140,\\\n",
    "                            alpha = .5) + \\\n",
    "    hl.ggplot.scale_y_log10(\"Count (log scale)\") +\\\n",
    "    hl.ggplot.xlab(\"Freemix\") + \\\n",
    "    hl.ggplot.ggtitle(\"Bam metrics: Freemix, Pre-QC\")+ \\\n",
    "    hl.ggplot.coord_cartesian(xlim = (0,.5))\n",
    "\n",
    "#show plot\n",
    "freemix_pre_qc_proj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b2ee4",
   "metadata": {},
   "source": [
    "# 6. Post-QC Plots\n",
    "\n",
    "The following plots are the same as those made above with pre-qc data except now the dataset has gone through:\n",
    "- sample filtering\n",
    "- variant filtering\n",
    "- duplicate removal\n",
    "- PCA outlier removal\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in postQC matrix table\n",
    "post_qc = read_qc(post_qc=True)\n",
    "\n",
    "# As of hail v. 0.2.82, ggplot only takes in tables as input\n",
    "# Making a table of samples for plotting\n",
    "post_qc_col = post_qc.cols()\n",
    "post_qc_row = post_qc.rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cfc02",
   "metadata": {},
   "source": [
    "#### 6a. Number of SNPs - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bad534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ggplot, differentiate between populations\n",
    "# Used to do fill by geographic region\n",
    "n_snp_post_qc = hl.ggplot.ggplot(post_qc_col, hl.ggplot.aes(x = post_qc_col.sample_qc.n_snp)) + \\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill=post_qc_col.hgdp_tgp_meta.Genetic.region), min_val = 5000000, \n",
    "                             max_val = 7500000, bins = 200, position=\"identity\", alpha = .7) + \\\n",
    "    hl.ggplot.xlab(\"Number of SNPs\")+ \\\n",
    "    hl.ggplot.ggtitle(\"Number of SNPs, Post-QC\") + \\\n",
    "    hl.ggplot.coord_cartesian(ylim = (0,260)) \n",
    "\n",
    "\n",
    "# Update legends\n",
    "n_snp_post_qc = n_snp_post_qc.to_plotly()\n",
    "\n",
    "n_snp_post_qc.for_each_trace(\n",
    "    lambda trace: trace.update(marker=dict(color = newnames[trace.name]))\n",
    ")\n",
    "\n",
    "#show plot\n",
    "n_snp_post_qc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb93004",
   "metadata": {},
   "source": [
    "#### 6b. Mean Coverage - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d91ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of mean coverage from bam_metrics\n",
    "# Separate by project (HGDP or 1kGP)\n",
    "cov_post_qc = hl.ggplot.ggplot(post_qc_col, hl.ggplot.aes(x = post_qc_col.bam_metrics.mean_coverage)) + \\\n",
    "    hl.ggplot.geom_density(hl.ggplot.aes(fill=post_qc_col.project_meta.title),\n",
    "                             alpha = .7) + \\\n",
    "    hl.ggplot.xlab(\"Coverage (x)\")+ \\\n",
    "    hl.ggplot.ggtitle(\"Mean coverage, Post-QC\")\n",
    "\n",
    "cov_post_qc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4567800",
   "metadata": {},
   "source": [
    "#### 6c. Freemix - \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e784b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freemix_post_qc_pop = hl.ggplot.ggplot(post_qc_col, hl.ggplot.aes(x = post_qc_col.bam_metrics.freemix)) +\\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill=post_qc_col.hgdp_tgp_meta.Genetic.region), bins = 70,\\\n",
    "                            alpha = 1) + \\\n",
    "    hl.ggplot.scale_y_log10(\"Count (log scale)\") +\\\n",
    "    hl.ggplot.xlab(\"Freemix\") + \\\n",
    "    hl.ggplot.ggtitle(\"Bam metrics: Freemix, Post-QC\")+ \\\n",
    "    hl.ggplot.coord_cartesian(xlim = (0,.5))\n",
    "\n",
    "#update legends\n",
    "freemix_post_qc_pop = freemix_post_qc_pop.to_plotly()\n",
    "freemix_post_qc_pop.for_each_trace(lambda trace: trace.update(marker=dict(color = newnames[trace.name])))\n",
    "\n",
    "#show plot\n",
    "freemix_post_qc_pop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freemix_post_qc_proj = hl.ggplot.ggplot(post_qc_col, hl.ggplot.aes(x = post_qc_col.bam_metrics.freemix)) +\\\n",
    "    hl.ggplot.geom_histogram(hl.ggplot.aes(fill=post_qc_col.project_meta.title), position=\"identity\", bins = 70,\\\n",
    "                            alpha = .5) + \\\n",
    "    hl.ggplot.scale_y_log10(\"Count (log scale)\") +\\\n",
    "    hl.ggplot.xlab(\"Freemix\") + \\\n",
    "    hl.ggplot.ggtitle(\"Bam metrics: Freemix, Post-QC\")+ \\\n",
    "    hl.ggplot.coord_cartesian(xlim = (0,.5))\n",
    "\n",
    "#show plot\n",
    "freemix_post_qc_proj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d299b1",
   "metadata": {},
   "source": [
    "#### 6d. Site Frequency Spectrum -\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f646f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregating site frequency data for plotting\n",
    "# # Writing out an intermediate file to cut down on plotting time\n",
    "# # This section is commented out since users will only need to read in the new dataset\n",
    "# sfs_data = ht_rows.aggregate(hl.agg.hist(post_qc.freq.AF[1], 0,1,250))\n",
    "# with hl.hadoop_open('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/sfs_pre_qc.txt', 'w') as f:\n",
    "#     f.write(str(dict(sfs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14729418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "sfs_post_qc = hl.hadoop_open('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/sfs_post_qc.txt')\n",
    "sfs_dict = eval(sfs_post_qc.read())\n",
    "sfs_struct = hl.Struct(**sfs_dict)\n",
    "\n",
    "# Plot site frequency spectrum histogram\n",
    "sfs_p = hl.plot.histogram(sfs_struct, log = True, legend = \"Frequency of major allele at site\")\n",
    "show(sfs_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot this in ggplot as well, and I have included the code below. \n",
    "# But, there currently does not exist a way to directly use pre-aggregated data (which takes ~30 mins to compile)\n",
    "# Plotting this way takes the same amount of time and resources as running the cell above.\n",
    "p = hl.ggplot.ggplot(pre_qc_row, hl.ggplot.aes(x = pre_qc_row.freq.AF[1])) + \\\n",
    "    hl.ggplot.geom_histogram(bins = 200, position=\"identity\", alpha = .7) + \\\n",
    "    hl.ggplot.xlab(\"Allele frequency\")+ \\\n",
    "    hl.ggplot.ggtitle(\"Site Frequency Spectrum\") + \\\n",
    "    hl.ggplot.scale_y_log10(\"Number of loci (log scale)\")\n",
    "    \n",
    "p.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

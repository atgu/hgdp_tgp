{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Notebook 1: Merging and annotating tables and matrix tables, Sample QC, PCA outlier removal, Variant QC\n",
    "\n",
    "To do:\n",
    "1. add in plotting code generated by Ally & make sure it runs\n",
    "2. add more detailed descriptions of why each step is run\n",
    "3. remove certain table/mt write-outs\n",
    "4. potentially (need to discuss with Mary first) write a function to read data\n",
    "5. create more detailed index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "great-jenny",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Functions](#Functions)\n",
    "- [Reading in datasets](#Reading-in-datasets)\n",
    "- [Combining metadata by merging hail tables and matrix tables](#Combining-metadata)\n",
    "- [Annotating metadata onto matrix table](#Annotating-merged-metadata-onto-matrix-table)\n",
    "- [Sample QC filtering](#Sample-QC-filtering)\n",
    "- [PCA outlier removal](#PCA-outlier-removal)\n",
    "- [Variant QC filtering](#Variant-QC-filtering)\n",
    "- [Exporting datasets post QC](#Exporting-final-dataset-post-QC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-football",
   "metadata": {},
   "source": [
    "# General Overview\n",
    "The purpose of this script is to merge metadata components needed for the HGDP+1kGP dataset and then run  QC filters on that resulting dataset. The metadata included sample and variant information such as geographic region, and which samples/variants passed QC were initially located in different datasets. The QC filters were run using sample/variant flags from the metadata datasets. These flags were generated as a result of the dataset being run through the gnomAD QC pipeline.\n",
    "\n",
    "**This script contains information on how to**: \n",
    "- annotate new fields onto a matrix table from another matrix table or hail table\n",
    "- unflatten a hail matrix table\n",
    "- harmonize datasets to prevent merge conflicts\n",
    "- filter matrix tables using a field within the matrix table\n",
    "- filter samples using a hardcoded list of samples to remove\n",
    "- write out a matrix table in vcf format\n",
    "\n",
    "**Datasets merged are**: \n",
    "- sample_meta: sample metadata table which contains harmonized metadata for the HGDP_1kGP dataset\n",
    "- sample_qc_meta: gnomad v3.1 sample qc metadata from for the hgdp_1kg subset which contains flags to denote which samples failed gnomAD QC filters\n",
    "- var_meta: hail matrix table with a field of flags (mt.filters) to denote which variants passed or failed gnomAD qc filters\n",
    "- dense_mt: densified hgdp_1kg matrix table\n",
    "    \n",
    "**Author: Zan Koenig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apart-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.5\n",
      "SparkUI available at http://znk-plink-m.c.diverse-pop-seq-ref.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.64-1ef70187dc78\n",
      "LOGGING: writing to /home/hail/hail-20211202-1706-0.2.64-1ef70187dc78.log\n"
     ]
    }
   ],
   "source": [
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='Functions'></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "finite-dispatch",
   "metadata": {},
   "source": [
    "# Data reformatting and printing functions\n",
    "For interactive analyses scripts such as this, defining functions at the top of the script allow for ease of use. There are other ways to organize the definition of functions in python and the best method depends on the intended usage of the script as well as the writers personal preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "def dict_to_struct(d):\n",
    "    fields = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = dict_to_struct(v)\n",
    "        fields[k] = v\n",
    "    return hl.struct(**fields)\n",
    "\n",
    "# Formats the output of using hl.count in a more user-friendly format\n",
    "def print_count(mt):\n",
    "    '''\n",
    "    Prints out total sample/variant count for a mt\n",
    "    :param mt: hail matrix table\n",
    "    :return: print statement with number of samples and variants\n",
    "    '''\n",
    "    # Since hl.count() is being used on a matrix table, the result has two numbers in output\n",
    "    # When using hl.count() on a matrix table the first number is the number of rows, equivalent to the number of variants\n",
    "    # The second number is the number of columns, equivalent to the number of samples\n",
    "    n = mt.count()\n",
    "    print('Number of Samples: {}\\nNumber of Variants: {}'.format(n[1], n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='Reading in datasets'></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# User specified output paths\n",
    "Below are variables for which the user can specify their desired output paths ***May be replaced with a read dataset function to allow user specified inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here the user needs to specify the output path they want all the output datasets to be written to.\n",
    "# There are default names which the data will be written out with\n",
    "# users can alter the default names by changing the variables below.\n",
    "output_path = input(\"Please input a path for datasets to be written out to: \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This will be a table of the merged metadata information needed to run QC filters.\n",
    "# It is being written out before annotating onto the main matrix table since the steps taken during merging are computationally expensive.\n",
    "# If you are going to run this tutorial multiple times, having this dataset output will save time.\n",
    "metadata_table = 'hdgp_1kgp_tutorial_metadata'\n",
    "# This is a pre-qc version of the hgdp+1kGP dataset. It contains all the metadata necessary to conduct qc but does not have any samples or variants removed yet\n",
    "# Hail's hl.sample_qc() and hl.variant_qc() methods have been run on the dataset prior to outputting so the metrics in those fields are based off of the pre-qc sample/variant counts\n",
    "pre_qc_dataset = 'hgdp_1kgp_tutorial_pre_qc'\n",
    "# This is a version of the dataset which has had sample QC filters run on it. It is being written out because some filtering steps which take place prior take some time to run.\n",
    "# Writing it out after those steps ensure that downstream the computational time does not take as long\n",
    "post_sample_qc = 'hdgp_1kgp_tutorial_post_sample_qc'\n",
    "# This is the final, post_qc version of the dataset. It will have all the sample and variant QC filters run on it as well as having PCA outliers removed\n",
    "post_qc = 'hgdp_1kgp_tutorial_post_qc'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "sought-plane",
   "metadata": {},
   "source": [
    "# Reading in datasets\n",
    "Setting separate variables for paths before the datasets are read in makes it easier to update paths if datasets move in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "skilled-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for sample metadata file which contains metadata for the HDGP dataset\n",
    "sample_meta_path = 'gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv'\n",
    "\n",
    "# path for hail table which contains information on which samples passed gnomAD QC filters\n",
    "sample_qc_meta_path = 'gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht'\n",
    "\n",
    "# path for hail table which contains info on which variants passed/failed gnomAD QC\n",
    "var_metadata_path = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'\n",
    "\n",
    "# path for densfied pre-qc matrix table which was generated from the original pre-qc sparse version of the hgdp_1kgp dataset\n",
    "dense_mt_path = 'gs://hgdp_tgp/output/tgp_hgdp.mt'\n",
    "\n",
    "# Path for a txt file which contains the latest list of PCA outliers\n",
    "pca_outlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:02:28 Hail: INFO: Reading table to impute column types\n",
      "2021-11-02 17:02:35 Hail: INFO: Loading 184 fields. Counts by type:\n",
      "  str: 80\n",
      "  bool: 44\n",
      "  float64: 40\n",
      "  int32: 20\n"
     ]
    }
   ],
   "source": [
    "# reading in Alicia's sample metadata file (Note: this file uses the 'v3.1::' prefix as done in gnomAD)\n",
    "sample_meta = hl.import_table(sample_meta_path, impute=True)\n",
    "\n",
    "# reading in Julia's sample metadata file\n",
    "sample_qc_meta = hl.read_table(sample_qc_meta_path)\n",
    "\n",
    "# reading in variant qc information\n",
    "var_meta = hl.read_table(var_metadata_path)\n",
    "\n",
    "# reading in densified pre-qc matrix table\n",
    "dense_mt = hl.read_matrix_table(dense_mt_path)\n",
    "\n",
    "# To read in the PCA outlier list, first need to read the file in as a list\n",
    "# using hl.hadoop_open here which allows one to read in files into hail from Google cloud storage\n",
    "with hl.utils.hadoop_open(pca_outlier_path) as file:\n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# Using hl.literal here to convert the list from a python object to a hail expression so that it can be used to filter out samples\n",
    "outliers_list = hl.literal(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-output",
   "metadata": {},
   "source": [
    "# Combining metadata\n",
    "The HGDP_1kPG dataset is dense_mt but in the state in which it was read in, it does not contain all the information about the dataset that we need for filtering purposes. This includes information from sample_meta which contains information on which geographic region and which population each sample is from. The sample_qc_meta dataset contains information on which samples and variants failed qc filters.\n",
    "Before conducting QC, the different metadata datasets must be merged together. The first cell below is an example of having to alter the structure of a dataset before being able to merge with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bits below were written by Tim Poterba to help troubleshoot unflattening a ht with nested structure\n",
    "# dict to hold struct names as well as nested field names\n",
    "d = {}\n",
    "\n",
    "# Getting just the row field names \n",
    "row = sample_meta.row_value\n",
    "\n",
    "# returns a dict with the struct names as keys and their inner field names as values\n",
    "for name in row:\n",
    "    def recur(dict_ref, split_name):\n",
    "        if len(split_name) == 1:\n",
    "            dict_ref[split_name[0]] = row[name]\n",
    "            return\n",
    "        existing = dict_ref.get(split_name[0])\n",
    "        if existing is not None:\n",
    "            assert isinstance(existing, dict), existing  # fails on foo.bar and foo.bar.baz\n",
    "            recur(existing, split_name[1:])\n",
    "        else:\n",
    "            existing = {}\n",
    "            dict_ref[split_name[0]] = existing\n",
    "            recur(existing, split_name[1:])\n",
    "    recur(d, name.split('.'))\n",
    "\n",
    "\n",
    "# using the dict created from flattened struct, creating new structs now unflattened\n",
    "sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "sample_meta = sample_meta.key_by('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "formed-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the columns needed from Alicia's metadata\n",
    "new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "# creating a table with Julia's metadata and Alicia's metadata\n",
    "ht = sample_qc_meta.annotate(**new_meta[sample_qc_meta.s])\n",
    "\n",
    "# stripping 'v3.1::' from the names to match with Konrad's MT\n",
    "ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 18:31:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:31:55 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:32:25 Hail: INFO: wrote table with 4150 rows in 155 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_sample_metadata.ht\n",
      "    Total size: 1.69 MiB\n",
      "    * Rows: 1.68 MiB\n",
      "    * Globals: 6.51 KiB\n",
      "    * Smallest partition: 1 rows (758.00 B)\n",
      "    * Largest partition:  173 rows (68.18 KiB)\n"
     ]
    }
   ],
   "source": [
    "# When writing out any dataset, you want to make sure the path is as intended and the resulting name is descriptive\n",
    "# hl.write() takes the entire output path as an argument as well as the name of the resulting table or matrix table\n",
    "ht.write('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-division",
   "metadata": {},
   "source": [
    "# Annotating merged metadata onto matrix table\n",
    "Now that the two metadata datasets are merged together and in the proper format, the next step is to annotate the dense matrix table with all of the samples and variants preQC with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT\n",
    "# reading in table annotated with Alicia and Julia's respective metadata\n",
    "ht = hl.read_table('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "center-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hl.count() returns the counts of samples and variants within a matrix table or table.\n",
    "# In this case since it is a hail table, it only returns the count of the number of samples\n",
    "# The number of samples is equal to the number of rows\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "million-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'sex_imputation_ploidy_cutoffs': struct {\n",
      "        x_ploidy_cutoffs: struct {\n",
      "            upper_cutoff_X: float64, \n",
      "            lower_cutoff_XX: float64, \n",
      "            upper_cutoff_XX: float64, \n",
      "            lower_cutoff_XXX: float64\n",
      "        }, \n",
      "        y_ploidy_cutoffs: struct {\n",
      "            lower_cutoff_Y: float64, \n",
      "            upper_cutoff_Y: float64, \n",
      "            lower_cutoff_YY: float64\n",
      "        }, \n",
      "        f_stat_cutoff: float64\n",
      "    } \n",
      "    'population_inference_pca_metrics': struct {\n",
      "        min_prob: float64, \n",
      "        include_unreleasable_samples: bool, \n",
      "        max_mislabeled_training_samples: int32, \n",
      "        known_pop_removal_iterations: int32, \n",
      "        n_pcs: int32\n",
      "    } \n",
      "    'relatedness_inference_cutoffs': struct {\n",
      "        min_individual_maf: float64, \n",
      "        min_emission_kinship: float64, \n",
      "        ibd0_0_max: float64, \n",
      "        second_degree_kin_cutoff: float64, \n",
      "        first_degree_kin_thresholds: tuple (\n",
      "            float64, \n",
      "            float64\n",
      "        )\n",
      "    } \n",
      "    'outlier_detection_metrics': struct {\n",
      "        lms: struct {\n",
      "            n_snp: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_singleton: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_ti_tv: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_insertion_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_insertion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_het_hom_var: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transition: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transversion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }\n",
      "        }, \n",
      "        qc_metrics_stats: struct {\n",
      "            n_snp_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_singleton_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_ti_tv_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_insertion_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_insertion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_het_hom_var_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transition_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transversion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }\n",
      "        }, \n",
      "        n_pcs: int32, \n",
      "        used_regressed_metrics: bool\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    'project_meta': struct {\n",
      "        sample_id: str, \n",
      "        research_project_key: str, \n",
      "        seq_project: str, \n",
      "        ccdg_alternate_sample_id: str, \n",
      "        ccdg_gender: str, \n",
      "        ccdg_center: str, \n",
      "        ccdg_study: str, \n",
      "        cram_path: str, \n",
      "        project_id: str, \n",
      "        v2_age: float64, \n",
      "        v2_sex: str, \n",
      "        v2_hard_filters: str, \n",
      "        v2_perm_filters: str, \n",
      "        v2_pop_platform_filters: str, \n",
      "        v2_related: bool, \n",
      "        v2_data_type: str, \n",
      "        v2_product: str, \n",
      "        v2_product_simplified: str, \n",
      "        v2_qc_platform: str, \n",
      "        v2_project_id: str, \n",
      "        v2_project_description: str, \n",
      "        v2_internal: bool, \n",
      "        v2_investigator: str, \n",
      "        v2_known_pop: str, \n",
      "        v2_known_subpop: str, \n",
      "        v2_pop: str, \n",
      "        v2_subpop: str, \n",
      "        v2_neuro: bool, \n",
      "        v2_control: bool, \n",
      "        v2_topmed: bool, \n",
      "        v2_high_quality: bool, \n",
      "        v2_release: bool, \n",
      "        v2_pcr_free: bool, \n",
      "        v2_project_name: str, \n",
      "        v2_release_2_0_2: bool, \n",
      "        project_ancestry: str, \n",
      "        project_pop: str, \n",
      "        project_subpop: str, \n",
      "        pdo_owner: str, \n",
      "        category: str, \n",
      "        contact_pi: str, \n",
      "        sample_pi: str, \n",
      "        pm: str, \n",
      "        research_project: str, \n",
      "        pdo: str, \n",
      "        title: str, \n",
      "        product: str, \n",
      "        probably_releasable: str, \n",
      "        releasable: bool, \n",
      "        broad_external: str, \n",
      "        sex: str, \n",
      "        subpop_description: str, \n",
      "        exclude: bool, \n",
      "        exclude_reason: str, \n",
      "        case_control: str, \n",
      "        age: int32, \n",
      "        age_bin: str, \n",
      "        tcga_tumor: bool, \n",
      "        age_alt: int32, \n",
      "        v2_s_match: str, \n",
      "        topmed: bool, \n",
      "        neuro_cohort: bool, \n",
      "        neuro_case: bool\n",
      "    } \n",
      "    'subsets': struct {\n",
      "        non_topmed: bool, \n",
      "        controls_and_biobanks: bool, \n",
      "        non_neuro: bool, \n",
      "        non_v2: bool, \n",
      "        non_cancer: bool, \n",
      "        tgp: bool, \n",
      "        hgdp: bool\n",
      "    } \n",
      "    'bam_metrics': struct {\n",
      "        pct_bases_20x: float64, \n",
      "        pct_chimeras: float64, \n",
      "        freemix: float64, \n",
      "        mean_coverage: float64, \n",
      "        median_coverage: float64, \n",
      "        mean_insert_size: float64, \n",
      "        median_insert_size: float64, \n",
      "        pct_bases_10x: float64\n",
      "    } \n",
      "    'sex_imputation': struct {\n",
      "        is_female: bool, \n",
      "        chr20_mean_dp: float32, \n",
      "        chrX_mean_dp: float32, \n",
      "        chrY_mean_dp: float32, \n",
      "        chrX_ploidy: float32, \n",
      "        chrY_ploidy: float32, \n",
      "        X_karyotype: str, \n",
      "        Y_karyotype: str, \n",
      "        sex_karyotype: str, \n",
      "        impute_sex_stats: struct {\n",
      "            f_stat: float64, \n",
      "            n_called: int64, \n",
      "            expected_homs: float64, \n",
      "            observed_homs: int64\n",
      "        }\n",
      "    } \n",
      "    'sample_qc': struct {\n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64, \n",
      "        n_snp_residual: float64, \n",
      "        n_singleton_residual: float64, \n",
      "        r_ti_tv_residual: float64, \n",
      "        r_insertion_deletion_residual: float64, \n",
      "        n_insertion_residual: float64, \n",
      "        n_deletion_residual: float64, \n",
      "        r_het_hom_var_residual: float64, \n",
      "        n_transition_residual: float64, \n",
      "        n_transversion_residual: float64\n",
      "    } \n",
      "    'population_inference': struct {\n",
      "        training_pop: str, \n",
      "        pca_scores: array<float64>, \n",
      "        pop: str, \n",
      "        prob_afr: float64, \n",
      "        prob_ami: float64, \n",
      "        prob_amr: float64, \n",
      "        prob_asj: float64, \n",
      "        prob_eas: float64, \n",
      "        prob_fin: float64, \n",
      "        prob_mid: float64, \n",
      "        prob_nfe: float64, \n",
      "        prob_oth: float64, \n",
      "        prob_sas: float64, \n",
      "        training_pop_all: str\n",
      "    } \n",
      "    'sample_filters': struct {\n",
      "        sex_aneuploidy: bool, \n",
      "        insert_size: bool, \n",
      "        chimera: bool, \n",
      "        contamination: bool, \n",
      "        bad_qc_metrics: bool, \n",
      "        low_coverage: bool, \n",
      "        ambiguous_sex: bool, \n",
      "        failed_fingerprinting: bool, \n",
      "        TCGA_tumor_sample: bool, \n",
      "        hard_filters: set<str>, \n",
      "        hard_filtered: bool, \n",
      "        release_related: bool, \n",
      "        release_duplicate: bool, \n",
      "        release_parent_child: bool, \n",
      "        release_sibling: bool, \n",
      "        all_samples_related: bool, \n",
      "        all_samples_duplicate: bool, \n",
      "        all_samples_parent_child: bool, \n",
      "        all_samples_sibling: bool, \n",
      "        fail_n_snp_residual: bool, \n",
      "        fail_n_singleton_residual: bool, \n",
      "        fail_r_ti_tv_residual: bool, \n",
      "        fail_r_insertion_deletion_residual: bool, \n",
      "        fail_n_insertion_residual: bool, \n",
      "        fail_n_deletion_residual: bool, \n",
      "        fail_r_het_hom_var_residual: bool, \n",
      "        fail_n_transition_residual: bool, \n",
      "        fail_n_transversion_residual: bool, \n",
      "        qc_metrics_filters: set<str>\n",
      "    } \n",
      "    'relatedness_inference': struct {\n",
      "        relationships: set<str>\n",
      "    } \n",
      "    'high_quality': bool \n",
      "    'release': bool \n",
      "    'hgdp_tgp_meta': struct {\n",
      "        Project: str, \n",
      "        Study: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Population: str, \n",
      "        Genetic: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Latitude: float64, \n",
      "        Longitude: float64, \n",
      "        Continent: struct {\n",
      "            colors: str\n",
      "        }, \n",
      "        n: int32, \n",
      "        rownum: int32, \n",
      "        Pop: struct {\n",
      "            colors: str, \n",
      "            shapes: int32\n",
      "        }\n",
      "    } \n",
      "    'bergstrom': struct {\n",
      "        hgdp: str, \n",
      "        lp: str, \n",
      "        source: str, \n",
      "        library_type: str, \n",
      "        region: str, \n",
      "        sex: str, \n",
      "        coverage: float64, \n",
      "        freemix: float64, \n",
      "        capmq: int32, \n",
      "        insert_size_average: float64, \n",
      "        array_non_reference_discordance: float64, \n",
      "        sample: str\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['s']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# hl.describe() gives you an overview of all the fields in a matrix table or table\n",
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36669a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hl.annotate_cols() method to annotate the metadata onto the matrix table\n",
    "# Using hl.annotate_cols() in this way is essentially merging dense_mt with ht\n",
    "# In order for this hl.annotate_cols() to work, both of the datasets to merge need to share the same key\n",
    "# In this case that key is 's'\n",
    "# When using hl.annotate_cols() the table is being indexed by the equivalent key in  the dense_mt\n",
    "mt = dense_mt.annotate_cols(**ht[dense_mt.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "british-visibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4151)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_count(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "capital-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-08 15:38:02 Hail: INFO: wrote matrix table with 211358784 rows and 4151 columns in 5000 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_dense_meta_preQC.mt\n",
      "    Total size: 3.32 TiB\n",
      "    * Rows/entries: 3.32 TiB\n",
      "    * Columns: 1.71 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (32.13 MiB)\n",
      "    * Largest partition:  183321 rows (4.39 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out a pre-qc version of the dataset for Mary's PCA analyses\n",
    "mt.write(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/pre_qc_final.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-QC Plots\n",
    "When conducting quality control on a dataset, making plots to visualize metrics which explain the data is useful to assess the effects of filters on the dataset\n",
    "\n",
    "When running QC filters individually it can be useful to make plots before and after specific filters. In the case of this dataset, the results of the individual sample and variant filters have been merged into respective field which then allows us to remove all samples/variants that fail any respective QC steps all at once."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rational-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the preQC dataset \n",
    "# This is a merged version of the metadata from different sources and the sample/variant dense dataset\n",
    "mt = hl.read_matrix_table(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/pre_qc_final.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8769dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a preliminary count before filtering on the dataset\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As of hail v. 0.2.82, ggplot only takes in tables as input. as such, we will make a table from our pre_qc matrix table\n",
    "ht = mt.cols()\n",
    "\n",
    "# Create dictionary that maps color to region name\n",
    "newnames = {\"#E41A1C\": 'AMR', \"#984EA3\": 'AFR', \"#999999\": 'OCE', \"#FF7F00\": 'CSA',\n",
    "            \"#4DAF4A\": 'EAS', \"#377EB8\": 'EUR', \"#A65628\": 'MID'}\n",
    "\n",
    "\n",
    "# Using ggplot, differentiate between populations\n",
    "p = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.sample_qc.n_snp)) +\n",
    "hl.ggplot.geom_histogram(hl.ggplot.aes(fill=ht.hgdp_tgp_meta.Continent.colors), min_val=5000000,\n",
    "                         max_val=7500000, bins=200, position=\"identity\", alpha=.7) +\n",
    "hl.ggplot.xlab(\"Number of SNPs\") +\n",
    "hl.ggplot.ggtitle(\"Number of SNPs, Pre-QC\") +\n",
    "hl.ggplot.coord_cartesian(ylim=(0, 260))\n",
    "\n",
    "# Update legends so that the geographic region name corresponds with the correct\n",
    "p = p.to_plotly()\n",
    "p.for_each_trace(lambda t: t.update(name=newnames[t.name]))\n",
    "\n",
    "#show plot\n",
    "p.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample QC filtering\n",
    "As previously mentioned, sample QC filtering for this dataset was conducted using metadata which was annotated onto the main matrix table. Sample QC was run using gnomAD's QC pipeline and the fields used to filter below contain information on whether samples passed or failed gnomAD QC. More details on the gnomAD sample qc steps can be found [here.](https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#sample-qc-hard-filtering)\n",
    "\n",
    "For more information on how sample QC filters were developed see [nb3.](https://github.com/atgu/hgdp_tgp/blob/907619ac3fedf8c9239920c82a9842cf090fbc66/tutorials/nb3.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering samples to those who should pass QC\n",
    "# this filters to only samples that passed gnomad sample QC hard filters\n",
    "mt_filt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "# annotating partially filtered dataset with variant metadata\n",
    "mt_filt = mt_filt.annotate_rows(**var_meta[mt_filt.locus, mt_filt.alleles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the counts of samples/filters after filtering to those who passed sample QC\n",
    "mt_filt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlikely-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 19:18:12 Hail: INFO: wrote matrix table with 211358784 rows and 4120 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt\n",
      "    Total size: 3.82 TiB\n",
      "    * Rows/entries: 3.82 TiB\n",
      "    * Columns: 1.70 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (38.69 MiB)\n",
      "    * Largest partition:  183321 rows (4.82 GiB)\n"
     ]
    }
   ],
   "source": [
    "mt_filt.write('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-glance",
   "metadata": {},
   "source": [
    "# PCA outlier removal\n",
    "For information on how PCA outliers were found see [nb4.](https://github.com/atgu/hgdp_tgp/blob/907619ac3fedf8c9239920c82a9842cf090fbc66/tutorials/nb4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "persistent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the annotated & partially filtered dataset\n",
    "mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecological-bundle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sample and variant count before removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the list of PCA outliers, using the ~ operator which is a negation operator and obtains the compliment\n",
    "# In this case the compliment is samples which are not contained in the pca outlier list\n",
    "mt = mt.filter_cols(~outliers_list.contains(mt['s']))\n",
    "# Removing any duplicates in the dataset using hl.distinct_by_col() which removes columns with a duplicate column key. It keeps one column for each unique key.\n",
    "mt = mt.distinct_by_col()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "technological-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4097)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a count of samples/variants after removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-funeral",
   "metadata": {},
   "source": [
    "# Variant QC filtering\n",
    "Variant QC was run using annotated flags which denoted which variants passed/failed gnomAD's QC pipeline. More details on the variant QC steps conducted can be found on the gnomAD website [here.](https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#variant-qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "disabled-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the variants in the dataset to only PASS variants (those which passed variant QC)\n",
    "# PASS variants are variants which have an entry in the filters field. This field contains an array which contains a bool if any variant qc filter was failed\n",
    "# This is the last step in the QC process\n",
    "mt = mt.filter_rows(hl.len(mt.filters) !=0  ,keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "honest-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155648020, 4097)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the final count of the dataset before writing out the dataset to different formats\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-progress",
   "metadata": {},
   "source": [
    "# Exporting final dataset post QC\n",
    "In order to write out matrix tables in hail, you use the mt.write() method. As a string inside that method you put the path where you want your matrix table to be written out to. Keep in mind a matrix table is a directory format and is a large size. Writing out this dataset will take some time until complete. On your Google cloud cluster, you can switch to worker nodes instead of secondary worker nodes in order to shorten the time it takes to write out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olympic-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 21:19:22 Hail: INFO: wrote matrix table with 155648020 rows and 4097 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/new_hgdp_tgp_postQC.mt\n",
      "    Total size: 3.09 TiB\n",
      "    * Rows/entries: 3.09 TiB\n",
      "    * Columns: 1.69 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (20.00 B)\n",
      "    * Largest partition:  96270 rows (2.23 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out the postQC dataset with PCA sample outliers removed and subset to PASS variants\n",
    "mt.write('gs://hgdp-1kg/post_qc_final.mt', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
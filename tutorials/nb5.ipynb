{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887a96d7",
   "metadata": {},
   "source": [
    "# Assigning Ancestry Labels Using a Random Forest Model\n",
    "Author: Lethukuthula Nkambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fece539",
   "metadata": {},
   "source": [
    "**To run this tutorial, we suggest you start your cluster with the following commmand.** *If you have not done so, shut down your current cluster and start a new session.* \n",
    "\n",
    "<code>hailctl dataproc start qc-notebook5 --project [YOUR_PROJECT_NAME] --num-secondary-workers 50 --region=us-central1 --zone=us-central1-b --packages git+https://github.com/broadinstitute/gnomad_methods.git --master-machine-type n1-highmem-8 --worker-machine-type n1-highmem-8 --big-executors</code>\n",
    "\n",
    "See the tutorials [README](https://github.com/atgu/hgdp_tgp/tree/master/tutorials#readme) for more information on how to start a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f9b97",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Set Default Paths](#1.-Set-Default-Paths)\n",
    "2. [Intersecting Two Datasets](#2.-Intersecting-Two-Datasets)\n",
    "3. [Applying gnomAD RF Model to HGDP+1kGP-GGV Intersect](#3.-Applying-gnomAD-RF-Model-to-Intersected-Dataset)\n",
    "    1. [Plotting PCA After Applying gnomAD RF to HGDP+1kGP-GGV Intersect](#3a.-Plotting-PCA-After-Applying-gnomAD-RF-to-Intersected-Dataset) \n",
    "4. [Building an RF Model Using HGDP+1kGP and Applying to a New Dataset](#4.-Building-a-Random-Forest-Model-from-HGDP+1kGP-and-Applying-to-a-New-Dataset)\n",
    "    1. [Plotting PCA After Applying HGDP+1kGP RF to GGV](#4a.-Plotting-PCA-After-Building-RF-Model-from-HGDP+1kGP-Dataset-and-Applying-It-to-GGV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2814cb4",
   "metadata": {},
   "source": [
    "# General Overview:\n",
    "\n",
    "The purpose of this notebook is to show how to use the HGDP+1kGP resource with an external dataset to do ancestry analyses. Specifically, we show how to apply a machine learning method called a random forest (RF) classifier trained on the population metadata labels from gnomAD to an external dataset to learn population labels in the new dataset. The gnomAD random forest has already been [generated and released previously](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/).\n",
    "We also show how to build a random forest classifier for HGDP+1kGP so you can build a new one for your dataset with an arbitrary set of SNVs. \n",
    "\n",
    "**This notebook contains information on how to:**\n",
    "- Intersect two datasets\n",
    "- Apply a random forest model \n",
    "- Build a random forest model\n",
    "- Plot PCA after applying a RF model to a dataset \n",
    "\n",
    "\n",
    "**Abbreviations**<br>\n",
    "HGDP: [Human Genome Diversity Project](https://www.internationalgenome.org/data-portal/data-collection/hgdp)<br>\n",
    "1kGP: [1000 Genomes Project](https://www.internationalgenome.org/1000-genomes-summary)<br>\n",
    "GGV: [Gambian Genome Variation Project](https://www.internationalgenome.org/gambian-genome-variation-project/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7997e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import hail as hl\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import show, output_notebook, output_file\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import ColumnDataSource, Legend, TableColumn, DataTable\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.transform import factor_cmap\n",
    "from gnomad.sample_qc.ancestry import assign_population_pcs, pc_project\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfaefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77d494",
   "metadata": {},
   "source": [
    "# 1. Set Default Paths\n",
    "\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets. \n",
    "\n",
    "By default all of the write sections are shown as markdown cells. If you would like to write out your own datasets, you can copy the code and paste it into a new code cell. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b427ac5",
   "metadata": {},
   "source": [
    "# 2. Intersecting Two Datasets \n",
    "\n",
    "The first step in building the random forest model is to intersect the HGDP+1kGP unrelateds dataset with the Gambian Genome Variation Project dataset.  \n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "    \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.key_rows_by\"> More on  <i> key_rows_by() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.collect\"> More on  <i> collect() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.union_rows\"> More on  <i> union_cols() </i></a></li>\n",
    "</ul>\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input file path - HGDP+1kGP mt produced in nb1 with gnomAD sample, variant & genotype QC applied \n",
    "hgdp_tgp_path = 'gs://hgdp-1kg/tutorial_datasets/metadata_and_qc/post_qc.mt'\n",
    "\n",
    "# Second input file path - GGV dataset \n",
    "ggv_path = 'gs://gnomaf/gambian-genomes/COMBINED_GVCFS/gambian_genomes_merged_gvcfs.mt'\n",
    "\n",
    "# Path for gnomAD loadings Hail Table\n",
    "gnomad_loadings_path = 'gs://gcp-public-data--gnomad/release/3.1/pca/gnomad.v3.1.pca_loadings.ht'\n",
    "\n",
    "# Path for gnomAD's RF model\n",
    "gnomad_rf_path = 'gs://gcp-public-data--gnomad/release/3.1/pca/gnomad.v3.1.RF_fit.pkl'\n",
    "\n",
    "# Path for HGDP+1kGP metadata obtained from gnomAD  \n",
    "gnomad_metadata_path = 'gs://hgdp-1kg/tutorial_datasets/metadata_and_qc/gnomad_meta_v1.tsv'\n",
    "\n",
    "# File path for unrelated individuals without outliers - mt written out at the end of nb4 \n",
    "unrelateds_without_outliers_path = 'gs://hgdp-1kg/tutorial_datasets/pca_results/unrelateds_without_outliers.mt'\n",
    "\n",
    "# Path for the intersected dataset \n",
    "data_intersect_path = 'gs://hgdp-1kg/tutorial_datasets/data_intersection/hgdp_tgp_ggv_intersect.mt'\n",
    "\n",
    "# Path for file containing SuperPopulation labels \n",
    "# Created using this script - https://github.com/atgu/GWASpy/blob/main/gwaspy/pca/filter_ref_data.py \n",
    "# Further filtered to only include unrelated samples and no outliers \n",
    "ref_info_path = 'gs://hgdp-1kg/tutorial_datasets/data_intersection/hgdp_1kg_sample_info.unrelateds.pca_outliers_removed.with_project.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121fd35",
   "metadata": {},
   "source": [
    "### 2a. Read HGDP+1kGP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108374c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in HGDP+1kGP dataset \n",
    "mt_hgdp_tgp = hl.read_matrix_table(hgdp_tgp_path, _n_partitions=500)\n",
    "\n",
    "print(f'Number of variants in HGDP+1kGP before intersecting: {mt_hgdp_tgp.count_rows()}') # 159795273"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022669f",
   "metadata": {},
   "source": [
    "### 2b. Read GGV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in GGV dataset - a sparse mt from combining GVCFs\n",
    "mt_ggv = hl.read_matrix_table(ggv_path, _n_partitions=500)\n",
    "\n",
    "# Hail still keeps the non-variant sites (contain only REF allele). So we have to filter to variant-sites only\n",
    "mt_ggv = mt_ggv.filter_rows(hl.len(mt_ggv.alleles) > 1)\n",
    "\n",
    "# The GGV dataset has multiallelic variants that need to be split\n",
    "mt_ggv = hl.experimental.sparse_split_multi(mt_ggv) # split multiallelic sites\n",
    "\n",
    "print(f'Number of variant and bi-allelic sites only in GGV before intersecting: {mt_ggv.count_rows()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516aa8c",
   "metadata": {},
   "source": [
    "### 2c. Select only fields that will be used downstream\n",
    "### In order to intersect two datasets, three requirements must be met:\n",
    "\n",
    "1. The row keys must match.\n",
    "\n",
    "2. The column key schemas and column schemas must match.\n",
    "\n",
    "3. The entry schemas must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_hgdp_tgp_clean = mt_hgdp_tgp.select_cols() # select s (sampleID) field\n",
    "mt_hgdp_tgp_clean = mt_hgdp_tgp_clean.select_rows(mt_hgdp_tgp_clean.rsid) # select rsid field\n",
    "mt_hgdp_tgp_clean = mt_hgdp_tgp_clean.select_entries(mt_hgdp_tgp_clean.GT) # select GT field\n",
    "\n",
    "# Collect sample ID list to be used later to check how they were classified by the RF model\n",
    "hgdp_tgp_samples = mt_hgdp_tgp_clean.s.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_ggv_clean = mt_ggv.select_cols()\n",
    "mt_ggv_clean = mt_ggv_clean.select_rows(mt_ggv_clean.rsid)\n",
    "mt_ggv_clean = mt_ggv_clean.select_entries(mt_ggv_clean.GT)\n",
    "\n",
    "# Collect GGV samples to list so we can later use this to check how they were classified by the RF model\n",
    "ggv_samples = mt_ggv_clean.s.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35651d",
   "metadata": {},
   "source": [
    "### 2d. Intersect the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdp_tgp_ggv_intersect = mt_hgdp_tgp_clean.union_cols(mt_ggv_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint the intersected dataset so that the following commands don't take a long time to run\n",
    "## Took 10min to checkpoint \n",
    "#hgdp_tgp_ggv_intersect = hgdp_tgp_ggv_intersect.checkpoint(data_intersect_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file back in \n",
    "hgdp_tgp_ggv_intersect = hl.read_matrix_table(data_intersect_path)\n",
    "\n",
    "print(f'Number of variants after intersecting HGDP+1kGP with GGV: {hgdp_tgp_ggv_intersect.count_rows()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3dbd9",
   "metadata": {},
   "source": [
    "# 3. Applying gnomAD RF Model to Intersected Dataset\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "    \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/experimental/index.html#hail.experimental.pc_project\"> More on  <i> pc_project() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/utils/index.html#hail.utils.hadoop_open\"> More on  <i> hadoop_open() </i></a></li>\n",
    "\n",
    "</ul>\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd47903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomAD loadings Hail Table\n",
    "loadings_ht = hl.read_table(gnomad_loadings_path)\n",
    "\n",
    "gnomad_loadings_count = loadings_ht.count()\n",
    "print(f'Number of variants in gnomAD loadings: {gnomad_loadings_count}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of variants found in gnomAD loadings and hgdp_tgp_ggv_intersect\n",
    "# Scores usually shrink towards zero for missingness > 5% and more samples will get classified as OTH\n",
    "\n",
    "hgdp_tgp_ggv_intersect = hgdp_tgp_ggv_intersect.annotate_rows(\n",
    "        pca_loadings=loadings_ht[hgdp_tgp_ggv_intersect.row_key]['loadings'],\n",
    "        pca_af=loadings_ht[hgdp_tgp_ggv_intersect.row_key]['pca_af'],\n",
    "    )\n",
    "\n",
    "gnomad_loadings_data_interset_count = hgdp_tgp_ggv_intersect.aggregate_rows(hl.agg.count_where(\n",
    "    hl.is_defined(hgdp_tgp_ggv_intersect.pca_loadings) & hl.is_defined(hgdp_tgp_ggv_intersect.pca_af)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of variants common between HGDP+1kGP+GGV & gnomAD RF: {gnomad_loadings_data_interset_count}')\n",
    "missingness = round((1 - (gnomad_loadings_data_interset_count/gnomad_loadings_count)) * 100, 2)\n",
    "print(f'Level of missingness: {missingness}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d728a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project HGDP+1kGP+GGV genotypes onto gnomAD loadings\n",
    "ht = hl.experimental.pc_project(\n",
    "    hgdp_tgp_ggv_intersect.GT,\n",
    "    loadings_ht.loadings,\n",
    "    loadings_ht.pca_af,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4301854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gnomAD RF model\n",
    "with hl.hadoop_open(gnomad_rf_path, 'rb') as f:\n",
    "    fit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04861100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the scores to only those used in the RF model, this was 6 for v2 and 16 for v3.1 \n",
    "num_pcs = fit.n_features_\n",
    "ht = ht.annotate(scores=ht.scores[:num_pcs])\n",
    "\n",
    "# Infer population labels in HGDP+1kGP+GGV using gnomAD RF model\n",
    "ht, rf_model = assign_population_pcs(\n",
    "    ht,\n",
    "    pc_cols=[(i + 1) for i in range(num_pcs)],\n",
    "    fit=fit,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC scores are in one column saved as an array, split this into columns for each PC\n",
    "gnomad_rf_output = ht.transmute(**{f'PC{i}': ht.pca_scores[i - 1] for i in range(1, num_pcs+1)})\n",
    "\n",
    "# Convert Hail Table to Pandas DataFrame\n",
    "gnomad_rf_output = gnomad_rf_output.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54094db8",
   "metadata": {},
   "source": [
    "## 3a. Plotting PCA After Applying gnomAD RF to Intersected Dataset\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to use to store colors for each population label\n",
    "color_map = {}\n",
    "\n",
    "# Get a list of population labels (inferred) in the data\n",
    "rf_labels_inferred = list(gnomad_rf_output['pop'].unique())\n",
    "\n",
    "# Update the dictionary with unique colors for each population\n",
    "for i in range(len(rf_labels_inferred)):\n",
    "    color_map[rf_labels_inferred[i]] = Category10[len(rf_labels_inferred)][i]\n",
    "\n",
    "tabs1 = []\n",
    "\n",
    "# Split the dataframe into two dataframes: HGDP+1kGP and GGV\n",
    "ref_samples_df1 = gnomad_rf_output[gnomad_rf_output['s'].isin(hgdp_tgp_samples)]\n",
    "ggv_samples_df1 = gnomad_rf_output[gnomad_rf_output['s'].isin(ggv_samples)]\n",
    "\n",
    "def plot_pca(\n",
    "        ref_df: pd.DataFrame = None,\n",
    "        data_df: pd.DataFrame = None,\n",
    "        x_pc: str = None,\n",
    "        y_pc: str = None\n",
    ") -> Tuple[figure, figure, figure]:\n",
    "    \"\"\"\n",
    "    This function is for plotting PCA scores\n",
    "\n",
    "    :param pd.DataFrame ref_df: DataFrame with reference PCA scores to be plotted\n",
    "    :param pd.DataFrame data_df: DataFrame with data PCA scores to be plotted\n",
    "    :param str x_pc: x-axis (bottom) PC scores\n",
    "    :param str y_pc: y-axis (left) PC scores\n",
    "    \n",
    "    :rtype: figure, figure, figure\n",
    "    \"\"\"\n",
    "    pref = figure(width=600, height=500, background_fill_color='#fafafa', title = 'HGDP+1kGP')\n",
    "    pref.add_layout(Legend(), 'right')\n",
    "    pref.xaxis.axis_label = x_pc\n",
    "    pref.yaxis.axis_label = y_pc\n",
    "    \n",
    "    pdata = figure(width=600, height=500, background_fill_color='#fafafa', title = 'GGV')\n",
    "    pdata.add_layout(Legend(), 'right')\n",
    "    pdata.xaxis.axis_label = x_pc\n",
    "    pdata.yaxis.axis_label = y_pc\n",
    "    \n",
    "    pcomb = figure(width=600, height=500, background_fill_color='#fafafa', title = 'HGDP+1kGP+GGV')\n",
    "    pcomb.add_layout(Legend(), 'right')\n",
    "    pcomb.xaxis.axis_label = x_pc\n",
    "    pcomb.yaxis.axis_label = y_pc\n",
    "    pcomb.circle(ref_df[x_pc].tolist(), ref_df[y_pc].tolist(), size=3, color='grey',\n",
    "                 alpha=0.3, legend_label='HGDP+1kGP')\n",
    "\n",
    "    for pop, col in color_map.items():\n",
    "        # reference\n",
    "        if pop in list(ref_df['pop'].unique()):\n",
    "            pref.circle(ref_df[(ref_df['pop'] == pop)][x_pc].tolist(), ref_df[(ref_df['pop'] == pop)][y_pc].tolist(),\n",
    "                        size=3, color=col, alpha=0.8, legend_label=pop)\n",
    "        \n",
    "        # data\n",
    "        if pop in list(data_df['pop'].unique()):\n",
    "            pdata.circle(data_df[(data_df['pop'] == pop)][x_pc].tolist(), data_df[(data_df['pop'] == pop)][y_pc].tolist(),\n",
    "                         size=3, color=col, alpha=0.8, legend_label=pop)\n",
    "        \n",
    "        # ref+data combined\n",
    "        if pop in list(data_df['pop'].unique()):\n",
    "            pcomb.circle(data_df[(data_df['pop'] == pop)][x_pc].tolist(), data_df[(data_df['pop'] == pop)][y_pc].tolist(),\n",
    "                         size=3, color=col, alpha=0.8, legend_label=pop)\n",
    "        \n",
    "    return pref, pdata, pcomb\n",
    "\n",
    "\n",
    "for i in range(1, num_pcs, 2):\n",
    "    xpc = f'PC{i}'\n",
    "    ypc = f'PC{i + 1}'\n",
    "    \n",
    "    p1, p2, p3 = plot_pca(ref_df=ref_samples_df1, data_df=ggv_samples_df1, x_pc=xpc, y_pc=ypc)\n",
    "        \n",
    "    tab = Panel(child=column(row(p1, p2), row(p3)), title=f'{xpc}v{ypc}')\n",
    "\n",
    "    tabs1.append(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(Tabs(tabs=tabs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47663284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in truth population for HGDP+1kGP sample - from gnomAD metadata\n",
    "truth_pop_labels = pd.read_csv(gnomad_metadata_path, sep='\\t', low_memory=False)\n",
    "truth_pop_labels = truth_pop_labels[['project_meta.sample_id', 'hgdp_tgp_meta.Project', 'hgdp_tgp_meta.Genetic.region']]\n",
    "truth_pop_labels.columns = ['Sample', 'Project', 'SuperPop']\n",
    "\n",
    "# Add population labels to the dataframe with inferred (using RF model) population labels\n",
    "merged = pd.merge(left=gnomad_rf_output[['s', 'pop']], right=truth_pop_labels,\n",
    "                  left_on='s', right_on='Sample', how='left')\n",
    "\n",
    "# All GGV samples are AFR\n",
    "merged['Project'].fillna('GGV', inplace=True)\n",
    "merged.loc[(merged.Project == 'GGV'),'SuperPop'] = 'AFR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e760b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to a Hail Table so we can easily get counts (not straight forward to do in Pandas)\n",
    "# First make sure we have every column type as string\n",
    "merged = merged.astype({'s': str, 'pop': str, 'Sample': str, 'SuperPop': str, 'Project': str})\n",
    "\n",
    "# Convert DataFrame to a Hail Table\n",
    "t = hl.Table.from_pandas(merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print counts of how many samples were classified: (1) correctly; (2) incorrectly; or (3) as OTH\n",
    "(t.group_by(t.Project).aggregate(n=hl.agg.count(),\n",
    "                                 match=hl.agg.count_where(t.pop.upper() == t.SuperPop),\n",
    "                                 mismatch=hl.agg.count_where((t.pop.upper() != t.SuperPop) & (t.pop.upper() != 'OTH')),\n",
    "                                 oth=hl.agg.count_where(t.pop.upper() == 'OTH'))).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d93870",
   "metadata": {},
   "source": [
    "### Note about the plots and table above:\n",
    "Because the gnomAD random forest is trained on 76,399 SNVs and our dataset only has 40005 of these, we are missing almost half (47.64%) of the training data. As a result, most of the samples are assigned “oth” or misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5294f5",
   "metadata": {},
   "source": [
    "# 4. Building a Random Forest Model from HGDP+1kGP and Applying to a New Dataset \n",
    "\n",
    "In the following steps we are building a random forest (RF) model with unrelated individuals from the HGDP+1kGP dataset. This was done using global region labels. \n",
    "We then apply the model to the Gambian Genome Variation Project (GGV) dataset. \n",
    "\n",
    "[INSERT LINK] For more information on Random Forest models click [here]().\n",
    "\n",
    "[INSERT LINK] For more information on the GGV dataset click [here]().\n",
    "    \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99367fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_ref(\n",
    "    ref_mt: hl.MatrixTable = None, \n",
    "    data_mt: hl.MatrixTable = None\n",
    ") -> Tuple[hl.MatrixTable, hl.MatrixTable]:\n",
    "    \"\"\"\n",
    "    This function is for intersecting reference data with input data\n",
    "\n",
    "    :param hl.MatrixTable ref_mt: reference data to be intersected with input data\n",
    "    :param hl.MatrixTable data_mt: input data to be intersected with reference data\n",
    "    \n",
    "    :rtype: hl.MatrixTable, hl.MatrixTable\n",
    "    \"\"\"\n",
    "    \n",
    "    data_in_ref = data_mt.filter_rows(hl.is_defined(ref_mt.rows()[data_mt.row_key]))\n",
    "    print('sites common between the data and ref, inds in data: {}'.format(data_in_ref.count()))\n",
    "\n",
    "    ref_in_data = ref_mt.filter_rows(hl.is_defined(data_mt.rows()[ref_mt.row_key]))\n",
    "    print('sites commond between the ref and data, inds in ref: {}'.format(ref_in_data.count()))\n",
    "    \n",
    "    return ref_in_data, data_in_ref\n",
    "\n",
    "\n",
    "def run_ref_pca(\n",
    "    mt: hl.MatrixTable = None,\n",
    "    npcs: int = 20\n",
    ") -> Tuple[hl.Table, hl.Table]:\n",
    "    \"\"\"\n",
    "    This function is for running PCA\n",
    "\n",
    "    :param hl.MatrixTable mt: data to be used to run PCA\n",
    "    :param int npcs: number of principal components to use in running PCA\n",
    "    \n",
    "    :rtype: hl.Table, hl.Table\n",
    "    \"\"\"\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=npcs, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "\n",
    "    # individual-level PCs\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, npcs+1)})\n",
    "    \n",
    "    return pca_loadings, pca_scores\n",
    "\n",
    "\n",
    "def merge_data_with_ref(\n",
    "    ref_scores: hl.Table = None,\n",
    "    ref_info: str = ref_info_path,\n",
    "    data_scores: hl.Table = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is for merging the reference scores DataFrame with the data scores DataFrame\n",
    "\n",
    "    :param hl.Table ref_scores: Table with reference scores\n",
    "    :param str ref_info: path to file containing SuperPopulation labels\n",
    "    :param hl.Table data_scores: Table with data scores\n",
    "    \n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    print('Merging data with ref')\n",
    "    ref_info = hl.import_table(ref_info,\n",
    "                           impute=True, key='Sample')\n",
    "    ref_merge = ref_scores.annotate(SuperPop = ref_info[ref_scores.s].SuperPop)\n",
    "\n",
    "    print('merging data and ref data')\n",
    "    data_ref = ref_merge.union(data_scores, unify=True)\n",
    "    print('Done merging data with ref')\n",
    "\n",
    "    return data_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pruned post-QC mt with unrelated individuals (without outliers) to speed up things\n",
    "mt_unrel = hl.read_matrix_table(unrelateds_without_outliers_path, _n_partitions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the HGDP+1kGP and GGV datasets to variants ONLY common between the two\n",
    "# Took ~1hr to run\n",
    "hgdp_tgp_in_ggv_mt, ggv_in_hgdp_tgp_mt = intersect_ref(ref_mt=mt_unrel, data_mt=mt_ggv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loadings and scores for the HGDP+1kGP data \n",
    "# Took ~2hrs & 48min to run\n",
    "ref_pca_loadings, ref_pca_scores = run_ref_pca(mt=hgdp_tgp_in_ggv_mt, npcs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa544dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project GGV's genotypes onto HGDP+1kGP PCs we computed above\n",
    "data_projections_ht = pc_project(mt=ggv_in_hgdp_tgp_mt, loadings_ht=ref_pca_loadings,\n",
    "                                 loading_location='loadings', af_location='pca_af')\n",
    "\n",
    "# Instead of having all PCs in one column as an array, create a column for each PC\n",
    "data_scores = data_projections_ht.transmute(**{f'PC{i}': data_projections_ht.scores[i - 1] for i in range(1, 20+1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took ~1hr to run\n",
    "data_ref = merge_data_with_ref(ref_scores=ref_pca_scores, data_scores=data_scores)\n",
    "\n",
    "data_ref_df = data_ref.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht, rf_model = assign_population_pcs(\n",
    "    data_ref_df,\n",
    "    pc_cols=['PC{}'.format(i + 1) for i in range(20)],\n",
    "    known_col=\"SuperPop\",\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d40108",
   "metadata": {},
   "source": [
    "## 4a. Plotting PCA After Building RF Model from HGDP+1kGP Dataset and Applying It to GGV\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to use to store colors for each population label\n",
    "color_map = {}\n",
    "\n",
    "# Get a list of population labels in the data\n",
    "rf_pop_labels = data_ref_df['pop'].unique().tolist()\n",
    "\n",
    "# update the dictionary with unique colors for each population\n",
    "for i in range(len(rf_pop_labels)):\n",
    "    color_map[rf_pop_labels[i]] = Category10[len(rf_pop_labels)][i]\n",
    "    \n",
    "    \n",
    "tabs2 = []\n",
    "\n",
    "ref_samples_df2 = data_ref_df[data_ref_df['s'].isin(hgdp_tgp_samples)]\n",
    "ggv_samples_df2 = data_ref_df[data_ref_df['s'].isin(ggv_samples)]\n",
    "\n",
    "def plot_pca(\n",
    "        ref_df: pd.DataFrame = None,\n",
    "        data_df: pd.DataFrame = None,\n",
    "        x_pc: str = None,\n",
    "        y_pc: str = None\n",
    ") -> Tuple[figure, figure]:\n",
    "    \"\"\"\n",
    "    This function is for plotting PCA scores\n",
    "\n",
    "    :param pd.DataFrame ref_df: DataFrame with reference PCA scores to be plotted\n",
    "    :param pd.DataFrame data_df: DataFrame with data PCA scores to be plotted\n",
    "    :param str x_pc: x-axis (bottom) PC scores\n",
    "    :param str y_pc: y-axis (left) PC scores\n",
    "    \n",
    "    :rtype: figure, figure\n",
    "    \"\"\"\n",
    "    pcomb1 = figure(width=600, height=500, background_fill_color='#fafafa', title = 'HGDP+1kGP and GGV')\n",
    "    pcomb1.add_layout(Legend(), 'right')\n",
    "    pcomb1.xaxis.axis_label = x_pc\n",
    "    pcomb1.yaxis.axis_label = y_pc\n",
    "    pcomb1.circle(data_df[x_pc].tolist(), data_df[y_pc].tolist(), size=3, color='grey',\n",
    "                 alpha=0.3, legend_label='GGV')\n",
    "    \n",
    "    pcomb2 = figure(width=600, height=500, background_fill_color='#fafafa', title = 'GGV and HGDP+1kGP')\n",
    "    pcomb2.add_layout(Legend(), 'right')\n",
    "    pcomb2.xaxis.axis_label = x_pc\n",
    "    pcomb2.yaxis.axis_label = y_pc\n",
    "    pcomb2.circle(ref_df[x_pc].tolist(), ref_df[y_pc].tolist(), size=3, color='grey',\n",
    "                 alpha=0.3, legend_label='HGDP+1kGP')\n",
    "\n",
    "    for pop, col in color_map.items():\n",
    "        # HGDP+1kGP colored and GGV grey\n",
    "        if pop in ref_df['pop'].unique().tolist():\n",
    "            pcomb1.circle(ref_df[(ref_df['pop'] == pop)][x_pc].tolist(), ref_df[(ref_df['pop'] == pop)][y_pc].tolist(),\n",
    "                        size=3, color=col, alpha=0.8, legend_label=pop)\n",
    "        \n",
    "        # HGDP+1kGP grey and GGV colored\n",
    "        if pop in data_df['pop'].unique().tolist():\n",
    "            pcomb2.circle(data_df[(data_df['pop'] == pop)][x_pc].tolist(), data_df[(data_df['pop'] == pop)][y_pc].tolist(),\n",
    "                         size=3, color=col, alpha=0.8, legend_label=pop)\n",
    "        \n",
    "    return pcomb1, pcomb2\n",
    "\n",
    "\n",
    "for i in range(1, 20, 2):\n",
    "    xpc = f'PC{i}'\n",
    "    ypc = f'PC{i + 1}'\n",
    "    \n",
    "    p1, p2 = plot_pca(ref_df=ref_samples_df2, data_df=ggv_samples_df2, x_pc=xpc, y_pc=ypc)\n",
    "        \n",
    "    tab = Panel(child=column(row(p1, p2)), title=f'{xpc}v{ypc}')\n",
    "\n",
    "    tabs2.append(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(Tabs(tabs=tabs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts by POP\n",
    "ggv_samples_df2['pop'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0692fd",
   "metadata": {},
   "source": [
    "### Note about the plots:\n",
    "\n",
    "We can see that all the GGV samples are getting classified as correclty AFR. So building our own model using the HGDP+1kGP data instead of using the gnomAD RF model did a better job at inferring ancestry labels in this case.\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
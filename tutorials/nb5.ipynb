{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1f5be1",
   "metadata": {},
   "source": [
    "Notebook 5: F2 and FST\n",
    "\n",
    "1. FST in Hail is currently in the works so is a bit messy right now (second half of the nb)\n",
    "2. In the meantime, FST is calculated using PLINK\n",
    "3. Heat maps for this analysis were plotted using R - will be kept that way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94ffb8",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [General Overview](#General-Overview)\n",
    "- [F2 analysis](#F2-analysis)\n",
    "- [FST](#FST)\n",
    "    - [FST with PLINK](#1.-FST-with-PLINK)\n",
    "    - [FST with HAIL](#2.-FST-with-HAIL)\n",
    "- [Additional Notes](#Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63d717",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "The purpose of this notebook is to show two population genetics analyses (F2 and FST) to understand recent and deep history. It contains steps on how to: \n",
    "\n",
    "- Read in a matrix table and filter using sample IDs that were obtained from another matrix table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a matrix table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain fields\n",
    "- Calculate FST (once there is progress on this, I will elaborate more)\n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Hail\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d0863",
   "metadata": {},
   "source": [
    "# F2 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28caa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the right intermediate file \n",
    "mt_filt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt')\n",
    "\n",
    "# filter to just the unrelated samples\n",
    "# use the file exported for Lindo - unrelated samples mt without outliers - to obtain desired samples  \n",
    "\n",
    "mt_unrel = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt') # unrelated mt - 3380 samples\n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt to only unrelated samples\n",
    "\n",
    "mt_filt_unrel.count() # (155648020, 3380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run common variant statistics (quality control metrics) - more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# separate the AC array into individual fields   \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "\n",
    "# extract the doubletons\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "#mt_unrel_only2.count() # (18018978, 3380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### checking # of doubletons with d/t filter parameters \n",
    "\n",
    "# how many of the first allele are doubletons? \n",
    "mt_unrel_only2_ac1 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2))\n",
    "print(mt_unrel_only2_ac1.count()) # (3354, 3380)\n",
    "\n",
    "# how many of the second allele are doubletons? \n",
    "mt_unrel_only2_ac2 = mt_unrel_interm.filter_rows(mt_unrel_interm.AC2 == 2)\n",
    "print(mt_unrel_only2_ac2.count()) # (18015720, 3380)\n",
    "\n",
    "# where both alleles are equal 2\n",
    "mt_unrel_only2_both = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) & (mt_unrel_interm.AC2 == 2))\n",
    "print(mt_unrel_only2_both.count()) # (96, 3380)\n",
    "\n",
    "\n",
    "# sanity check \n",
    "# mt_unrel_only2_ac1 + mt_unrel_only2_ac2 - mt_unrel_only2_both = mt_unrel_only2\n",
    "3354+18015720-96  == 18018978 # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it out saving took ~23 min  \n",
    "#mt_unrel_only2.write('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it back in \n",
    "mt_unrel_only2 = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f22d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "#mt_unrel_only2_filtered.count() # (17997741, 3380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check AF to see if the doubleton is the ref or alt allele \n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print(mt_doubl_ref.count()) # (3159, 3380)\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print(mt_doubl_alt.count()) # (17994582, 3380)\n",
    "\n",
    "# sanity check \n",
    "# mt_doubl_ref.count()[0] + mt_doubl_alt.count()[0] = mt_unrel_only2_filtered.count()[0]\n",
    "#print(3159 + 17994582 == 17997741) # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the samples that are doubletons in each mt and compile them in a set \n",
    "# chose set instead of a list cause a list isn't hashable and the next step woulde not run \n",
    "\n",
    "# doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# if one sample in the new column field then it's 0|0. If two, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1))| (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "\n",
    "# doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# if one sample in the new column field then it's 1|1. If two, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1))| (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_ref_collected.samples_with_doubletons.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089cfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field \n",
    "# returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "\n",
    "len(all_doubl_count) # 3183039\n",
    "\n",
    "# sanity check - add up the count of the two dict and then subtract the # of keys that intersect b/n the two \n",
    "# the value that you get should be equal to the combined dict length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of samples from mt\n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# make pairs from sample list - 5710510 pairs - n(n-1)/2)\n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(sample_list) for j,y in enumerate(sample_list) if i<j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463df740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "# subset dict to keys with sample pairs (not just 1)\n",
    "dict_pair_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) != 1}\n",
    "\n",
    "# sanity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True \n",
    "\n",
    "# further investigation\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True - are the samples in the list the same as the dict keys?  \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from mt than what's in the dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sample \n",
    "\n",
    "# go through the list of samples obtained from the mt and see if any of them are keys in the single sample dict \n",
    "# if they are, record the sample ID twice and it's corresponding value from the dict \n",
    "# if they are not, then record the sample ID twice and set the value to 0 \n",
    "\n",
    "# list Comprehension version \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# # for loop version \n",
    "# sample_ids = ['NA12546B', 'NA12830A', 'HG02688', 'HG02334', 'NA21130'] # trail list \n",
    "# sample_dict = {frozenset({'HG02334'}): 639, frozenset({'NA21130'}): 497, frozenset({'HG02688'}): 83} # trial dict \n",
    "# final_data = []\n",
    "# for s in sample_ids:\n",
    "#     if sample_dict.get(frozenset([s])) is None:\n",
    "#         final_data.append([s, s, 0])\n",
    "#     else:\n",
    "#         final_data.append([s, s, sample_dict[frozenset([s])]])\n",
    "\n",
    "# sanity check \n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True - for the single samples, the len should be consistent across dict, mt sample list and final list\n",
    "# compare the counts in the final list against the ones in the dict \n",
    "# if all the comparisons are True, the output of this command will also be True \n",
    "all([single_sample_final_list[x][2] == dict_single_samples[frozenset([single_sample_final_list[x][0]])] for x in range(len(single_sample_final_list))]) # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample pairs  \n",
    "\n",
    "# go through the list of sample pairs created from the samples obtained from mt and see if any of them are keys in the sample pair dict \n",
    "# if they are, record the two sample IDs and the corresponding value from the dict\n",
    "# if they are not, then record the two sample IDs and set the value to 0 \n",
    "\n",
    "# list Comprehension version \n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# # for loop version \n",
    "# sample_ids = [{'NA12546B', 'NA12830A'}, {'HG02757', 'NA12546B'}, {'HG02184', 'HGDP00863'}, {'LP6005443-DNA_D02', 'NA19068'}, {'HG02611', 'NA12546B'}] # trail list \n",
    "# sample_dict = {frozenset({'HG02184', 'HGDP00863'}): 639, frozenset({'LP6005443-DNA_D02', 'NA19068'}): 497, frozenset({'NA19982', 'NA20356'}): 83} # trial dict \n",
    "\n",
    "# sanity check \n",
    "len(sample_pair_final_list) == len(mt_sample_pairs) # True\n",
    "# STILL IN THE WORKS :-\n",
    "# compare the counts in the final list against the ones in the dict \n",
    "# if all the comparisons are True, the output of this command will also be True \n",
    "# all([sample_pair_final_list[x][2] == dict_pair_samples[frozenset([sample_pair_final_list[x][0], sample_pair_final_list[x][1]])] for x in range(len(sample_pair_final_list))]) # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# sanity check \n",
    "len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f73200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format list as pandas table \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(final_list)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8af25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column names \n",
    "# used 'inplace' instead of assigning it back to df\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to the cloud so it can be plotted with R \n",
    "df.to_csv('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton_sample_pair_count_tbl.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample ID and population for heatmap annotation \n",
    "sampleID_pop_reg = (mt_unrel_only2_filtered.select_cols(mt_unrel_only2_filtered['hgdp_tgp_meta']['Population'], mt_unrel_only2_filtered['hgdp_tgp_meta']['Genetic']['region'])).cols()\n",
    "sampleID_pop_reg.export('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/sampleID_pop_reg.txt', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57682fbc",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128db93",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dict key is frozenset \n",
    "\n",
    "# get value \n",
    "#list(ref_doubl_count.values())[0]\n",
    "\n",
    "# get key\n",
    "#list(list(ref_doubl_count.keys())[0])\n",
    "\n",
    "# get one sample from the pair \n",
    "#list(set(list(ref_doubl_count.keys())[2]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check to make sure that mt.GT.is_het() and mt.GT == hl.call(0, 1) produce the same results \n",
    "\n",
    "# trial_1 = mt_unrel_only2_filtered.annotate_rows(\n",
    "#     samples_with_doubletons = hl.agg.filter(\n",
    "#         mt_unrel_only2_filtered.GT.is_het(), hl.agg.collect(mt_unrel_only2_filtered.s)))\n",
    "\n",
    "# trial_2 = mt_unrel_only2_filtered.annotate_rows(\n",
    "#     samples_with_doubletons = hl.agg.filter(\n",
    "#         mt_unrel_only2_filtered.GT == hl.call(0, 1), hl.agg.collect(mt_unrel_only2_filtered.s)))\n",
    "\n",
    "# trial_1_list = trial_1.samples_with_doubletons.collect()\n",
    "# trial_2_list = trial_2.samples_with_doubletons.collect()\n",
    "\n",
    "# trial_1_list == trial_2_list # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_hom_var() - only 1|1 - Evaluate whether the call includes two identical alternate alleles.\n",
    "# is_hom_ref() - only 0|0 - Evaluate whether the call includes two reference alleles.\n",
    "# is_het() - only 0/1 - Evaluate whether the call includes two different alleles.\n",
    "\n",
    "# # samples with doubletons and how many per variant  \n",
    "# p = mt_unrel_only2_filtered.annotate_rows(\n",
    "#     samples_with_doubletons = hl.agg.filter(\n",
    "#         mt_unrel_only2_filtered.GT == hl.call(0, 1), \n",
    "#         hl.agg.collect(mt_unrel_only2_filtered.s)),\n",
    "#     n_doubleton = hl.agg.filter(\n",
    "#         mt_unrel_only2_filtered.GT == hl.call(0, 1), \n",
    "#         hl.agg.count()))\n",
    "\n",
    "# # n_doubleton by sample - maybe can be used for sanity check \n",
    "# k = mt_unrel_only2_filtered.annotate_cols(\n",
    "#     n_doubleton = hl.agg.filter(\n",
    "#         mt_unrel_only2_filtered.GT.is_non_ref(), \n",
    "#         hl.agg.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(s) - if s is a frozenset with a string as the element, this only changes the type into a list (with the string characters intact)\n",
    "# [s] - if s is a frozenset with a string as the element, this creates a list of the characters that make up the string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5aee5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5e6c8",
   "metadata": {},
   "source": [
    "# FST\n",
    "\n",
    "For FST, we are using the data we had prior to running pc_relate (*filtered_n_pruned_output_updated.mt*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the file exported for Lindo - data before running pc_relate and without outliers \n",
    "mt_to_plink = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/whole_dataset.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ecd54",
   "metadata": {},
   "source": [
    "### 1. FST with PLINK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d613fff",
   "metadata": {},
   "source": [
    "#### FOR ZAN -- convert mt to PLINK files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export mt as PLINK2 BED, BIM and FAM files with 'hgdp_tgp' prefix\n",
    "hl.export_plink(mt_to_plink, 'gs://hgdp-1kg/hgdp_tgp/FST_F2/FST/PLINK/hgdp_tgp', fam_id=mt_to_plink.hgdp_tgp_meta.Population)\n",
    "\n",
    "## FST was then calculated using PLINK (with a VM - how to in your offline notebook: FST -> PLINK for Zan) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f65afa",
   "metadata": {},
   "source": [
    "### 2. FST with HAIL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427b548",
   "metadata": {},
   "source": [
    "#### 2a. *Hudson Estimator* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b137298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dataset  \n",
    "mt_for_fst = mt_to_plink\n",
    "\n",
    "# previous code ##########################\n",
    "# same dataset as gs://hgdp-1kg/filtered_n_pruned_output_updated.mt after removing the outliers (gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt)\n",
    "##with hl.utils.hadoop_open('gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt') as file: # read the outliers file into a list\n",
    "    ##outliers = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "##outliers_list = hl.literal(outliers) # capture and broadcast the list as an expression\n",
    "\n",
    "##mt_var_pru_filt = mt_var_pru_filt.filter_cols(~outliers_list.contains(mt_var_pru_filt['s'])) # remove 22 outliers \n",
    "##########################\n",
    "\n",
    "mt_for_fst.count() # double checking - (248634, 4097)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d5a48",
   "metadata": {},
   "source": [
    "##### *pair-wise comparison*\n",
    "\n",
    "Formula to calculate number of pair-wise comparisons = (k * (k-1))/2\n",
    "\n",
    "So in our case, since we have 78 populations, we would expect = (78 * (78-1))/2 = 6006/2 = 3003 pair-wise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = mt_for_fst['hgdp_tgp_meta']['Population'].collect()\n",
    "pop = list(dict.fromkeys(pop)) \n",
    "len(pop) # 78 populations in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "ex = ['a','b','c']\n",
    "# pair-wise comparison \n",
    "ex_pair_com = [[x,y] for i, x in enumerate(ex) for j,y in enumerate(ex) if i<j]\n",
    "ex_pair_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafdce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair-wise comparison - creating list of lists \n",
    "# enumerate gives index values for each population in the 'pop' list (ex. 0 CEU, 1 YRI, 2 LWK ...) and then by \n",
    "# comparing those index values, we create a pair-wise comparison between the populations \n",
    "# i < j so that it only does a single comparison among two different populations \n",
    "# ex. for a comparison between populations CEU and YRI, it only keeps CEU-YRI and discards YRI-CEU, CEU-CEU and YRI-YRI\n",
    "pair_com = [[x,y] for i, x in enumerate(pop) for j,y in enumerate(pop) if i<j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 elements in the list  \n",
    "pair_com[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c259a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check \n",
    "len(pair_com) # 3003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c6643",
   "metadata": {},
   "source": [
    "##### *subset mt into popns according to the pair-wise comparisons and run common variant statistics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9764bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_com[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example - pair_com[0] = ['CEU', 'YRI'] and pair_com[0][0] = 'CEU'\n",
    "CEU_mt = mt_for_fst.filter_cols(mt_for_fst['hgdp_tgp_meta']['Population'] == pair_com[0][0])\n",
    "YRI_mt = mt_for_fst.filter_cols(mt_for_fst['hgdp_tgp_meta']['Population'] == pair_com[0][1])\n",
    "\n",
    "# previous code ##########################\n",
    "##CEU_YRI_mt = mt_for_fst.filter_cols((mt_for_fst['hgdp_tgp_meta']['Population'] == pair_com[0][0]) | (mt_for_fst['hgdp_tgp_meta']['Population'] == pair_com[0][1]))\n",
    "\n",
    "# sanity check \n",
    "##CEU_mt.count()[1] + YRI_mt.count()[1] == CEU_YRI_mt.count()[1] # 176 + 175 = 351\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7769985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run common variant statistics for each population and their combined mt \n",
    "CEU_var = hl.variant_qc(CEU_mt) # individual - 176 \n",
    "YRI_var = hl.variant_qc(YRI_mt) # individual - 175\n",
    "\n",
    "##CEU_YRI_var = hl.variant_qc(CEU_YRI_mt) # total - 351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb499f7",
   "metadata": {},
   "source": [
    "##### *Set up mt table for FST calculation - the next code is run for each population and their combos*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9ee58",
   "metadata": {},
   "source": [
    "##### a. *population 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df345369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop certain fields first to make mt smaller \n",
    "\n",
    "# drop all entry fields\n",
    "# everything except for 's' (key) from the column fields\n",
    "# everything from the row fields except for the keys -'locus' and 'alleles' and row field 'variant_qc'  \n",
    "CEU_interm = CEU_var.drop(*list(CEU_var.entry), *list(CEU_var.col)[1:], *list(CEU_var.row)[2:-1])\n",
    "\n",
    "# only select the row field keys (locus and allele) and row field 'AF' which is under 'variant_qc'\n",
    "CEU_interm2 = CEU_interm.select_rows(CEU_interm['variant_qc']['AF'])  \n",
    "\n",
    "# previous code ##########################\n",
    "# only select the row field keys (locus and allele) and row fields 'AF' & 'AN' which are under 'variant_qc'\n",
    "##CEU_interm2 = CEU_interm.select_rows(CEU_interm['variant_qc']['AF'], CEU_interm['variant_qc']['AN'])  \n",
    "##########################\n",
    "\n",
    "# quick look at the condensed mt \n",
    "CEU_interm2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c740c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include the second entry of the array from the row field 'AF' \n",
    "CEU_interm3 = CEU_interm2.transmute_rows(AF = CEU_interm2.AF[1])\n",
    "\n",
    "# previous code ##########################\n",
    "# key the rows only by 'locus' so that the 'allele' row field can be split into two row fields (one for each allele)\n",
    "# also, only include the second entry of the array from 'AF' row field  \n",
    "##CEU_interm3 = CEU_interm2.key_rows_by('locus')\n",
    "##CEU_interm3 = CEU_interm3.transmute_rows(AF = CEU_interm3.AF[1], A1 = CEU_interm3.alleles[0], A2 = CEU_interm3.alleles[1])\n",
    "##########################\n",
    "\n",
    "# add a row field with population name to keep track of which mt it came from \n",
    "CEU_final = CEU_interm3.annotate_rows(pop = pair_com[0][0])\n",
    "CEU_final.rows().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff98a74",
   "metadata": {},
   "source": [
    "##### b. *population 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fields  \n",
    "\n",
    "# drop all entry fields\n",
    "# everything except for 's' (key) from the column fields\n",
    "# everything from the row fields except for the keys -'locus' and 'alleles' and row field 'variant_qc'  \n",
    "##CEU_YRI_interm = CEU_YRI_var.drop(*list(CEU_YRI_var.entry), *list(CEU_YRI_var.col)[1:], *list(CEU_YRI_var.row)[2:-1])\n",
    "\n",
    "# only select the row field keys (locus and allele) and row fields 'AF' & 'AN' which are under 'variant_qc'\n",
    "##CEU_YRI_interm2 = CEU_YRI_interm.select_rows(CEU_YRI_interm['variant_qc']['AF'], CEU_YRI_interm['variant_qc']['AN'])  \n",
    "\n",
    "# quick look at the condensed mt \n",
    "##CEU_YRI_interm2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d464e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include the second entry of the array from the row field 'AF' \n",
    "##CEU_YRI_interm3 = CEU_YRI_interm2.transmute_rows(AF = CEU_YRI_interm2.AF[1])\n",
    "\n",
    "# previous code ##########################\n",
    "# key the rows only by 'locus' so that the 'allele' row field can be split into two row fields (one for each allele)\n",
    "# also, only include the second entry of the array from 'AF' row field  \n",
    "##CEU_YRI_interm3 = CEU_YRI_interm2.key_rows_by('locus')\n",
    "##CEU_YRI_interm3 = CEU_YRI_interm3.transmute_rows(AF = CEU_YRI_interm3.AF[1], A1 = CEU_YRI_interm3.alleles[0], A2 = CEU_YRI_interm3.alleles[1])\n",
    "##########################\n",
    "\n",
    "# add a row field with population name to keep track of which mt it came from \n",
    "##CEU_YRI_final = CEU_YRI_interm3.annotate_rows(pop = f'{pair_com[0][0]}-{pair_com[0][1]}')\n",
    "##CEU_YRI_final.rows().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a39204",
   "metadata": {},
   "source": [
    "### FST formula pre-setup - trial run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16a9ed",
   "metadata": {},
   "source": [
    "##### *Variables needed for FST calculation* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ce9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting lists into numpy arrarys cause it is easier to work with and more readable\n",
    "\n",
    "# assign populations to formula variables \n",
    "pop1 = CEU_final\n",
    "pop2 = CEU_YRI_final\n",
    "\n",
    "# number of alleles \n",
    "n1 = np.array(pop1.AN.collect())\n",
    "n2 = np.array(pop2.AN.collect())\n",
    "\n",
    "# allele frequencies \n",
    "FREQpop1 = np.array(pop1.AF.collect()) \n",
    "FREQpop2 = np.array(pop2.AF.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d9abc",
   "metadata": {},
   "source": [
    "##### *Weighted average allele frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ = ((n1*FREQpop1) + (n2*FREQpop2)) / (n1+n2)\n",
    "\n",
    "# sanity checks\n",
    "print(((n1[0]*FREQpop1[0]) + (n2[0]*FREQpop2[0])) / (n1[0]+n2[0]) == FREQ[0])\n",
    "print(len(FREQ) == len(FREQpop1)) # length of output should be equal to the length of arrays we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc24f5b",
   "metadata": {},
   "source": [
    "##### *Filter to only freqs between 0 and 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE=(FREQ>0) & (FREQ<1) # only include ave freq between 0 and 1 - started with FREQ = 248634\n",
    "print(np.count_nonzero(INCLUDE)) # 246984 ave freq values were between 0 and 1 - returned True to the conditions above; 248634 - 246984 = 1650 were False \n",
    "\n",
    "# subset allele frequencies \n",
    "FREQpop1=FREQpop1[INCLUDE]\n",
    "FREQpop2=FREQpop2[INCLUDE]\n",
    "FREQ=FREQ[INCLUDE]\n",
    "\n",
    "# sanity check \n",
    "print(len(FREQpop1) == np.count_nonzero(INCLUDE)) # TRUE\n",
    "\n",
    "# subset the number of alleles \n",
    "n1 = n1[INCLUDE]\n",
    "n2 = n2[INCLUDE]\n",
    "\n",
    "# sanity check \n",
    "print(len(n1) == np.count_nonzero(INCLUDE)) # TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb9df9",
   "metadata": {},
   "source": [
    "#### 2b. *W&C ESTIMATOR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b10f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## average sample size that incorporates variance\n",
    "nc =((1/(s-1)) * (n1+n2)) - ((np.square(n1) + np.square(n2))/(n1+n2))\n",
    "\n",
    "msa= (1/(s-1))*((n1*(np.square(FREQpop1-FREQ)))+(n2*(np.square(FREQpop2-FREQ))))\n",
    "\n",
    "msw =  (1/((n1-1)+(n2-1))) * ((n1*(FREQpop1*(1-FREQpop1))) + (n2*(FREQpop2*(1-FREQpop2))))\n",
    "\n",
    "numer = msa-msw\n",
    "\n",
    "denom = msa + ((nc-1)*msw)\n",
    "\n",
    "FST_val = numer/denom\n",
    "\n",
    "# sanity check using the first element \n",
    "nc_0 =((1/(s-1)) * (n1[0]+n2[0])) - ((np.square(n1[0]) + np.square(n2[0]))/(n1[0]+n2[0]))\n",
    "\n",
    "msa_0= (1/(s-1))*((n1[0]*(np.square(FREQpop1[0]-FREQ[0])))+(n2[0]*(np.square(FREQpop2[0]-FREQ[0]))))\n",
    "\n",
    "msw_0 =  (1/((n1[0]-1)+(n2[0]-1))) * ((n1[0]*(FREQpop1[0]*(1-FREQpop1[0]))) + (n2[0]*(FREQpop2[0]*(1-FREQpop2[0]))))\n",
    "\n",
    "numer_0 = msa_0-msw_0\n",
    "\n",
    "denom_0 = msa_0 + ((nc_0-1)*msw_0)\n",
    "\n",
    "FST_0 = numer_0/denom_0\n",
    "\n",
    "print(FST_0 == FST_val[0]) # TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FST_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fde71",
   "metadata": {},
   "source": [
    "### *Which FST value is for which locus-allele?* - actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d14726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting variables for the actual FST run \n",
    "\n",
    "# assign populations to formula variables \n",
    "pop1 = CEU_final\n",
    "pop2 = CEU_YRI_final\n",
    "\n",
    "# number of alleles \n",
    "n1 = np.array(pop1.AN.collect())\n",
    "n2 = np.array(pop2.AN.collect())\n",
    "\n",
    "# allele frequencies \n",
    "FREQpop1 = np.array(pop1.AF.collect()) \n",
    "FREQpop2 = np.array(pop2.AF.collect())  \n",
    "\n",
    "# locus + alleles = keys - needed for reference purposes - these values are uniform across all populations \n",
    "locus = np.array(hl.str(pop1.locus).collect())\n",
    "alleles = np.array(hl.str(pop1.alleles).collect())\n",
    "key = np.array([i + ' ' + j for i, j in zip(locus, alleles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=2   # s is the number of populations - since we are calculating pair-wise FSTs, this is always 2 \n",
    "key_FST = {}\n",
    "for i in range(len(key)):\n",
    "    FREQ = ((n1[i]*FREQpop1[i]) + (n2[i]*FREQpop2[i])) / (n1[i]+n2[i])\n",
    "    \n",
    "    if (FREQ>0) & (FREQ<1): # only include ave freq between 0 and 1\n",
    "        \n",
    "    ## average sample size that incorporates variance\n",
    "        nc = ((1/(s-1)) * (n1[i]+n2[i])) - ((np.square(n1[i]) + np.square(n2[i]))/(n1[i]+n2[i]))\n",
    "\n",
    "        msa= (1/(s-1))*((n1[i]*(np.square(FREQpop1[i]-FREQ)))+(n2[i]*(np.square(FREQpop2[i]-FREQ))))\n",
    "\n",
    "        msw = (1/((n1[i]-1)+(n2[i]-1))) * ((n1[i]*(FREQpop1[i]*(1-FREQpop1[i]))) + (n2[i]*(FREQpop2[i]*(1-FREQpop2[i]))))\n",
    "\n",
    "        numer = msa-msw\n",
    "\n",
    "        denom = msa + ((nc-1)*msw)\n",
    "\n",
    "        FST = numer/denom\n",
    "        \n",
    "        key_FST[key[i]] = FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37907b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ffa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks \n",
    "print(all(np.array(list(key_FST.values())) == FST_val)) # True \n",
    "print(len(key_FST) == len(FST_val)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12a6a3",
   "metadata": {},
   "source": [
    "### *other pair*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec41e7",
   "metadata": {},
   "source": [
    "##### c. population 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ccbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population - YRI\n",
    "# same steps we did to CEU\n",
    "\n",
    "YRI_interm = YRI_var.drop(*list(YRI_var.entry), *list(YRI_var.col)[1:], *list(YRI_var.row)[2:-1])\n",
    "\n",
    "# only select the row field keys (locus and allele) and row fields 'AF' & 'AN' which are under 'variant_qc'\n",
    "YRI_interm2 = YRI_interm.select_rows(YRI_interm['variant_qc']['AF'], YRI_interm['variant_qc']['AN'])  \n",
    "\n",
    "# only include the second entry of the array from the row field 'AF' \n",
    "YRI_interm3 = YRI_interm2.transmute_rows(AF = YRI_interm2.AF[1])\n",
    "\n",
    "# add a row field with population name to keep track of which mt it came from \n",
    "YRI_final = YRI_interm3.annotate_rows(pop = pair_com[0][1])\n",
    "YRI_final.rows().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864ea70",
   "metadata": {},
   "source": [
    "### FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting variables for the actual FST run \n",
    "\n",
    "# assign populations to formula variables \n",
    "pop1 = YRI_final\n",
    "pop2 = CEU_YRI_final\n",
    "\n",
    "# number of alleles \n",
    "n1 = np.array(pop1.AN.collect())\n",
    "n2 = np.array(pop2.AN.collect())\n",
    "\n",
    "# allele frequencies \n",
    "FREQpop1 = np.array(pop1.AF.collect()) \n",
    "FREQpop2 = np.array(pop2.AF.collect())  \n",
    "\n",
    "# locus + alleles = keys - needed for reference purposes - these values are uniform across all populations \n",
    "locus = np.array(hl.str(pop1.locus).collect())\n",
    "alleles = np.array(hl.str(pop1.alleles).collect())\n",
    "key = np.array([i + ' ' + j for i, j in zip(locus, alleles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=2   # s is the number of populations - since we are calculating pair-wise FSTs, this is always 2 \n",
    "key_FST_YRI = {}\n",
    "for i in range(len(key)):\n",
    "    FREQ = ((n1[i]*FREQpop1[i]) + (n2[i]*FREQpop2[i])) / (n1[i]+n2[i])\n",
    "    \n",
    "    if (FREQ>0) & (FREQ<1): # only include ave freq between 0 and 1\n",
    "        \n",
    "    ## average sample size that incorporates variance\n",
    "        nc = ((1/(s-1)) * (n1[i]+n2[i])) - ((np.square(n1[i]) + np.square(n2[i]))/(n1[i]+n2[i]))\n",
    "\n",
    "        msa= (1/(s-1))*((n1[i]*(np.square(FREQpop1[i]-FREQ)))+(n2[i]*(np.square(FREQpop2[i]-FREQ))))\n",
    "\n",
    "        msw = (1/((n1[i]-1)+(n2[i]-1))) * ((n1[i]*(FREQpop1[i]*(1-FREQpop1[i]))) + (n2[i]*(FREQpop2[i]*(1-FREQpop2[i]))))\n",
    "\n",
    "        numer = msa-msw\n",
    "\n",
    "        denom = msa + ((nc-1)*msw)\n",
    "\n",
    "        FST = numer/denom\n",
    "        \n",
    "        key_FST_YRI[key[i]] = FST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df48df",
   "metadata": {},
   "source": [
    "### *three popn pairs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example using three sample pairs ['CEU', 'YRI'], ['CEU', 'LWK'], ['CEU', 'ESN'] and setting up the function \n",
    "example_pairs = pair_com[0:3]\n",
    "\n",
    "ex_dict = {} # empty dictionary to hold final outputs \n",
    "for pairs in example_pairs:\n",
    "    l = [] # empty list to hold the subsetted datasets \n",
    "    l.append(mt_var_pru_filt.filter_cols(mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[0])) # first population \n",
    "    l.append(mt_var_pru_filt.filter_cols(mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[1])) # second population \n",
    "    l.append(mt_var_pru_filt.filter_cols((mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[0]) | (mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[1]))) # first + second = total population\n",
    "    \n",
    "    # sanity check - the sample count of the first and second subset mts should be equal to the total subset mt \n",
    "    if l[0].count()[1] + l[1].count()[1] == l[2].count()[1]: \n",
    "        v = [] # empty list to hold output mts from running common variant statistics \n",
    "        # run common variant statistics for each population and their combined mt\n",
    "        v.append(hl.variant_qc(l[0])) # first population  \n",
    "        v.append(hl.variant_qc(l[1])) # second population \n",
    "        v.append(hl.variant_qc(l[2])) # both/total population\n",
    "        \n",
    "        # add to dictionary \n",
    "        ex_dict[\"-\".join(pairs)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54505733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three mt subsets per comparison pair - set up as a dictionary \n",
    "ex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population - YRI\n",
    "# same steps we did to CEU\n",
    "\n",
    "\n",
    "YRI_var == ex_dict['CEU-YRI'][0]\n",
    "\n",
    "YRI_interm = ex_dict['CEU-YRI'][0].drop(*list(ex_dict['CEU-YRI'][0].entry)\n",
    "\n",
    "\n",
    "YRI_interm = ex_dict['CEU-YRI'][0].drop(*list(ex_dict['CEU-YRI'][0].entry), *list(ex_dict['CEU-YRI'][0].col)[1:], *list(ex_dict['CEU-YRI'][0].row)[2:-1])\n",
    "\n",
    "# only select the row field keys (locus and allele) and row fields 'AF' & 'AN' which are under 'variant_qc'\n",
    "YRI_interm2 = YRI_interm.select_rows(YRI_interm['variant_qc']['AF'], YRI_interm['variant_qc']['AN'])  \n",
    "\n",
    "# only include the second entry of the array from the row field 'AF' \n",
    "YRI_interm3 = YRI_interm2.transmute_rows(AF = YRI_interm2.AF[1])\n",
    "\n",
    "# add a row field with population name to keep track of which mt it came from \n",
    "YRI_final = YRI_interm3.annotate_rows(pop = pairs[0])\n",
    "YRI_final.rows().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as CEU_var['variant_qc'].show(5)\n",
    "ex_dict['CEU-YRI'][0]['variant_qc'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying things out for the main function \n",
    "\n",
    "a = ['CEU-YRI','CEU-LWK', 'CEU-ESN']\n",
    "b = [0,1,2]\n",
    "dc = {}\n",
    "for i in a:\n",
    "    li = []\n",
    "    for j in b:\n",
    "        li.append(str(j) + i)\n",
    "    dc[i] = li \n",
    "    \n",
    "#########################################\n",
    "\n",
    "for i in range(len(v)-1):\n",
    "    print(i)\n",
    "\n",
    "#########################################\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(list)\n",
    "\n",
    "for d in (key_FST, key_FST_YRI):\n",
    "    print(d)\n",
    "    #for key, value in d.items():\n",
    "        #dd[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fe642",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dic = {}\n",
    "for pair in ex_dict.keys(): # for each population pair \n",
    "    u = [] # list to hold updated mts  \n",
    "    for i in range(len(ex_dict[pair])): # for each population (each mt)\n",
    "        # pop1\n",
    "        # drop certain fields and only keep the ones we need \n",
    "        interm = ex_dict[pair][i].drop(*list(ex_dict[pair][i].entry), *list(ex_dict[pair][i].col)[1:], *list(ex_dict[pair][i].row)[2:-1])\n",
    "        interm2 = interm.select_rows(interm['variant_qc']['AF'], interm['variant_qc']['AN'])  \n",
    "        interm3 = interm2.transmute_rows(AF = interm2.AF[1])\n",
    "        #final = interm3.annotate_rows(pop = pair) # keep track of which mt it came from\n",
    "        u.append(interm3) # add updated mt to list \n",
    "    \n",
    "    # variables for FST run \n",
    "\n",
    "    # assign populations to formula variables \n",
    "    pop1 = u[0]\n",
    "    pop2 = u[1]\n",
    "    total = u[2]\n",
    "        \n",
    "    # number of alleles \n",
    "    n1 = np.array(pop1.AN.collect())\n",
    "    n2 = np.array(pop2.AN.collect())\n",
    "    total_n = np.array(total.AN.collect())\n",
    "\n",
    "    # allele frequencies \n",
    "    FREQpop1 = np.array(pop1.AF.collect()) \n",
    "    FREQpop2 = np.array(pop2.AF.collect())\n",
    "    total_FREQ = np.array(total.AF.collect()) \n",
    "    \n",
    "    # locus + alleles = keys - needed for reference purposes during FST calculations - these values are uniform across all populations \n",
    "    locus = np.array(hl.str(pop1.locus).collect())\n",
    "    alleles = np.array(hl.str(pop1.alleles).collect())\n",
    "    key = np.array([i + ' ' + j for i, j in zip(locus, alleles)])\n",
    "    \n",
    "    s=2   # s is the number of populations - since we are calculating pair-wise FSTs, this is always 2 \n",
    "    \n",
    "    # FST pop1 and total popn\n",
    "    key_pop1_total = {}\n",
    "    for i in range(len(key)):\n",
    "        FREQ = ((n1[i]*FREQpop1[i]) + (total_n[i]*total_FREQ[i])) / (n1[i]+total_n[i])\n",
    "\n",
    "        if (FREQ>0) & (FREQ<1): # only include ave freq between 0 and 1\n",
    "\n",
    "        ## average sample size that incorporates variance\n",
    "            nc = ((1/(s-1)) * (n1[i]+total_n[i])) - ((np.square(n1[i]) + np.square(total_n[i]))/(n1[i]+total_n[i]))\n",
    "\n",
    "            msa= (1/(s-1))*((n1[i]*(np.square(FREQpop1[i]-FREQ)))+(total_n[i]*(np.square(total_FREQ[i]-FREQ))))\n",
    "\n",
    "            msw = (1/((n1[i]-1)+(total_n[i]-1))) * ((n1[i]*(FREQpop1[i]*(1-FREQpop1[i]))) + (total_n[i]*(total_FREQ[i]*(1-total_FREQ[i]))))\n",
    "\n",
    "            numer = msa-msw\n",
    "\n",
    "            denom = msa + ((nc-1)*msw)\n",
    "\n",
    "            FST = numer/denom\n",
    "\n",
    "            key_pop1_total[key[i]] = FST\n",
    "            \n",
    "    # FST pop2 and total popn\n",
    "    key_pop2_total = {}\n",
    "    for i in range(len(key)):\n",
    "        FREQ = ((n2[i]*FREQpop2[i]) + (total_n[i]*total_FREQ[i])) / (n2[i]+total_n[i])\n",
    "\n",
    "        if (FREQ>0) & (FREQ<1): # only include ave freq between 0 and 1\n",
    "\n",
    "        ## average sample size that incorporates variance\n",
    "            nc = ((1/(s-1)) * (n2[i]+total_n[i])) - ((np.square(n2[i]) + np.square(total_n[i]))/(n2[i]+total_n[i]))\n",
    "\n",
    "            msa= (1/(s-1))*((n2[i]*(np.square(FREQpop2[i]-FREQ)))+(total_n[i]*(np.square(total_FREQ[i]-FREQ))))\n",
    "\n",
    "            msw = (1/((n2[i]-1)+(total_n[i]-1))) * ((n2[i]*(FREQpop2[i]*(1-FREQpop2[i]))) + (total_n[i]*(total_FREQ[i]*(1-total_FREQ[i]))))\n",
    "\n",
    "            numer = msa-msw\n",
    "\n",
    "            denom = msa + ((nc-1)*msw)\n",
    "\n",
    "            FST = numer/denom\n",
    "\n",
    "            key_pop2_total[key[i]] = FST\n",
    "    \n",
    "    # merge the two FST results together\n",
    "    from collections import defaultdict\n",
    "\n",
    "    dd = defaultdict(list)\n",
    "\n",
    "    for d in (key_pop1_total, key_pop2_total):\n",
    "        for key, value in d.items():\n",
    "            dd[key].append(value)\n",
    "    \n",
    "    final_dic[pair] = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a table \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(final_dic) \n",
    "\n",
    "len(final_dic['CEU-YRI']) # 246984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure what this code is \n",
    "# actual function/run using all population pairs\n",
    "dict = {} # empty dictionary to hold final outputs \n",
    "for pairs in pair_com:\n",
    "    l = [] # empty list to hold the subsetted datasets \n",
    "    l.append(mt_var_pru_filt.filter_cols(mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[0])) # first population \n",
    "    l.append(mt_var_pru_filt.filter_cols(mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[1])) # second population \n",
    "    l.append(mt_var_pru_filt.filter_cols((mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[0]) | (mt_var_pru_filt['hgdp_tgp_meta']['Population'] == pairs[1]))) # first + second = total population\n",
    "    \n",
    "    # sanity check - the sample count of the first and second subset mts should be equal to the total subset mt \n",
    "    if l[0].count()[1] + l[1].count()[1] == l[2].count()[1]: \n",
    "        v = [] # empty list to hold output mts from running common variant statistics \n",
    "        # run common variant statistics for each population and their combined mt\n",
    "        v.append(hl.variant_qc(l[0])) # first population  \n",
    "        v.append(hl.variant_qc(l[1])) # second population \n",
    "        v.append(hl.variant_qc(l[2])) # both/total population\n",
    "        \n",
    "        # add to dictionary \n",
    "        dict[\"-\".join(pairs)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing dictionary element with index \n",
    "ex_dict[list(ex_dict)[0]][0]['variant_qc'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list(ex_dict):\n",
    "    print(ex_dict[l][1]['variant_qc']['AF'][1].show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEU_af_freq = ex_dict[list(ex_dict)[0]][0]['variant_qc']['AF'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_mt = hl.utils.range_matrix_table(0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f448d3",
   "metadata": {},
   "source": [
    "## junk code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is if the alleles were split into their separate columns and if we expect a mismatch across popns \n",
    "\n",
    "# remove indels - only include single letter varients for each allele in both populations \n",
    "# this is b/c the FST formula is set up for single letter alleles \n",
    "#pop1 = CEU_final.filter_rows((CEU_final.A1.length() == 1) & (CEU_final.A2.length() == 1))\n",
    "#pop2 = CEU_YRI_final.filter_rows((CEU_YRI_final.A1.length() == 1) & (CEU_YRI_final.A2.length() == 1))\n",
    "\n",
    "\n",
    "# sanity check \n",
    "#A1 = pop1.A1.collect()\n",
    "#A1 =  list(set(A1)) # OR can also do: \n",
    "### from collections import OrderedDict \n",
    "### A1 = list(OrderedDict.fromkeys(A1))\n",
    "\n",
    "#print(A1) \n",
    "#len(A1) == 4\n",
    "\n",
    "# total # of snps at the beginning - 255666 \n",
    "# unique snps before removing indels - 2712 \n",
    "# total # of snps after removing indels - 221017 (34649 snps were indels for A1, A2 or both)\n",
    "# unique snps after removing indels - 4 ['C', 'A', 'T', 'G'] - which is what we expect \n",
    "\n",
    "\n",
    "\n",
    "## *use the same reference allele - A2 is minor allele here*  \n",
    "\n",
    "# get the minor alleles from both populations  \n",
    "#pop1_A2 = pop1.A2.collect()\n",
    "#pop2_A2 = pop2.A2.collect()\n",
    "\n",
    "\n",
    "# find values that are unequal \n",
    "#import numpy as np\n",
    "#switch1 = (np.array(pop1_A2) != np.array(pop2_A2))\n",
    "#print(switch1.all()) # all comparisons returned 'FALSE' which means that all variants that were compared are the same \n",
    "\n",
    "# sanity check \n",
    "#print(len(pop1_A2) == len(pop2_A2) == len(switch1)) # True \n",
    "\n",
    "\n",
    "### *if there is a variant mismatch among the minor alleles of the two populations*\n",
    "# in case there was a comparison that didn't match correctly among the minor alleles of the two populations, we would adjust the allele frequency(AF) accordingly   \n",
    "#new_frq = pop2.AF.collect() \n",
    "#new_frq = np.array(new_frq) # convert to numpy array for the next step\n",
    "\n",
    "# explanation (with an example) for what this does is right below it \n",
    "#new_frq[switch1] = 1-(new_frq[switch1]) \n",
    "# Example: for pop_1, A1 and A2 are 'T' and 'C' with AF of 0.25 \n",
    "# and for pop_2, A1 and A2 are 'C and 'T' with AF of 0.25\n",
    "# then since the same reference allele is not used (alleles don't correctly align) in this case, \n",
    "# we would subtract the AF of pop_2 from 1, to get the correct allele frequency \n",
    "# the AF of pop_2 with A1 and A2 oriented the same way as pop_1: 'T' and 'C', would be 1-0.25 = 0.75 (w/c is the correct AF)\n",
    "\n",
    "# if we wanted to convert array back to list \n",
    "#pop2_frq = new_frq.tolist() \n",
    "\n",
    "\n",
    "# junk code \n",
    "#pop2.rows().show(5)\n",
    "\n",
    "#p = pop2.filter_rows(str(pop2.locus) =='chr10:38960343')\n",
    "p.row.show()\n",
    "\n",
    "\n",
    "# for i in locus:\n",
    "#     if i =='chr1:94607079':\n",
    "#         print (\"True\")\n",
    "        \n",
    "sum(num == dup for num,dup in zip(locus, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check if there are duplicates in a list and print them out \n",
    "#import collections\n",
    "#dup = [item for item, count in collections.Counter(key).items() if count > 1]\n",
    "#print('Num of duplicate loci: ' + str(len(dup))) \n",
    "#print(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which FST value is for which locus? \n",
    "key_freq1 = {key[i]: FREQpop1[i] for i in range(len(key))}\n",
    "key_freq2 = {key[i]: FREQpop2[i] for i in range(len(key))}\n",
    "\n",
    "\n",
    "key_n1 = {key[i]: n1[i] for i in range(len(key))}\n",
    "key_n2 = {key[i]: n2[i] for i in range(len(key))}\n",
    "\n",
    "# for key,value in zip (locus, FREQpop1):\n",
    "#     print(dict(key, value))\n",
    "#for v1,v2 in zip(list(locus_freq1.values())[0:5], list(locus_freq2.values())[0:5]):\n",
    "    #lq = ((n1*locus_freq1.values()) + (n2*locus_freq2.values())) / (n1+n2)\n",
    "    #print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9918a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locus #220945\n",
    "#len(set(FREQpop1))\n",
    "\n",
    "\n",
    "# check if there are duplicates in locus list and print them out - 72 duplicates  \n",
    "# import collections\n",
    "# d = [item for item, count in collections.Counter(locus).items() if count > 1]\n",
    "\n",
    "# list.sort(locus)\n",
    "#locus\n",
    "\n",
    "# from collections import Counter\n",
    "# [k for k,v in Counter(locus).items() if v>1]\n",
    "\n",
    "# where are each of the duplicated loci located?\n",
    "from collections import defaultdict\n",
    "\n",
    "D = defaultdict(list)\n",
    "for i,item in enumerate(locus):\n",
    "    D[item].append(i)\n",
    "D = {k:v for k,v in D.items() if len(v)>1}\n",
    "locus[6202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_locus = locus[INCLUDE=='FALSE']\n",
    "\n",
    "# ave freq values that were not between 0 and 1 - returned FALSE to the conditions in the above chuck of code \n",
    "print(np.count_nonzero(INCLUDE==0))\n",
    "DONT_INCLUDE= (FREQ=='') & (FREQ>=1)\n",
    "np.count_nonzero(DONT_INCLUDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f41664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output from the preimp_qc module (qced.mt) into a vcf file in Hail \n",
    "import hail as hl \n",
    "mt = hl.read_matrix_table('gs://nepal-geno/GWASpy/Preimp_QC/Nepal_PTSD_GSA_Updated_May2021_qced.mt')\n",
    "hl.export_vcf(mt, 'gs://nepal-geno/Nepal_PTSD_GSA_Updated_May2021_qced.vcf.bgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to figure out which samples were removed\n",
    "s_pre = mt.s.collect()\n",
    "s_post = mt_filt.s.collect()\n",
    "removed_samples = set(s_pre) ^ set(s_post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
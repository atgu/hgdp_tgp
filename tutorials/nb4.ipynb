{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 4: filtering to common independent SNPS, relatedness, PCA, joint calling with new dataset, applying RF on new dataset. Analyses that need to be run:\n",
    "1. Work with Lindo: - *PENDING*\n",
    "    - Joint calling with GGV, sample QC \n",
    "    - Use gnomAD RF (subset to variants in RF model) - doesn’t need VQSR \n",
    "    - Intersect HGDP+1kG+GGV, build a RF with 1kG+ HGDP, apply it to a new dataset (GGV) - doesn’t need VQSR \n",
    "2. PCA plots - Ally - *PENDING*\n",
    "    - already implemented in R, just need to plot it in Hail\n",
    "    \n",
    "----------------------------------------\n",
    "Further edits needed in this nb: \n",
    "- Update the global and subcontinental paths and add them to section 1\n",
    "- Add the path to the PCA plotting Rmarkdown (in section 5) once available  \n",
    "- Complete the table in section 5\n",
    "- Add Ally's code for plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [General Overview](#1.-General-Overview)\n",
    "- [Variant Filtering and LD Pruning](#2.-Variant-Filtering-and-LD-Pruning)\n",
    "- [Run PC-Relate](#3.-Run-PC-Relate)\n",
    "- [PCA](#4.-PCA)\n",
    "    - [Function to Run PCA on Unrelated Individuals](#4a.-Function-to-Run-PCA-on-Unrelated-Individuals)\n",
    "    - [Function to Project Related Individuals](#4b.-Function-to-Project-Related-Individuals)\n",
    "    - [Global PCA](#4c.-Global-PCA)\n",
    "    - [Subcontinental PCA](#4d.-Subcontinental-PCA)\n",
    "- [Outlier Removal](#5.-Outlier-Removal)\n",
    "- [Rerun PCA](#6.-Rerun-PCA)\n",
    "    - [Global PCA](#6a.-Global-PCA)\n",
    "    - [Subcontinental PCA](#6b.-Subcontinental-PCA)\n",
    "- [Write Out Matrix Table](#7.-Write-Out-Matrix-Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General Overview \n",
    "The purpose of this notebook is to further filter the matrix table obtained from notebook 3, run relatedness and Principal Component Analysis (PCA), joint call with new data set, and apply RF. It contains steps on how to:\n",
    "\n",
    "- Read in the a matrix table and run Hail common variant statistics  \n",
    "- Filter using allele frequency and call rate\n",
    "- Run LD pruning \n",
    "- Run relatedness and separate related and unrelated individuals\n",
    "- Calculate PC scores and project samples on to a PC space  \n",
    "- Run global and Subcontinental PCA and plot them \n",
    "- Remove PCA outliers (filter using sample IDs)\n",
    "- Joint call with a new data set\n",
    "- Build/apply RF\n",
    "- Write out a matrix table \n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a. Import needed libraries and packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hail\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b. Input and output path variables to be edited by users as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "input_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt'\n",
    "\n",
    "# intermediate file\n",
    "intermediate_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt'\n",
    "\n",
    "# pre-outlier paths for unrelated and related samples \n",
    "unrel_path = 'gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt'\n",
    "rel_path = 'gs://hgdp-1kg/hgdp_tgp/rel_updated.mt' \n",
    "\n",
    "# pre-outlier file path is missing - global & subcont pca results [here]\n",
    "\n",
    "# outliers file \n",
    "outliers_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "\n",
    "# post-outlier file path is missing - global & subcont pca results[here]\n",
    "\n",
    "# final output paths for unrelated and related samples (post-outlier)\n",
    "unrel_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt'\n",
    "rel_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Variant Filtering and LD Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Why are we doing this? [click here] </summary>\n",
    "    \n",
    "At this point, we have 155,648,020 SNPs and since we need fewer number of variants (~100-300k) for PCA, we filter on:\n",
    "- AF - allele frequency \n",
    "- call rate - fraction of calls neither missing nor filtered\n",
    "\n",
    "and then run LD pruning.     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the right intermediate file \n",
    "mt_filt = hl.read_matrix_table(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. Variant Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Hail's common variant statistics (QC metrics) \n",
    "mt_var = hl.variant_qc(mt_filt) \n",
    "\n",
    "# filter to variants with AF between 0.05 & 0.95, and call rate greater than 0.999    \n",
    "mt_var_filt = mt_var.filter_rows((mt_var.variant_qc.AF[0] > 0.05) & (mt_var.variant_qc.AF[0] < 0.95) & (mt_var.variant_qc.call_rate > 0.999))\n",
    "print('Num of variants after filtering = ' + str(mt_var_filt.count()[0])) # 6787034 snps; this line take ~20min to run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b. LD Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated variants \n",
    "pruned = hl.ld_prune(mt_var_filt.GT, r2=0.1, bp_window_size=500000) # ~113 min to run  \n",
    "mt_var_pru_filt = mt_var_filt.filter_rows(hl.is_defined(pruned[mt_var_filt.row_key])) \n",
    "print('Num of variants after LD pruning = ' + str(mt_var_pru_filt.count()[0])) # 248634 snps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the number of variants is now in the ~100-300k range, we proceed to the PCA analysis without any further adjustments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c. Write out an intermediate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pruning step took a bit of time to run so we have to write out the filtered and pruned mt as an intermediate file\n",
    "mt_var_pru_filt.write(intermediate_path, overwrite=False) # ~23 min to run\n",
    "\n",
    "# read the intermediate file back in for subsequent analyses\n",
    "mt_var_pru_filt = hl.read_matrix_table(intermediate_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - More information on Hail methods and expressions [click here] </summary>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc\"> More on  <i> variant_qc() </i></a>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.ld_prune\"> More on  <i> ld_prune() </i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run PC-Relate   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Why are we doing this? [click here] </summary>\n",
    "    \n",
    "When doing Principal Component Analysis (PCA), we need to separate the related and unrelated samples before computing the PC scores and ploting them. This is because if we compute PCA with the related samples in the data set, the population structure and clustering will be affected by the relatedness that exists among those samples. Thus, we first have to identify the related individuals by computing relatedness estimates (kinship statistic in this case) using a variant of the PC-Relate method in Hail. We used a minimum minor allele frequency (MAF) filter of 0.05, excluded sample pairs with kinship less than 0.05, and used 20 principal components (PC) to control for population structure. After getting the sample ID pairs for the related samples, we then separate the filtered and pruned mt into relateds and unrelateds.     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the kinship statistic\n",
    "relatedness_ht = hl.pc_relate(mt_var_pru_filt.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by() # ~4min to run\n",
    "\n",
    "# identify closely related individuals in pairs (list of sample IDs) \n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False) # ~2hr & 22min to run\n",
    "\n",
    "# subset the filtered and pruned mt to samples that are NOT present in the list of related individuals  \n",
    "mt_unrel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=False) \n",
    "\n",
    "# do the same as above but this time subset to samples that are present in the related-individuals list   \n",
    "mt_rel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the unrelated and related mts since they are used beyond this notebook in other analyses     \n",
    "mt_unrel.write(unrel_path, overwrite=False) # unrelated mt\n",
    "mt_rel.write(rel_path, overwrite=False) # related mt \n",
    "\n",
    "# read the saved mts back in\n",
    "mt_unrel = hl.read_matrix_table(unrel_path) # unrelated mt\n",
    "mt_rel = hl.read_matrix_table(rel_path) # related mt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - More information on Hail methods and expressions [click here] </summary>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate\"> More on  <i> pc_relate() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/misc.html?highlight=maximal_independent_set#hail.methods.maximal_independent_set\"> More on  <i> maximal_independent_set() </i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - What are we doing here? [click here] </summary>\n",
    "    \n",
    "PCA is ran on the unrelated samples first. Then, the related samples are projected onto the PC space of the unrelated samples to get their PC scores. This way the population structure and clustering is not affected by the relatedness among samples.      \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Function to Run PCA on Unrelated Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a data set\n",
    "    :param mt: data set to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_prefix: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_prefix + reg_name + '_scores.txt.bgz')  # save individual-level-genetic-region PCs\n",
    "    pca_loadings.write(out_prefix + reg_name + '_loadings.ht', overwrite)  # save PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Function to Project Related Individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Something to note: [click here] </summary>\n",
    "    \n",
    "If this is being run on Google Cloud, add \"--packages gnomad\" when starting a cluster so that the library import works without an issue.      \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space of unrelated samples \n",
    "    :param project_mt: matrix table of related samples to project  \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param project_prefix: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_prefix + reg_name + '_projected_scores.txt.bgz') # save output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Global PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Why are we doing this? [click here] </summary>\n",
    "    \n",
    "To see the population structure and clustering on a continental level and contextualize the data globally.      \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the unrelated samples\n",
    "run_pca(mt_unrel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/', False)  \n",
    "\n",
    "# read in the PCA loadings of the unrelated samples\n",
    "loadings = hl.read_table('gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/global_loadings.ht') \n",
    "\n",
    "# project the related samples onto the unrelated-samples' PC space \n",
    "project_individuals(loadings, mt_rel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/', False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Subcontinental PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Why are we doing this? [click here] </summary>\n",
    "    \n",
    "To see the population structure and clustering on a subcontinental level and contextualize data within continental regions. This also helped us identify outliers which were removed later on. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Something to note: [click here] </summary>\n",
    " \n",
    "When running the next code chunk, the notebook might freeze after printing the log for EUR, AFR and AMR. If this happens, don't restart it. Just let it run and follow the progress with the outputs being generated. Even after all the outputs have been generated (3 for each region so 21 in total), the code chunk will seem as if it's still running. So after checking that the desired outputs are there, just exit the current notebook, open a new session, and proceed to the next step. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the continental regions in the data set (used the unrelated data set since it had more samples) \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # convert into a list\n",
    "# There are 7 regions: EUR, AFR, AMR, EAS, CSA, OCE, and MID\n",
    "\n",
    "# set argument values for PCA \n",
    "subcont_pca_prefix = 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False\n",
    "\n",
    "# for each region, run PCA on the unrelated samples (~27min to run)\n",
    "for i in regions:  \n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each region, project the related samples onto the unrelated-samples' PC space (~2min to run)\n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # read in the PCA loadings of the unrelated samples for each region \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the related mt per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) # project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - More information on Hail methods and expressions [click here] </summary>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca\"> More on  <i> hwe_normalized_pca() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.annotate_rows\"> More on  <i> annotate_rows() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.annotate\"> More on  <i> annotate() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.transmute\"> More on  <i> transmute() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.export\"> More on  <i>  export() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/experimental/index.html#hail.experimental.pc_project\"> More on  <i> pc_project() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.collect\"> More on  <i> collect() </i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Outlier Removal\n",
    "\n",
    "After plotting the PCs using R (link_the_plotting_Rmarkdown_here), 22 outliers were identified (complete_the_table)\n",
    "\n",
    "| s | Genetic region | Population | Note |\n",
    "| --- | --- | --- | -- |\n",
    "| NA20314 | AFR | ASW | Clusters with AMR in global PCA | \n",
    "| NA20299 | - | - | - |\n",
    "| NA20274 | - | - | - |\n",
    "| HG01880 | - | - | - |\n",
    "| HG01881 | - | - | - |\n",
    "| HG01628 | - | - | - |\n",
    "| HG01629 | - | - | - |\n",
    "| HG01630 | - | - | - |\n",
    "| HG01694 | - | - | - |\n",
    "| HG01696 | - | - | - |\n",
    "| HGDP00013 | - | - | - |\n",
    "| HGDP00150 | - | - | - |\n",
    "| HGDP00029 | - | - | - |\n",
    "| HGDP01298 | - | - | - |\n",
    "| HGDP00130 | CSA | Makrani | Closer to AFR than most CSA |\n",
    "| HGDP01303 | - | - | - |\n",
    "| HGDP01300 | - | - | - |\n",
    "| HGDP00621 | MID | Bedouin | Closer to AFR than most MID |\n",
    "| HGDP01270 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP01271 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP00057 | - | - | - | \n",
    "| LP6005443-DNA_B02 | - | - | - |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the unrelated and related mts to remove outliers and rerun pca  \n",
    "mt_unrel_unfiltered = hl.read_matrix_table(unrel_path) # unrelated mt\n",
    "mt_rel_unfiltered = hl.read_matrix_table(rel_path) # related mt\n",
    "\n",
    "# read the outliers file into a list\n",
    "with hl.utils.hadoop_open(outliers_path) as file: \n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "# capture and broadcast the list as an expression\n",
    "outliers_list = hl.literal(outliers)\n",
    "\n",
    "# remove the 22 outliers from both mts\n",
    "mt_unrel = mt_unrel_unfiltered.filter_cols(~outliers_list.contains(mt_unrel_unfiltered['s']))\n",
    "mt_rel = mt_rel_unfiltered.filter_cols(~outliers_list.contains(mt_rel_unfiltered['s']))\n",
    "\n",
    "# sanity check \n",
    "print('Unrelated: Before outlier removal ' + str(mt_unrel_unfiltered.count()[1]) + ' | After outlier removal ' + str(mt_unrel.count()[1]))\n",
    "print('Related: Before outlier removal: ' + str(mt_rel_unfiltered.count()[1]) + ' | After outlier removal ' + str(mt_rel.count()[1]))\n",
    "num_outliers = (mt_unrel_unfiltered.count()[1] - mt_unrel.count()[1]) + (mt_rel_unfiltered.count()[1] - mt_rel.count()[1])\n",
    "print('Total samples removed = ' + str(num_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - More information on Hail methods and expressions [click here] </summary>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/utils/index.html#hail.utils.hadoop_open\"> More on  <i> hl.utils.hadoop_open() </i></a>\n",
    "    \n",
    "- <a href=\"more info https://hail.is/docs/0.2/functions/core.html#hail.expr.functions.literal\"> More on  <i> hl.literal() </i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Rerun PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - What's different here? [click here] </summary> \n",
    "\n",
    "We are using:\n",
    "- the updated unrelated and related mts (outliers removed)\n",
    "- new paths for the outputs     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Something to note: [click here] </summary>\n",
    " \n",
    "Make sure the code blocks for the PCA (4a) and the projection (4b) functions in section 4 above are run prior to running the following.     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Global PCA (without outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the unrelated samples  \n",
    "run_pca(mt_unrel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/', False)\n",
    "\n",
    "# read in the PCA loadings of the unrelated samples  \n",
    "loadings = hl.read_table('gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/global_loadings.ht') \n",
    "\n",
    "# project the related samples onto the unrelated-samples' PC space \n",
    "project_individuals(loadings, mt_rel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/', False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Subcontinental PCA (without outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values for PCA \n",
    "subcont_pca_prefix = 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False \n",
    "\n",
    "# if not done so already, read \"Something to note\" in section 4d above before running the following code \n",
    "# for each region, run PCA on the unrelated samples (~26 min to run) \n",
    "for i in regions: # \"regions\" is a list containing the 7 continental regions found in the data set. It comes from the code chunk 4d above.    \n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each region, project the related samples onto the unrelated-samples' PC space (~3min to run)\n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # read in the PCA loadings of the unrelated samples for each region\n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the relateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) # project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Write Out Matrix Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the updated unrelated and related mts separately (post-outlier removal) \n",
    "mt_unrel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt', overwrite=False) # unrelated mt \n",
    "mt_rel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt', overwrite=False) # related mt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

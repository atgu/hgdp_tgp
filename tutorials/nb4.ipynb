{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21949f31",
   "metadata": {},
   "source": [
    "# Computing Population Genetics Statistics (F<sub>2</sub> and F<sub>ST</sub>)\n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0999ebd",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Setting Default Paths](#1.-Set-Default-Paths)\n",
    "2. [F<sub>2</sub> analysis](#2.-F<sub>2</sub>-Analysis)\n",
    "3. [F<sub>ST</sub>](#3.-F_st)\n",
    "    1. [F<sub>ST</sub> with PLINK](#3a.-F_st-with-PLINK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1175b",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "\n",
    "The purpose of this notebook is to show two common population genetics analyses (F<sub>2</sub> and F<sub>ST</sub>) used to understand recent and deep history. F<sub>2</sub> analyses compute the number of SNVs that appear twice in a dataset and compares how often they are shared among individuals. Since doubletons are rare variants, they tend to have arisen relatively recently, so give us information about recent population history. In contrast, F<sub>ST</sub> is a “fixation index” which calculates the extent of variation within versus between populations using SNVs of many frequencies. Because common variants are used in F<sub>ST</sub> analyses which arose a long time ago, this gives us information about older population history.\n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in a matrix table (mt) and filter using sample IDs that were obtained from another matrix table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a matrix table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain fields\n",
    "- Download a tool such as PLINK using a link and shell command \n",
    "- Run shell commands, and set up a script & run it from inside a code block \n",
    "- Calculate F<sub>st</sub> in PLINK (once there is progress on this, I will elaborate more) \n",
    "- Go through a log file and extract needed information\n",
    "- Write out results onto the Cloud \n",
    "- Plot certain fields from the Matrix Table:\n",
    "    - Heterozygosity (plot distribution to show how it doesn’t work with diverse populations as expected (e.g. high for AFR, low for FIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef84e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdebbe",
   "metadata": {},
   "source": [
    "# 1. Set Default Paths\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets. \n",
    "\n",
    "By default all of the write sections are shown as markdown cells. If you would like to write out your own datasets, you can copy the code and paste it into a new code cell. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2855c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file for F2 analysis - we use the mt that was produced in nb1 \n",
    "f2_input_path = 'gs://hgdp-1kg/tutorial_datasets/metadata_and_qc/post_qc.mt'\n",
    "\n",
    "# Unrelated samples mt without outliers -  for subsetting purposes\n",
    "unrelateds_path = 'gs://hgdp-1kg/tutorial_datasets/pca_results/unrelateds_without_outliers.mt'\n",
    "\n",
    "# Final count table for the F2 analysis \n",
    "final_doubleton_count_path = 'gs://hgdp-1kg/tutorial_datasets/F2_FST/F2/doubleton_count.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file for F_st analysis - mt generated in nb2 after LD pruning\n",
    "fst_input_path = 'gs://hgdp-1kg/tutorial_datasets/pca_preprocessing/ld_pruned.mt'\n",
    "\n",
    "# Path for exporting the PLINK files \n",
    "# Include file prefix at the end of the path - here the prefix is 'hgdp_tgp'\n",
    "plink_files_path = 'gs://hgdp-1kg/tutorial_datasets/F2_FST/FST/PLINK/hgdp_tgp'\n",
    "\n",
    "# Final F_st output  \n",
    "final_mean_fst_path = 'gs://hgdp-1kg/tutorial_datasets/F2_FST/FST/PLINK/mean_fst.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1f621",
   "metadata": {},
   "source": [
    "# 2. F<sub>2</sub> Analysis\n",
    "<br>\n",
    "\n",
    "The file being imported here has:\n",
    "<ul>\n",
    "<li>been run through gnomAD's sample QC filters</li> \n",
    "<li>been run through gnomAD's variant QC filters</li>\n",
    "<li>not been filtered on allele frequency and call rate</li>\n",
    "<li>not been LD pruned</li>\n",
    "</ul>\n",
    "\n",
    "We are running F<sub>2</sub> on unrelated samples only and using the data generated after removing outliers and rerunning PCA (<a href=\"https://nbviewer.org/github/atgu/hgdp_tgp/blob/master/tutorials/nb4.ipynb\">nb4</a>) for subsetting. After obtaining the desired samples, we run Hail's common variant statistics so we can separate out doubletons*. Once we have the doubletons filtered, we then remove variants with a call rate less than 0.05 (no more than 5% missingness/low missingness).\n",
    "\n",
    "*Doubletons are variants that show up twice and only twice in a data set and useful in detecting rare variance & understanding recent history.\n",
    "\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43812da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk took 20min to run \n",
    "\n",
    "# Read-in intermediate file \n",
    "mt_filt = hl.read_matrix_table(f2_input_path)\n",
    "\n",
    "# Filter to only unrelated samples - 3378 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt \n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_filt_unrel.count()[1])) # 3378 samples\n",
    "\n",
    "# Run common variant statistics (quality control metrics)  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# Separate the AC array into individual fields and extract the doubletons  \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "print('Num of variants that are doubletons = ' + str(mt_unrel_only2.count()[0])) # 17279480 variants \n",
    "\n",
    "# Remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "print('Num of variants > 0.05 = ' + str(mt_unrel_only2_filtered.count()[0])) # 17229743 variants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70803a2b",
   "metadata": {},
   "source": [
    "The next step is to check which allele, reference (ref) or alternate (alt), is the the doubleton. If the first element of the array in the allele frequency field (AF[0]) is less than the second elelement (AF[1]), then the doubleton is 1st allele (ref). If the first element (AF[0]) is greater than the second elelement (AF[1]), then the doubleton is 2nd allele (alt).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk took 44min to run because of the print commands \n",
    "\n",
    "# Check allele frequency (AF) to see if the doubleton is the ref or alt allele \n",
    "\n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print('Num of variants where the 1st allele (ref) is the doubleton = ' + str(mt_doubl_ref.count()[0])) # 2979 variants\n",
    "\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print('Num of variants where the 2nd allele (alt) is the doubleton = ' + str(mt_doubl_alt.count()[0])) # 17226764 variants\n",
    "\n",
    "# Validity check, should print True\n",
    "#mt_doubl_ref.count()[0] + mt_doubl_alt.count()[0] == mt_unrel_only2_filtered.count()[0] # True\n",
    "#print(3159 + 17994582 == 17997741) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62681f10",
   "metadata": {},
   "source": [
    "Once we have figured out which allele is the doubleton and divided the doubleton Matrix Table accordingly, the next step is to find the samples that have doubletons, compile them in a set, and annotate that onto the mt as a new row field. This done for each mt separately and can be achieved by looking at the genotype call (GT) field. When the doubleton is 1st allele (ref), the genotype call would be 0|1 & 0|0. When the doubleton is 2nd allele (alt), the genotype call would be 0|1 & 1|1. We chose a set for the results instead of a list because a list isn't hashable and the next step wouldn't have run. After the annotation of the new row field in each mt, we then count how many times a sample or a sample pair appears within that field and store the results in a dictionary. Once we have the two dictionaries (one for the ref and one for the alt), we merge them into one and add up the values for identical keys.\n",
    "\n",
    "If you want to do a validity check at this point, you can add up the count of the two dictionaries and then subtract the number of keys that intersect between the two. The value that you get should be equal to the length of the combined dictionary.  \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk took 21min to run\n",
    "\n",
    "# For each mt, find the samples that have doubletons, compile them in a set, and add as a new row field \n",
    "\n",
    "# Doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# If there is one sample in the new column field then it's 0|0. If there are two samples, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1))| (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "# Doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# If there is one sample in the new column field then it's 1|1. If there are two samples, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1))| (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n",
    "\n",
    "# Count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field - returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))\n",
    "\n",
    "# Combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "print('Length of dictionary = ' + str(len(all_doubl_count))) # 2989787 \n",
    "\n",
    "# Validity check \n",
    "#len(all_doubl_count) == (len(ref_doubl_count) + len(alt_doubl_count)) - # of keys that intersect b/n the two dictionaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253d358",
   "metadata": {},
   "source": [
    "For the next step, we get a list of samples that are in the doubleton mt and also create sample pairs out of them. We also divide the combined dictionary into two: one for when a sample is a key by itself (len(key) == 1) and the other for when the dictionary key is a pair of samples (len(key) != 1). We then go through the lists of samples obtained from the mt and see if any of them are keys in their respective doubleton dictionaries - list of samples by themselves is compared against the dictionary that has a single sample as a key and the list with sample pairs is compared against the dictionary where the key is a pair of samples. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of samples from mt - 3378 samples \n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# Make pairs from sample list: n(n-1)/2 - 5703753 pairs \n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(mt_sample_list) for j,y in enumerate(mt_sample_list) if i<j]\n",
    "\n",
    "# Subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "# subset dict to keys with sample pairs (not just 1)\n",
    "dict_pair_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) != 1}\n",
    "\n",
    "# Validity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True\n",
    "# Are the samples in the list the same as the dict keys?\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True \n",
    "# Are the sample pairs obtained from the mt equal to what's in the pair dict? \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from the mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd6529",
   "metadata": {},
   "source": [
    "If a single sample is a key in the single-sample-key dictionary, we record the sample ID twice and it's corresponding value from the dictionary. If it is not a key, we record the sample ID twice and set the value to 0. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample comparison \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# Validity check \n",
    "# For the single samples, the length should be consistent across dict, mt sample list, and final list\n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd34f6",
   "metadata": {},
   "source": [
    "If a sample pair is a key in the sample-pair-key dictionary, we record the two sample IDs and the corresponding value from the dictionary. If that is not the case, we record the two sample IDs and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceea59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pair comparison\n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# Validity check \n",
    "# Length of final list should be equal to the length of the sample list obtained from the mt \n",
    "print(len(sample_pair_final_list) == len(mt_sample_pairs)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e237c",
   "metadata": {},
   "source": [
    "Last step is to combine the two lists obtained from the comparisons, convert that into a pandas table, format it as needed, and write it out as a csv so that the values can be plotted as a heat map in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# Validity check \n",
    "len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list) # True\n",
    "\n",
    "# Format list as pandas table \n",
    "df = pd.DataFrame(final_list)\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) # rename column names \n",
    "\n",
    "# Write out table to the Cloud so it can be plotted in R \n",
    "df.to_csv(final_doubleton_count_path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ae0a9",
   "metadata": {},
   "source": [
    "### The sample-level F<sub>2</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/F2_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a38049",
   "metadata": {},
   "source": [
    "# 3. F<sub>st</sub>\n",
    "\n",
    "\n",
    "F<sub>st</sub> detects genetic divergence from common variance allowing us to understand past deep history. Although we are running F<sub>st</sub> only on unrelated samples in this tutorial, unlike the F<sub>2</sub> analysis, here we are using the filtered and pruned data set that doesn't include the 24 outliers (<code>whole_dataset.mt = filtered_n_pruned_output_updated.mt - 24 outliers</code>).\n",
    "\n",
    "Something to note: Since the mt we are starting with is filtered and pruned, running <code>hl.variant_qc</code> and filtering to variants with <code>call rate > 0.05 </code> (similar to what we did for the F<sub>2</sub> analysis) doesn't make a difference to the number of variants.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the filtered and pruned mt with outliers - 199974 variants and 4120 samples\n",
    "mt_FST_initial = hl.read_matrix_table(fst_input_path) \n",
    "\n",
    "# Filter to only unrelated samples - 3378 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_FST_unrel = mt_FST_initial.filter_cols(unrel_samples.contains(mt_FST_initial['s'])) # filter mt\n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_FST_unrel.count()[1])) # 3378 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e465364",
   "metadata": {},
   "source": [
    "### 3a. F<sub>ST</sub> with PLINK\n",
    "\n",
    "In order to calculate F<sub>ST</sub> using PLINK, we first need to export mt as PLINK files.\n",
    "\n",
    "After exporting the files to PLINK format, the rest of the analysis is done using shell commands within the notebook. \n",
    "\n",
    "**Place <code>!</code> before the command you want to run and proceed as if you are running codes in a terminal.** You can use <code>! ls</code> after each run to check for ouputs in the directory and see if commands have run correctly. \n",
    " \n",
    "**Every time you start a new cluster, you will need to download PLINK to run the F<sub>ST</sub> analysis since downloads and files are discarded when a cluster is stopped.** \n",
    "\n",
    "Something to note: when running the notebook on the Cloud, the shell commands still run even if we didn't use <code>!</code>. Not sure why but will check if that is also the case when running the notebook locally. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export mt as PLINK2 BED, BIM and FAM files - store on the Cloud -  06\n",
    "hl.export_plink(mt_FST_unrel, plink_files_path, fam_id=mt_FST_unrel.hgdp_tgp_meta.population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47bc8f",
   "metadata": {},
   "source": [
    "3a.1. PLINK Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PLINK using a link from the PLINK website (linux - recent version - 64 bit - stable) \n",
    "! wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
    "    \n",
    "# Unzip the \".gz\" file: \n",
    "! unzip plink_linux_x86_64_20210606.zip\n",
    "\n",
    "# A documentation output when you run this command indicates that PLINK has been installed properly \n",
    "! ./plink "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fb234",
   "metadata": {},
   "source": [
    "3a.2. Files Setup \n",
    "\n",
    "Because F<sub>ST</sub> is computed among groups, we need to create a list of all pairs of populations.\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef845208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the PLINK files that are stored on the Cloud into the current session directory\n",
    "! gsutil cp {plink_files_path}.fam . # fam \n",
    "! gsutil cp {plink_files_path}.bim . # bim\n",
    "! gsutil cp {plink_files_path}.bed . # bed\n",
    "\n",
    "# Obtain FID - in this case, it is the 78 populations in the first column of the FAM file\n",
    "! awk '{print $1}' hgdp_tgp.fam | sort -u > pop.codes\n",
    "\n",
    "# Make all possible combinations of pairs using the 78 populations \n",
    "! for i in `seq 78`; do for j in `seq 78`; do if [ $i -lt $j ]; then VAR1=`sed \"${i}q;d\" pop.codes`; VAR2=`sed \"${j}q;d\" pop.codes`; echo $VAR1 $VAR2; fi; done; done > pop.combos\n",
    "\n",
    "# Validity check \n",
    "! wc -l pop.combos # 3003\n",
    "\n",
    "# Create directories for intermediate files and F_Sst results \n",
    "! mkdir within_files # intermediate files\n",
    "! mkdir FST_results # F_st results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b72fdc",
   "metadata": {},
   "source": [
    "3a.3. Scripts Setup \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85131655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 1 ####\n",
    "\n",
    "# For each population pair, set up a bash script to create a \"within\" file and run F_st \n",
    "# Files to be produced: \n",
    "#    [pop_pairs].within will be saved in the \"within_files\" directory\n",
    "#    [pop_pairs].fst, [pop_pairs].log, and [pop_pairs].nosex will be save in \"FST_results\" directory\n",
    "\n",
    "fst_script = '''    \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "\n",
    "# create \"within\" files for each population pair using the FAM file (columns 1,2 and 1 again)\n",
    "    awk -v r1=$POP1 -v r2=$POP2 '$1 == r1 || $1 == r2' hgdp_tgp.fam | awk '{ print $1, $2, $1 }' > within_files/${POP1}_${POP2}.within\n",
    "\n",
    "# run F_st\n",
    "    ./plink --bfile hgdp_tgp --fst --within within_files/${POP1}_${POP2}.within --out FST_results/${POP1}_${POP2}\n",
    "done'''\n",
    "\n",
    "with open('run_fst.py', mode='w') as file:\n",
    "    file.write(fst_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 2 ### - script 1 has to be run for this script to run \n",
    "\n",
    "# Use the \"[pop_pairs].log\" file produced from script 1 above (located in \"FST_results\" directory) to get the \"Mean F_st estimate\" for each population pair \n",
    "# And compile all of the values in a single file for F_st heat map generation\n",
    "\n",
    "extract_mean_script = ''' \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "    mean_FST=$(tail -n4 FST_results/${POP1}_${POP2}.log | head -n 1 | awk -F: '{print $2}' | awk '{$1=$1};1')\n",
    "    printf \"%-20s\\t%-20s\\t%-20s\\n \" ${POP1} ${POP2} $mean_FST >> mean_fst_sum.txt\n",
    "done'''\n",
    "\n",
    "with open('extract_mean.py', mode='w') as file:\n",
    "    file.write(extract_mean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ce349",
   "metadata": {},
   "source": [
    "3a.4. Run Scripts \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 1 and direct the run log into a file (~20min to run) \n",
    "! sh run_fst.py > fst_script.log\n",
    "\n",
    "# Validity check \n",
    "! cd within_files/; ls | wc -l # 3003\n",
    "! cd FST_results/; ls | wc -l # 3003 * 3 = 9009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 2; requires script 1 to be run first (~ 1min to run)\n",
    "! sh extract_mean.py \n",
    "\n",
    "# Copy script 2 output to the Cloud for heat map plotting in R \n",
    "! gsutil cp mean_fst_sum.txt {final_mean_fst_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dc61b",
   "metadata": {},
   "source": [
    "### The population-level F<sub>ST</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/FST_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ee735e",
   "metadata": {},
   "source": [
    "Notebook 5: F2 and F_st\n",
    "\n",
    "1. F_st in Hail if there is time\n",
    "2. Heat maps for this analysis were plotted using R - will be kept that way\n",
    "    - once you edit and organize the R codes, link them here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921c368",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Setting Default Output Paths](#1.-Set-Default-Output-Paths)\n",
    "2. [F2 analysis](#2.-F2-analysis)\n",
    "3. [FST](#3.-F_st)\n",
    "    1. [FST with PLINK](#3a.-F_st-with-PLINK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df87f6e",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "The purpose of this notebook is to show two population genetics analyses (F2 and FST) to understand recent and deep history. \n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in a matrix table (mt) and filter using sample IDs that were obtained from another matrix table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a matrix table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain fields\n",
    "- Download a tool such as PLINK using a link and shell command \n",
    "- Run shell commands, and set up a script & run it from inside a code block \n",
    "- Calculate F_st in PLINK (once there is progress on this, I will elaborate more) \n",
    "- Go through a log file and extract needed information\n",
    "- Write out results onto the Cloud \n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "\n",
    "# import the read_qc function\n",
    "# tmp: this is commented out as the function will continue to change\n",
    "#from read_qc_function import read_qc\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd76bef",
   "metadata": {},
   "source": [
    "## Set Requester Pays Bucket\n",
    "Running through these tutorials, users must specify which project is to be billed. To change which project is billed, set the `GCP_PROJECT_NAME` variable to your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f26877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting requester pays bucket to use throughout tutorial\n",
    "GCP_PROJECT_NAME = \"diverse-pop-seq-ref\" # change this to your project name\n",
    "hl.init(spark_conf={\n",
    "    'spark.hadoop.fs.gs.requester.pays.mode': 'CUSTOM',\n",
    "    'spark.hadoop.fs.gs.requester.pays.buckets': 'hgdp_tgp,gcp-public-data--gnomad',\n",
    "    'spark.hadoop.fs.gs.requester.pays.project.id': GCP_PROJECT_NAME\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4a1ea",
   "metadata": {},
   "source": [
    "### tmp read_qc function \n",
    "to be removed once tutorials & function are complete and we can troubleshoot importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_qc(\n",
    "        default: bool = False,\n",
    "        post_qc:bool = False,\n",
    "        sample_qc: bool = False,\n",
    "        variant_qc: bool = False,\n",
    "        duplicate: bool = False,\n",
    "        outlier_removal: bool = False,\n",
    "        ld_pruning: bool = False,\n",
    "        rel_unrel: str = 'default') -> hl.MatrixTable:\n",
    "    \"\"\"\n",
    "    Wrapper function to get HGDP+1kGP data as Matrix Table at different stages of QC/filtering.\n",
    "    By default, returns pre QC MatrixTable with qc filters annotated but not filtered.\n",
    "\n",
    "    :param bool default: if True will preQC version of the dataset\n",
    "    :param bool post_qc: if True will return a post QC matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - outlier removal\n",
    "    :param bool sample_qc: if True will return a post sample QC matrix table\n",
    "    :param bool variant_qc: if True will return a post variant QC matrix table\n",
    "    :param bool duplicate: if True will return a matrix table with duplicate samples removed\n",
    "    :param bool outlier_removal: if True will return a matrix table with PCA outliers and duplicate samples removed\n",
    "    :param bool ld_pruning: if True will return a matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - LD pruning\n",
    "        - additional variant filtering\n",
    "    :param bool rel_unrel: default will return same mt as ld pruned above\n",
    "        if 'all' will return the same matrix table as if ld_pruning is True\n",
    "        if 'related_pre_outlier' will return a matrix table with only related samples pre pca outlier removal\n",
    "        if 'unrelated_pre_outlier' will return a matrix table with only unrelated samples pre pca outlier removal\n",
    "        if 'related_post_outlier' will return a matrix table with only related samples post pca outlier removal\n",
    "        if 'unrelated_post_outlier' wil return a matrix table with only unrelated samples post pca outlier removal\n",
    "    \"\"\"\n",
    "    # Reading in all the tables and matrix tables needed to generate the pre_qc matrix table\n",
    "    sample_meta = hl.import_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv')\n",
    "    sample_qc_meta = hl.read_table('gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht')\n",
    "    dense_mt = hl.read_matrix_table(\n",
    "        'gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_dense.mt')\n",
    "    \n",
    "    dense_mt = dense_mt.naive_coalesce(5000)\n",
    "\n",
    "\n",
    "    # Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "    def dict_to_struct(d):\n",
    "        fields = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                v = dict_to_struct(v)\n",
    "            fields[k] = v\n",
    "        return hl.struct(**fields)\n",
    "\n",
    "    # un-flattening a hail table with nested structure\n",
    "    # dict to hold struct names as well as nested field names\n",
    "    d = {}\n",
    "\n",
    "    # Getting the row field names\n",
    "    row = sample_meta.row_value\n",
    "\n",
    "    # returns a dict with the struct names as keys and their inner field names as values\n",
    "    for name in row:\n",
    "        def recur(dict_ref, split_name):\n",
    "            if len(split_name) == 1:\n",
    "                dict_ref[split_name[0]] = row[name]\n",
    "                return\n",
    "            existing = dict_ref.get(split_name[0])\n",
    "            if existing is not None:\n",
    "                assert isinstance(existing, dict), existing\n",
    "                recur(existing, split_name[1:])\n",
    "            else:\n",
    "                existing = {}\n",
    "                dict_ref[split_name[0]] = existing\n",
    "                recur(existing, split_name[1:])\n",
    "        recur(d, name.split('.'))\n",
    "\n",
    "    # using the dict created from flattened struct, creating new structs now un-flattened\n",
    "    sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "    sample_meta = sample_meta.key_by('s')\n",
    "\n",
    "    # grabbing the columns needed from HGDP metadata\n",
    "    new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "    # creating a table with gnomAD sample metadata and HGDP metadata\n",
    "    ht = sample_qc_meta.annotate(**new_meta[sample_qc_meta.s])\n",
    "\n",
    "    # stripping 'v3.1::' from the names to match with the densified MT\n",
    "    ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))\n",
    "\n",
    "    # Using hl.annotate_cols() method to annotate the gnomAD variant QC metadata onto the matrix table\n",
    "    mt = dense_mt.annotate_cols(**ht[dense_mt.s])\n",
    "    \n",
    "\n",
    "    print(f\"sample_qc: {sample_qc}\\nvariant_qc: {variant_qc}\\nduplicate: {duplicate}\" \\\n",
    "          f\"\\noutlier_removal: { outlier_removal}\\nld_pruning: {ld_pruning}\\nrel_unrel: {rel_unrel}\")\n",
    "    \n",
    "    if default:\n",
    "        print(\"Returning default preQC matrix table\")\n",
    "        # returns preQC dataset\n",
    "        return mt\n",
    "    \n",
    "    if post_qc:\n",
    "        print(\"Returning post sample and variant QC matrix table with duplicates and PCA outliers removed\")\n",
    "        sample_qc = True\n",
    "        variant_qc = True\n",
    "        duplicate = True\n",
    "        outlier_removal = True\n",
    "    \n",
    "    if sample_qc:\n",
    "        print(\"Running sample QC\")\n",
    "        # run data through sample QC\n",
    "        # filtering samples to those who should pass gnomADs sample QC\n",
    "        # this filters to only samples that passed gnomad sample QC hard filters\n",
    "        mt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "        # annotating partially filtered dataset with variant metadata\n",
    "        mt = mt.annotate_rows(**dense_mt[mt.locus, mt.alleles])\n",
    "\n",
    "    if variant_qc:\n",
    "        print(\"Running variant QC\")\n",
    "        # run data through variant QC\n",
    "        # Subsetting the variants in the dataset to only PASS variants (those which passed gnomAD's variant QC)\n",
    "        # PASS variants are variants which have an entry in the filters field.\n",
    "        # This field contains an array which contains a bool if any variant qc filter was failed\n",
    "        # This is the last step in the QC process\n",
    "        mt = mt.filter_rows(hl.len(mt.filters) != 0, keep=False)\n",
    "\n",
    "    if duplicate:\n",
    "        print(\"Removing any duplicate samples\")\n",
    "        # Removing any duplicates in the dataset using hl.distinct_by_col() which removes\n",
    "        # columns with a duplicate column key. It keeps one column for each unique key.\n",
    "        # after updating to the new dense_mt, this step is no longer necessary to run\n",
    "        mt = mt.distinct_by_col()\n",
    "\n",
    "    if outlier_removal:\n",
    "        print(\"Removing PCA outliers\")\n",
    "        # remove PCA outliers and duplicates\n",
    "        # reading in the PCA outlier list\n",
    "        # To read in the PCA outlier list, first need to read the file in as a list\n",
    "        # using hl.hadoop_open here which allows one to read in files into hail from Google cloud storage\n",
    "        pca_outlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "        with hl.utils.hadoop_open(pca_outlier_path) as file:\n",
    "            outliers = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "        # Using hl.literal here to convert the list from a python object to a hail expression so that it can be used\n",
    "        # to filter out samples\n",
    "        outliers_list = hl.literal(outliers)\n",
    "\n",
    "        # Using the list of PCA outliers, using the ~ operator which is a negation operator and obtains the compliment\n",
    "        # In this case the compliment is samples which are not contained in the pca outlier list\n",
    "        mt = mt.filter_cols(~outliers_list.contains(mt['s']))\n",
    "\n",
    "    if ld_pruning:\n",
    "        print(\"Returning ld pruned post variant and sample QC matrix table pre PCA outlier removal \")\n",
    "        # read in dataset which has additional variant filtering and ld pruning run\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt')\n",
    "\n",
    "    if rel_unrel == \"default\":\n",
    "        # do nothing\n",
    "        # created a default value because there are multiple options for rel/unrel datasets\n",
    "        mt = mt\n",
    "\n",
    "    elif rel_unrel == 'related_pre_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only related individuals\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/rel_updated.mt')\n",
    "\n",
    "        \n",
    "    elif rel_unrel == 'unrelated_pre_outlier':\n",
    "        print(\"Returning post QC matrix table with only unrelated individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only unrelated individuals\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt')\n",
    "\n",
    "\n",
    "    elif rel_unrel == 'related_post_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only related individuals\n",
    "        #   - PCA outlier removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt')\n",
    "\n",
    "\n",
    "    elif rel_unrel == 'unrelated_pst_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only unrelated individuals\n",
    "        #   - PCA outlier removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt')\n",
    "        \n",
    "    # Calculating both variant and sample_qc metrics on the mt before returning\n",
    "    # so the stats are up to date with the version being written out\n",
    "    mt = hl.sample_qc(mt)\n",
    "    mt = hl.variant_qc(mt)\n",
    "    \n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ea66",
   "metadata": {},
   "source": [
    "# 1. Set Default Output Paths\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets. The read_qc() function is intended to take the place of needing to write out and read in datasets by the user. \n",
    "\n",
    "By default we have commented out all of the write steps of the tutorials, if you would like to write out your own datasets, uncomment those sections and replace the paths with your own. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate file - beginning input file for F2 analysis \n",
    "f2_input_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt'\n",
    "\n",
    "# unrelated samples mt - for subsetting purposes \n",
    "unrelated_path = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt'\n",
    "\n",
    "# final F2 output \n",
    "f2_final_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton_sample_pair_count_tbl.csv'\n",
    "\n",
    "# annotation file for F2 heat map \n",
    "f2_annotation_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/sampleID_pop_reg.txt'\n",
    "\n",
    "# beginning input file for F_st analysis \n",
    "fst_input_path = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/whole_dataset.mt'\n",
    "\n",
    "# path for exporting the PLINK files \n",
    "# include file prefix at the end of the path - here the prefix is 'hgdp_tgp'\n",
    "plink_files_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/FST/PLINK/unrel/hgdp_tgp'\n",
    "\n",
    "# final F_st output  \n",
    "fst_final_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/FST/PLINK/mean_fst_sum_UPDATED.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e9546",
   "metadata": {},
   "source": [
    "# 2. F2 analysis\n",
    "\n",
    "The intermediate file being imported here is the file generated prior to any variant filter and LD pruning. We are running F2 on unrelated samples only and using the version generated after removing outliers and rerunning PCA (nb4) for subsetting. After obtaining the desired samples, we run Hail's common variant statistics so we can separate out doubletons - variants that shows up twice and only twice in a data set and useful in detecting rare variance & understanding recent history. Once we have the doubletons filtered, we then remove variants with call rate less than 0.05 (no more than 5% missingness/low missingness).\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in intermediate file \n",
    "mt_filt = read_qc(post_qc=True)\n",
    "\n",
    "# filter to only unrelated samples - 3380 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelated_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt \n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_filt_unrel.count()[1])) # 3380 samples\n",
    "\n",
    "# run common variant statistics (quality control metrics)  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# separate the AC array into individual fields and extract the doubletons  \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "print('Num of variants that are doubletons = ' + str(mt_unrel_only2.count()[0])) # 18018978 variants \n",
    "\n",
    "# write out an intermediate file and read it back in (saving took ~23 min) - MIGHT NOT NEED THIS LATER (DECIDE ONCE RUN WITH THE NEW DS)\n",
    "#mt_unrel_only2.write('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt', overwrite=False)\n",
    "#mt_unrel_only2 = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt')\n",
    "\n",
    "# remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "print('Num of variants > 0.05 = ' + str(mt_unrel_only2_filtered.count()[0])) # 17997741 variants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20fd2e",
   "metadata": {},
   "source": [
    "The next step is to check which allele, reference (ref) or alternate (alt), is the the doubleton. If the first element of the array in the allele frequency field (AF[0]) is less than the second elelement (AF[1]), then the doubleton is 1st allele (ref). If the first element (AF[0]) is greater than the second elelement (AF[1]), then the doubleton is 2nd allele (alt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a229812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check allele frequency (AF) to see if the doubleton is the ref or alt allele \n",
    "\n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print('Num of variants where the 1st allele (ref) is the doubleton = ' + str(mt_doubl_ref.count()[0])) # 3159 variants\n",
    "\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print('Num of variants where the 2nd allele (alt) is the doubleton = ' + str(mt_doubl_alt.count()[0])) # 17994582 variants\n",
    "\n",
    "# sanity check \n",
    "mt_doubl_ref.count()[0] + mt_doubl_alt.count()[0] = mt_unrel_only2_filtered.count()[0]\n",
    "print(3159 + 17994582 == 17997741) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85cb8c",
   "metadata": {},
   "source": [
    "Once we have figured out which allele is the doubleton and divided the doubleton matrix table accordingly, the next step is to find the samples that have doubletons, compile them in a set, and annotate that onto the mt as a new row field. This done for each mt separately and can be achieved by looking at the genotype call (GT) field. When the doubleton is 1st allele (ref), the genotype call would be 0|1 & 0|0. When the doubleton is 2nd allele (alt), the genotype call would be 0|1 & 1|1. We chose a set for the results instead of a list because a list isn't hashable and the next step wouldn't have run. After the annotation of the new row field in each mt, we then count how many times a sample or a sample pair appears within that field and store the results in a dictionary. Once we have the two dictionaries (one for the ref and one for the alt), we merge them into one and add up the values for identical keys.\n",
    "\n",
    "If you want to do a sanity check at this point, you can add up the count of the two dictionaries and then subtract the number of keys that intersect between the two. The value that you get should be equal to the length of the combined dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each mt, find the samples that have doubletons, compile them in a set, and add as a new row field \n",
    "\n",
    "# doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# if there is one sample in the new column field then it's 0|0. If there are two samples, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1))| (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "# doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# if there is one sample in the new column field then it's 1|1. If there are two samples, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1))| (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n",
    "\n",
    "# count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field - returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))\n",
    "\n",
    "# combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "print('Length of dictionary = ' + str(len(all_doubl_count))) # 3183039 \n",
    "\n",
    "# sanity check \n",
    "## len(all_doubl_count) == (len(ref_doubl_count) + len(alt_doubl_count)) - # of keys that intersect b/n the two dictionaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a20180",
   "metadata": {},
   "source": [
    "For the next step, we get a list of samples that are in the doubleton mt and also create sample pairs out of them. We also divide the combined dictionary into two: one for when a sample is a key by itself (len(key) == 1) and the other for when the dictionary key is a pair of samples (len(key) != 1). We then go through the lists of samples obtained from the mt and see if any of them are keys in their respective doubleton dictionaries - list of samples by themselves is compared against the dictionary that has a single sample as a key and the list with sample pairs is compared against the dictionary where the key is a pair of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of samples from mt\n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# make pairs from sample list: n(n-1)/2 - 5710510 pairs \n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(sample_list) for j,y in enumerate(sample_list) if i<j]\n",
    "\n",
    "# subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "    if duplicate:\n",
    "        print(\"Removing any duplicate samples\")\n",
    "        # Removing any duplicates in the dataset using hl.distinct_by_col() which removes\n",
    "        # columns with a duplicate column key. It keeps one column for each unique key.\n",
    "        # after updating to the new dense_mt, this step is no longer necessary to run\n",
    "        mt = mt.distinct_by_col()\n",
    "\n",
    "# sanity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True\n",
    "# are the samples in the list the same as the dict keys?\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True \n",
    "# are the sample pairs obtained from the mt equal to what's in the pair dict? \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from the mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d77b3",
   "metadata": {},
   "source": [
    "If a single sample is a key in the single-sample-key dictionary, we record the sample ID twice and it's corresponding value from the dictionary. If it is not a key, we record the sample ID twice and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sample comparison \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# sanity check \n",
    "## for the single samples, the length should be consistent across dict, mt sample list, and final list\n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b81c4",
   "metadata": {},
   "source": [
    "If a sample pair is a key in the sample-pair-key dictionary, we record the two sample IDs and the corresponding value from the dictionary. If that is not the case, we record the two sample IDs and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample pair comparison\n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# sanity check \n",
    "## length of final list should be equal to the length of the sample list obtained from the mt \n",
    "print(len(sample_pair_final_list) == len(mt_sample_pairs)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21564e",
   "metadata": {},
   "source": [
    "Last step is to combine the two lists obtained from the comparisons, convert that into a pandas table, format it as needed, and write it out as a csv so that the values can be plotted as a heat map in R. For plot annotation purposes, we also export the sample IDs and their respective populations & genetic regions.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# sanity check \n",
    "len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list) # True\n",
    "\n",
    "# format list as pandas table \n",
    "df = pd.DataFrame(final_list)\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) # rename column names \n",
    "\n",
    "# save table to the Cloud so it can be plotted in R \n",
    "df.to_csv(f2_final_path, index=False, sep='\\t')\n",
    "\n",
    "# save sample IDs and their respective populations & genetic regions for heat map annotation \n",
    "sampleID_pop_reg = (mt_unrel_only2_filtered.select_cols(mt_unrel_only2_filtered['hgdp_tgp_meta']['Population'], mt_unrel_only2_filtered['hgdp_tgp_meta']['Genetic']['region'])).cols()\n",
    "sampleID_pop_reg.export(f2_annotation_path, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc78ebc",
   "metadata": {},
   "source": [
    "# 3. F_st\n",
    "\n",
    "F_st detects genetic divergence from common variance allowing us to understand past deep history. Although we are running F_st only on unrelated samples in this tutorial, unlike the F2 analysis, here we are using the filtered and pruned mt that doesn't include the 22 outliers (whole_dataset.mt = *filtered_n_pruned_output_updated.mt* - 22 outliers). \n",
    "\n",
    "Something to note: Since the mt we are starting with is filtered and pruned, running *hl.variant_qc* and filtering to variants with call rate > 0.05 (similar to what we did for the F2 analysis) doesn't make a difference to the number of variants.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00496151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the mt before running pc_relate but without outliers - 248634 variants and 4097 samples\n",
    "mt_FST_initial = hl.read_matrix_table(fst_input_path) \n",
    "\n",
    "# filter to only unrelated samples - 3380 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelated_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_FST_unrel = mt_FST_initial.filter_cols(unrel_samples.contains(mt_FST_initial['s'])) # filter mt\n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_FST_unrel.count()[1])) # 3380 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcd857",
   "metadata": {},
   "source": [
    "### 3a. F_st with PLINK\n",
    "\n",
    "In order to calculate F_st using PLINK, we first need to export mt as PLINK files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export mt as PLINK2 BED, BIM and FAM files - store on the Cloud \n",
    "hl.export_plink(mt_FST_unrel, plink_files_path, fam_id=mt_FST_unrel.hgdp_tgp_meta.Population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77df47b",
   "metadata": {},
   "source": [
    "The rest of the analysis is done using shell commands within the notebook. Place *\"!\"* before the command you want to run and proceed as if you are running codes in a terminal. You can use *\"! ls\"* after each run to check for ouputs in the directory and see if commands have run correctly. Also, every time you start a new cluster, you would need to download PLINK to run the F_st analysis since downloads and files are discarded when a cluster is stopped. \n",
    "\n",
    "Something to note: when running the notebook on the Cloud, the shell commands still run even if we didn't use \"!\". Not sure why but will check if that is also the case when running the notebook locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5592a6f",
   "metadata": {},
   "source": [
    "3a.1. PLINK Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04399829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download PLINK using a link from the PLINK website (linux - recent version - 64 bit - stable) \n",
    "! wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
    "    \n",
    "# unzip the \".gz\" file: \n",
    "! unzip plink_linux_x86_64_20210606.zip\n",
    "\n",
    "# a documentation output when you run this command indicates that PLINK has been installed properly \n",
    "! ./plink "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8555f",
   "metadata": {},
   "source": [
    "3a.2. Files Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the PLINK files that are stored on the Cloud into the current session directory\n",
    "! gsutil cp {plink_files_path}.fam . # fam \n",
    "! gsutil cp {plink_files_path}.bim . # bim\n",
    "! gsutil cp {plink_files_path}.bed . # bed\n",
    "\n",
    "# obtain FID - in this case, it is the 78 populations in the first column of the FAM file\n",
    "! awk '{print $1}' hgdp_tgp.fam | sort -u > pop.codes\n",
    "\n",
    "# make all possible combinations of pairs using the 78 populations \n",
    "! for i in `seq 78`; do for j in `seq 78`; do if [ $i -lt $j ]; then VAR1=`sed \"${i}q;d\" pop.codes`; VAR2=`sed \"${j}q;d\" pop.codes`; echo $VAR1 $VAR2; fi; done; done > pop.combos\n",
    "\n",
    "# sanity check \n",
    "! wc -l pop.combos # 3003\n",
    "\n",
    "# create directories for intermediate files and F_Sst results \n",
    "! mkdir within_files # intermediate files\n",
    "! mkdir FST_results # F_st results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2445280",
   "metadata": {},
   "source": [
    "3a.3. Scripts Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009f50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### script 1 ####\n",
    "\n",
    "# for each population pair, set up a bash script to create a \"within\" file and run F_st \n",
    "# files to be produced: \n",
    "### [pop_pairs].within will be saved in the \"within_files\" directory\n",
    "### [pop_pairs].fst, [pop_pairs].log, and [pop_pairs].nosex will be save in \"FST_results\" directory\n",
    "\n",
    "fst_script = '''    \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "\n",
    "# create \"within\" files for each population pair using the FAM file (columns 1,2 and 1 again)\n",
    "    awk -v r1=$POP1 -v r2=$POP2 '$1 == r1 || $1 == r2' hgdp_tgp.fam | awk '{ print $1, $2, $1 }' > within_files/${POP1}_${POP2}.within\n",
    "\n",
    "# run F_st\n",
    "    ./plink --bfile hgdp_tgp --fst --within within_files/${POP1}_${POP2}.within --out FST_results/${POP1}_${POP2}\n",
    "done'''\n",
    "\n",
    "with open('run_fst.py', mode='w') as file:\n",
    "    file.write(fst_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2383c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### script 2 ### - script 1 has to be run for this script to run \n",
    "\n",
    "# use the \"[pop_pairs].log\" file produced from script 1 above (located in \"FST_results\" directory) to get the \"Mean F_st estimate\" for each population pair and compile all of the values in a single file for F_st heat map generation\n",
    "\n",
    "extract_mean_script = ''' \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "    mean_FST=$(tail -n4 FST_results/${POP1}_${POP2}.log | head -n 1 | awk -F: '{print $2}' | awk '{$1=$1};1')\n",
    "    printf \"%-20s\\t%-20s\\t%-20s\\n \" ${POP1} ${POP2} $mean_FST >> mean_fst_sum.txt\n",
    "done'''\n",
    "\n",
    "with open('extract_mean.py', mode='w') as file:\n",
    "    file.write(extract_mean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd0192",
   "metadata": {},
   "source": [
    "3a.4. Run Scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4550e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run script 1 and direct the run log into a file (~20min to run) \n",
    "! sh run_fst.py > fst_script.log\n",
    "\n",
    "# sanity check \n",
    "! cd within_files/; ls | wc -l # 3003\n",
    "! cd FST_results/; ls | wc -l # 3003 * 3 = 9009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00aa4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run script 2; requires script 1 to be run first (~ 1min to run)\n",
    "! sh extract_mean.py \n",
    "\n",
    "# copy script 2 output to the Cloud for heat map plotting in R \n",
    "! gsutil cp mean_fst_sum.txt {fst_final_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

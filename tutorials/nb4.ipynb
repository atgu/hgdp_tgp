{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8de0f3",
   "metadata": {},
   "source": [
    "Notebook 4: filtering to common independent SNPS, relatedness, PCA, joint calling with new dataset, applying RF on new dataset. Analyses that need to be run:\n",
    "1. Work with Lindo: - *PENDING*\n",
    "    - Joint calling with GGV, sample QC \n",
    "    - Use gnomAD RF (subset to variants in RF model) - doesn’t need VQSR \n",
    "    - Intersect HGDP+1kG+GGV, build a RF with 1kG+ HGDP, apply it to a new dataset (GGV) - doesn’t need VQSR \n",
    "2. PCA plots - Ally - *PENDING*\n",
    "    - already implemented in R, just need to plot it in Hail\n",
    "    \n",
    "----------------------------------------\n",
    "Further edits needed in this nb: \n",
    "- Add all paths below in part 1\n",
    "- Add desc above each code block if needed \n",
    "- Clean up comments inside the code block \n",
    "- Add Hail links below each code block\n",
    "- Add Ally's code for plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad714b26",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [General Overview](#1.-General-Overview)\n",
    "- [Variant Filtering and LD Pruning](#2.-Variant-Filtering-and-LD-Pruning)\n",
    "- [Run PC Relate](#3.-Run-PC-Relate)\n",
    "- [PCA](#4.-PCA)\n",
    "    - [Function to Run PCA on Unrelated Individuals](#4a.-Function-to-Run-PCA-on-Unrelated-Individuals)\n",
    "    - [Function to Project Related Individuals](#4b.-Function-to-Project-Related-Individuals)\n",
    "    - [Global PCA](#4c.-Global-PCA)\n",
    "    - [Subcontinental PCA](#4d.-Subcontinental-PCA)\n",
    "- [Outlier Removal](#5.-Outlier-Removal)\n",
    "- [Rerun PCA](#6.-Rerun-PCA)\n",
    "    - [Global PCA](#6a.-Global-PCA)\n",
    "    - [Subcontinental PCA](#6b.-Subcontinental-PCA)\n",
    "- [Write Out Matrix Table](#7.-Write-Out-Matrix-Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1530e",
   "metadata": {},
   "source": [
    "# 1. General Overview \n",
    "The purpose of this notebook is to further filter the matrix table obtained from notebook 3, run relatedness and Principal Component Analysis (PCA), joint call with new data set, and apply RF. It contains steps on how to:\n",
    "\n",
    "- Read in the a matrix table and run Hail common variant statistics  \n",
    "- Filter using allele frequency and call rate\n",
    "- Run LD pruning \n",
    "- Run relatedness and separate related and unrelated individuals\n",
    "- Calculate PC scores and project samples on to a PC space  \n",
    "- Run global and Subcontinental PCA and plot them \n",
    "- Remove PCA outliers (filter using sample IDs)\n",
    "- Joint call with a new data set\n",
    "- Build/apply RF\n",
    "- Write out a matrix table \n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a753877",
   "metadata": {},
   "source": [
    "1a. Import needed libraries and packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hail\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48ecb2",
   "metadata": {},
   "source": [
    "1b. Input and output path variables to be edited by users as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8dcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "input_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt'\n",
    "\n",
    "# temporary file\n",
    "intermediate_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt'\n",
    "\n",
    "# pre-outlier paths for unrelated and related samples \n",
    "unrel_path = 'gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt'\n",
    "rel_path = 'gs://hgdp-1kg/hgdp_tgp/rel_updated.mt' \n",
    "\n",
    "# pre-outlier file path is missing - global & subcont pca results [here]\n",
    "\n",
    "# outliers file \n",
    "outliers_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "\n",
    "# post-outlier file path is missing - global & subcont pca results[here]\n",
    "\n",
    "# final output paths for unrelated and related samples (post-outlier)\n",
    "unrel_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt'\n",
    "rel_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2dc0d6",
   "metadata": {},
   "source": [
    "# 2. Variant Filtering and LD Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e51b0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - Why are we doing this? [click here] </summary>\n",
    "    \n",
    "At this point, we have 155,648,020 SNPs and since we need fewer number of variants (~100-300k) for PCA, we filter on:\n",
    "- AF - allele frequency \n",
    "- call rate - fraction of calls neither missing nor filtered\n",
    "\n",
    "and then run LD pruning.     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ac861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the right intermediate file \n",
    "mt_filt = hl.read_matrix_table(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e2df8",
   "metadata": {},
   "source": [
    "2a. Variant Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Hail's common variant statistics (QC metrics) \n",
    "mt_var = hl.variant_qc(mt_filt) \n",
    "\n",
    "# filter to variants with AF between 0.05 & 0.95, and call rate greater than 0.999    \n",
    "mt_var_filt = mt_var.filter_rows((mt_var.variant_qc.AF[0] > 0.05) & (mt_var.variant_qc.AF[0] < 0.95) & (mt_var.variant_qc.call_rate > 0.999))\n",
    "print('Num of variants after filtering = ' + str(mt_var_filt.count()[0])) # 6787034 snps; this line take ~20min to run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06157419",
   "metadata": {},
   "source": [
    "2b. LD Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated variants \n",
    "pruned = hl.ld_prune(mt_var_filt.GT, r2=0.1, bp_window_size=500000) # ~113 min to run  \n",
    "mt_var_pru_filt = mt_var_filt.filter_rows(hl.is_defined(pruned[mt_var_filt.row_key])) \n",
    "print('Num of variants after LD pruning = ' + str(mt_var_pru_filt.count()[0])) # 248634 snps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f40212",
   "metadata": {},
   "source": [
    "- Since the number of variants is now in the ~100-300k range, we proceed to the PCA analysis without any more adjustments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43755d",
   "metadata": {},
   "source": [
    "2c. Write out an intermediate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pruning step took a bit of time to run so we have to write out the filtered and pruned mt as an intermediate file\n",
    "mt_var_pru_filt.write(intermediate_path, overwrite=False) # ~23 min to run\n",
    "\n",
    "# read the intermediate file back in for subsequent analyses\n",
    "mt_var_pru_filt = hl.read_matrix_table(intermediate_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b38b80",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> - More information on Hail methods and expressions [click here] </summary>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc\"> More on  <i> variant_qc() </i></a>\n",
    "\n",
    "- <a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.ld_prune\"> More on  <i> ld_prune() </i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79623b3e",
   "metadata": {},
   "source": [
    "# 3. Run PC Relate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relatedness estimates between individuals using a variant of the PC-Relate method (https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate)\n",
    "# only compute the kinship statistic using:\n",
    "# a minimum minor allele frequency filter of 0.05, \n",
    "# excluding sample-pairs with kinship less than 0.05, and \n",
    "# 20 principal components to control for population structure \n",
    "# a hail table is produced (~4min to run) \n",
    "relatedness_ht = hl.pc_relate(mt_var_pru_filt.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify related individuals in pairs to remove - returns a list of sample IDs (~2hr & 22 min to run) - previous one took ~13min\n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sample IDs (col_key of the matrixTable), pick out the samples that are not found in 'related_samples_to_remove' (had 'False' values for the comparison)  \n",
    "# subset the mt to those only \n",
    "mt_unrel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=False) \n",
    "\n",
    "# do the same as above but this time for the samples with 'True' values (found in 'related_samples_to_remove')  \n",
    "mt_rel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out mts of unrelated and related samples on to the cloud \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel.write(unrel_path, overwrite=False) \n",
    "\n",
    "# related mt \n",
    "mt_rel.write(rel_path, overwrite=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved mts back in \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel = hl.read_matrix_table(unrel_path) \n",
    "\n",
    "# related mt \n",
    "mt_rel = hl.read_matrix_table(rel_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86030a9c",
   "metadata": {},
   "source": [
    "# 4. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ca5ed",
   "metadata": {},
   "source": [
    "### 4a. Function to Run PCA on Unrelated Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f727a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a dataset\n",
    "    :param mt: dataset to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_prefix: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_prefix + reg_name + '_scores.txt.bgz')  # save individual-level genetic region PCs\n",
    "    pca_loadings.write(out_prefix + reg_name + '_loadings.ht', overwrite)  # save PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec280de7",
   "metadata": {},
   "source": [
    "### 4b. Function to Project Related Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running on GCS, need to add \"--packages gnomad\" when starting a cluster in order for the import to work  \n",
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space - unrelated samples \n",
    "    :param project_mt: matrixTable of data to project - related samples \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param project_prefix: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_prefix + reg_name + '_projected_scores.txt.bgz') # save output \n",
    "    #return ht_projections # return to user  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7ca94",
   "metadata": {},
   "source": [
    "### 4c. Global PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function for global pca   \n",
    "run_pca(mt_unrel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for global pca \n",
    "loadings = hl.read_table('gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/global_loadings.ht') # read in the PCA loadings that were obtained from 'run_pca' function \n",
    "project_individuals(loadings, mt_rel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/', False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbbc1fe",
   "metadata": {},
   "source": [
    "### 4d. Subcontinental PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e271d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset - used the unrelated dataset since it had more samples \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f646434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5bbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 'run_pca' function for each region - nb freezes after printing the log for AMR  \n",
    "# don't restart it - just let it run and you can follow the progress through the SparkUI\n",
    "# even after all the outputs are produced and the run is complete, the code chunk will seem as if it's still running (* in the left square bracket)\n",
    "# can check if the run is complete by either checking the output files in the Google cloud bucket or using the SparkUI \n",
    "# after checking the desired outputs are generated and the run is done, exit the current nb, open a new session, and proceed to the next step\n",
    "# ~27min to run \n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for each region (~2min to run)\n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the relateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae57dd6",
   "metadata": {},
   "source": [
    "# 5. Outlier Removal\n",
    "#### After plotting the PCs, 22 outliers that need to be removed were identified (the table below will be completed for the final report)\n",
    "\n",
    "\n",
    "| s | Genetic region | Population | Note |\n",
    "| --- | --- | --- | -- |\n",
    "| NA20314 | AFR | ASW | Clusters with AMR in global PCA | \n",
    "| NA20299 | - | - | - |\n",
    "| NA20274 | - | - | - |\n",
    "| HG01880 | - | - | - |\n",
    "| HG01881 | - | - | - |\n",
    "| HG01628 | - | - | - |\n",
    "| HG01629 | - | - | - |\n",
    "| HG01630 | - | - | - |\n",
    "| HG01694 | - | - | - |\n",
    "| HG01696 | - | - | - |\n",
    "| HGDP00013 | - | - | - |\n",
    "| HGDP00150 | - | - | - |\n",
    "| HGDP00029 | - | - | - |\n",
    "| HGDP01298 | - | - | - |\n",
    "| HGDP00130 | CSA | Makrani | Closer to AFR than most CSA |\n",
    "| HGDP01303 | - | - | - |\n",
    "| HGDP01300 | - | - | - |\n",
    "| HGDP00621 | MID | Bedouin | Closer to AFR than most MID |\n",
    "| HGDP01270 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP01271 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP00057 | - | - | - | \n",
    "| LP6005443-DNA_B02 | - | - | - |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in the unrelated and related mts to remove outliers and run pca \n",
    "# bucket was moved to another project so different paths are used from where these mts were previously saved \n",
    "mt_unrel_unfiltered = hl.read_matrix_table(unrel_path) # unrelated mt\n",
    "mt_rel_unfiltered = hl.read_matrix_table(rel_path) # related mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the outliers file into a list\n",
    "with hl.utils.hadoop_open(outliers_path) as file: \n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "# capture and broadcast the list as an expression\n",
    "outliers_list = hl.literal(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 22 outliers \n",
    "mt_unrel = mt_unrel_unfiltered.filter_cols(~outliers_list.contains(mt_unrel_unfiltered['s']))\n",
    "mt_rel = mt_rel_unfiltered.filter_cols(~outliers_list.contains(mt_rel_unfiltered['s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b461ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check \n",
    "print('Unrelated: Before outlier removal ' + str(mt_unrel_unfiltered.count()[1]) + ' | After outlier removal ' + str(mt_unrel.count()[1]))\n",
    "print('Related: Before outlier removal: ' + str(mt_rel_unfiltered.count()[1]) + ' | After outlier removal ' + str(mt_rel.count()[1]))\n",
    "\n",
    "num_outliers = (mt_unrel_unfiltered.count()[1] - mt_unrel.count()[1]) + (mt_rel_unfiltered.count()[1] - mt_rel.count()[1])\n",
    "print('Total samples removed = ' + str(num_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699d2ed",
   "metadata": {},
   "source": [
    "# 6. Rerun PCA\n",
    "### - The following steps are similar to the ones prior to removing the outliers except now we are using the updated unrelated & related dataset and a new GCS bucket path to save the outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fab21d",
   "metadata": {},
   "source": [
    "### 6a. Global PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95874435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function for global pca - make sure the code block for the function (located above) is run prior to running this    \n",
    "run_pca(mt_unrel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for global pca - make sure the code block for the function (located above) is run prior to running this    \n",
    "loadings = hl.read_table('gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/global_loadings.ht') # read in the PCA loadings that were obtained from 'run_pca' function \n",
    "project_individuals(loadings, mt_rel, 'global', 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/', False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876a3ee",
   "metadata": {},
   "source": [
    "### 6b. Subcontinental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset - used the unrelated dataset since it had more samples  \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3952d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ba073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function (located above) for each region \n",
    "# notebook became slow and got stuck - don't restart it, just let it run and you can follow the progress through the SparkUI\n",
    "# after checking the desired outputs are generated (GCS bucket) and the run is done (SparkUI), exit the current nb, open a new session, and proceed to the next step\n",
    "# took roughly 25-27 min  \n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b9d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 'project_relateds' function (located above) for each region - took ~3min \n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the relateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a3f9a",
   "metadata": {},
   "source": [
    "# 7. Write Out Matrix Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out mts of unrelated and related samples separately (post-outlier removal) \n",
    "\n",
    "mt_unrel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt', overwrite=False) #unrelated mt\n",
    "mt_rel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt', overwrite=False) #related mt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed97fd3b",
   "metadata": {},
   "source": [
    "# Computing Population Genetics Statistics (*f*<sub>2</sub> and F<sub>ST</sub>)\n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969d672",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Setting Default Paths](#1.-Set-Default-Paths)\n",
    "2. [Read in Pre-QC Dataset and Apply Quality Control Filters](#2.-Read-in-Pre-QC-Dataset-and-Apply-Quality-Control-Filters)\n",
    "3. [*f*<sub>2</sub> Analysis](#3.-f2-Analysis)\n",
    "4. [F<sub>ST</sub>](#4.-F_ST)\n",
    "    1. [F<sub>ST</sub> with PLINK](#4.a.-F_ST-with-PLINK)\n",
    "        1. [PLINK Set up](#4.a.1.-PLINK-Set-up)\n",
    "        2. [Files Set up](#4.a.2.-Files-Set-up)\n",
    "        3. [Scripts Set up](#4.a.3.-Scripts-Set-up) \n",
    "        4. [Run Scripts](#4.a.4.-Run-Scripts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30934f3a",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "\n",
    "The purpose of this notebook is to show two common population genetics analyses (*f*<sub>2</sub> and F<sub>ST</sub>) which are used to understand recent and deep history. \n",
    "\n",
    "*f*<sub>2</sub> analysis computes the number of SNVs that appear twice in a dataset and compares how often they are shared among individuals. Since doubletons are rare variants, they tend to have arisen relatively recently giving us information about recent population history. In contrast, F<sub>ST</sub> is a “fixation index” which calculates the extent of variation within versus between populations using SNVs of many frequencies. Because common variants are used in F<sub>ST</sub> analyses which arose a long time ago, this gives us information about older population history.\n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in a matrix table (mt) and filter using sample IDs that were obtained from another matrix table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a matrix table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain mt/table fields\n",
    "- Download a tool such as PLINK using a link and shell command \n",
    "- Run shell commands, set up a script & run it from inside a code block \n",
    "- Calculate F<sub>st</sub> using PLINK \n",
    "- Go through a log file and extract needed information\n",
    "- Write out results onto the Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7088e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "\n",
    "# Functions from gnomAD library to apply genotype filters   \n",
    "from gnomad.utils.filtering import filter_to_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd64e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://mty-m.c.diverse-pop-seq-ref.internal:42753\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.109-b71b065e4bb6\n",
      "LOGGING: writing to /home/hail/hail-20230317-1456-0.2.109-b71b065e4bb6.log\n"
     ]
    }
   ],
   "source": [
    "# Initializing Hail \n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51650ae",
   "metadata": {},
   "source": [
    "# 1. Set Default Paths\n",
    "\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets.\n",
    "\n",
    "**By default, all of the dataset write out sections are shown as markdown cells. If you would like to write out your own dataset, you can copy the code and paste it into a new code cell. Don't forget to change the paths in the following cell accordingly.** \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3162a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file path for f2 analysis - HGDP+1kGP dataset prior to applying gnomAD QC filters\n",
    "pre_qc_path = 'gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_dense.mt'\n",
    "\n",
    "# Path for unrelated samples mt without outliers -  for subsetting purposes\n",
    "unrelateds_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/pca_results/unrelateds_without_outliers.mt'\n",
    "\n",
    "# Path for final count table for the f2 analysis \n",
    "final_doubleton_count_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/doubleton_count.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b4a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file path for F_ST analysis - mt generated in Notebook 2: PCA and Ancestry Analyses after LD pruning\n",
    "fst_input_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/pca_preprocessing/ld_pruned.mt'\n",
    "\n",
    "# Path for exporting the PLINK files \n",
    "# Include file prefix at the end of the path - here the prefix is 'hgdp_tgp'\n",
    "plink_files_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/hgdp_tgp'\n",
    "\n",
    "# Path for final F_ST output  \n",
    "final_mean_fst_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/mean_fst.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d48e61",
   "metadata": {},
   "source": [
    "# 2. Read in Pre-QC Dataset and Apply Quality Control Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1985ab1",
   "metadata": {},
   "source": [
    "Since the post-QC mt was not written out, we run the same function as the previous tutorial notebooks to apply the quality control filters to the pre-QC dataset.\n",
    "\n",
    "**To avoid errors, make sure to run the next two cells before running any code that includes the post-QC dataset.**\n",
    "\n",
    "**If running the cell below results in an error, double check that you used the  `--packages gnomad` argument when starting your cluster.**  \n",
    "\n",
    "- See the tutorials [README](https://github.com/atgu/hgdp_tgp/tree/master/tutorials#readme) for more information on how to start a cluster.\n",
    "\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/impex.html#hail.methods.read_matrix_table\"> More on  <i> read_matrix_table() </i></a></li>\n",
    "        \n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.describe\"> More on  <i> describe() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.count\"> More on  <i> count() </i></a></li>\n",
    "    \n",
    "<li><a href=\"https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.filter_cols\"> More on  <i> filter_cols() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.filter_rows\"> More on  <i> filter_rows() </i></a></li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee381c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function to apply gnomAD's sample, variant and genotype QC filters\n",
    "\n",
    "def run_qc(mt):\n",
    "    \n",
    "    ## Apply sample QC filters to dataset \n",
    "    # This filters to only samples that passed gnomAD's sample QC hard filters  \n",
    "    mt = mt.filter_cols(~mt.gnomad_sample_filters.hard_filtered) # removed 31 samples\n",
    "    \n",
    "    ## Apply variant QC filters to dataset\n",
    "    # This subsets to only PASS variants - those which passed gnomAD's variant QC\n",
    "    # PASS variants have an entry in the filters field \n",
    "    mt = mt.filter_rows(hl.len(mt.filters) != 0, keep=False)\n",
    "\n",
    "    ## Apply genotype QC filters to the dataset\n",
    "    # This is done using a function imported from gnomAD and is the last step in the QC process\n",
    "    mt = filter_to_adj(mt)\n",
    "\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00259cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the HGDP+1kGP pre-QC mt\n",
    "pre_qc_mt = hl.read_matrix_table(pre_qc_path)\n",
    "\n",
    "# Run QC \n",
    "mt_filt = run_qc(pre_qc_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc391744",
   "metadata": {},
   "source": [
    "<a id='3.-f2-Analysis'></a>\n",
    "\n",
    "# 3. *f*<sub>2</sub> Analysis\n",
    "\n",
    "\n",
    "We are running *f*<sub>2</sub> on unrelated samples only so for subsetting, we are using the unrelateds mt which was generated after removing outliers and rerunning PCA in <a href=\"https://nbviewer.org/github/atgu/hgdp_tgp/blob/master/tutorials/nb4.ipynb\">Notebook 2</a>. After obtaining the desired samples, we run Hail's common variant statistics so we can separate out doubletons. Once we have the doubletons filtered, we then remove variants with a call rate less than 0.05 (no more than 5% missingness/low missingness).\n",
    "\n",
    "*Doubletons are variants that show up twice and only twice in a dataset and useful in detecting rare variance & understanding recent history.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5bd83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples after filtering (unrelated samples) = 3378\n",
      "Num of variants that are doubletons = 17279480\n",
      "Num of variants > 0.05 = 17229743\n"
     ]
    }
   ],
   "source": [
    "# This code chunk took 20min to run \n",
    "\n",
    "# Filter to only unrelated samples - 3378 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt \n",
    "print(f'Num of samples after filtering (unrelated samples) = {mt_filt_unrel.count_cols()}') # 3378 samples\n",
    "\n",
    "# Run common variant statistics (quality control metrics)  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# Separate the AC array into individual fields and extract the doubletons  \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "print(f'Num of variants that are doubletons = {mt_unrel_only2.count_rows()}') # 17279480 variants\n",
    "\n",
    "# Remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "print(f'Num of variants > 0.05 = {mt_unrel_only2_filtered.count_rows()}') # 17229743 variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5b99d",
   "metadata": {},
   "source": [
    "The next step is to check which allele, reference (ref) or alternate (alt), is the the doubleton. If the first element of the array in the allele frequency field (AF[0]) is less than the second elelement (AF[1]), then the doubleton is the 1st allele (ref). If the first element (AF[0]) is greater than the second elelement (AF[1]), then the doubleton is the 2nd allele (alt).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk took 44min to run because of the print commands which we've commented out here\n",
    "\n",
    "# Check allele frequency (AF) to see if the doubleton is the ref or alt allele \n",
    "\n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print(f'Num of variants where the 1st allele (ref) is the doubleton = {mt_doubl_ref.count_rows()}') # 2979 variants\n",
    "\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "#print(f'Num of variants where the 2nd allele (alt) is the doubleton = {mt_doubl_alt.count_rows()}') # 17226764 variants\n",
    "\n",
    "# Validity check: should print True\n",
    "#mt_doubl_ref.count_rows() + mt_doubl_alt.count_rows() == mt_unrel_only2_filtered.count_rows() # True\n",
    "#print(3159 + 17994582 == 17997741) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629b779",
   "metadata": {},
   "source": [
    "Once we've figured out which allele is the doubleton and divided the doubleton matrix table accordingly, the next step is to find the samples that have doubletons, compile them in a set, and annotate that onto the mt as a new row field. This done for each mt separately and can be achieved by looking at the genotype call (GT) field. When the doubleton is the 1st allele (ref), the genotype call would be 0|1 & 0|0. When the doubleton is the 2nd allele (alt), the genotype call would be 0|1 & 1|1. \n",
    "\n",
    "We chose a set for the results instead of a list because a list isn't hashable and the next step wouldn't run. After the annotation of the new row field in each mt, we then count how many times a sample or a sample pair appears within that field and store the results in a dictionary. Once we have the two dictionaries (one for the ref and one for the alt), we merge them into one and add up the values for identical keys.\n",
    "\n",
    "If you want to do a validity check at this point, you can add up the count of the two dictionaries and then subtract the number of keys that intersect between the two. The value that you get should be equal to the length of the combined dictionary.  \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "145c8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary = 2989787\n"
     ]
    }
   ],
   "source": [
    "# This code chunk took 21min to run\n",
    "\n",
    "# For each mt, find the samples that have doubletons, compile them in a set, and add as a new row field \n",
    "\n",
    "# Doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# If there is one sample in the new column field then it's 0|0\n",
    "# If there are two samples, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons=hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1)) | (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "# Doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# If there is one sample in the new column field then it's 1|1\n",
    "# If there are two samples, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons=hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1)) | (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n",
    "\n",
    "# Count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field - returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))\n",
    "\n",
    "# Combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "print(f'Length of dictionary = {len(all_doubl_count)}') # 2989787\n",
    "\n",
    "# Validity check \n",
    "#len(all_doubl_count) == (len(ref_doubl_count) + len(alt_doubl_count)) - # of keys that intersect b/n the two dictionaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5104ef",
   "metadata": {},
   "source": [
    "For the next step, we get a list of samples that are in the doubleton mt and also create sample pairs out of them. We also divide the combined dictionary into two: one for when a sample is a key by itself (len(key) == 1) and the other for when the dictionary key is a pair of samples (len(key) != 1). \n",
    "\n",
    "We then go through the lists of samples obtained from the mt and see if any of them are keys in their respective doubleton dictionaries:\n",
    "- list of samples by themselves is compared against the dictionary that has a single sample as a key \n",
    "- the list with sample pairs is compared against the dictionary where the key is a pair of samples \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b02091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Get list of samples from mt - 3378 samples \n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# Make pairs from sample list: n(n-1)/2 - 5703753 pairs \n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(mt_sample_list) for j,y in enumerate(mt_sample_list) if i<j]\n",
    "\n",
    "# Subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "# subset dict to keys with sample pairs (not just 1)\n",
    "dict_pair_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) != 1}\n",
    "\n",
    "# Validity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True\n",
    "\n",
    "# Are the samples in the list the same as the dict keys?\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True \n",
    "\n",
    "# Are the sample pairs obtained from the mt equal to what's in the pair dict? \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from the mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac353420",
   "metadata": {},
   "source": [
    "If a single sample is a key in the single-sample-key dictionary, we record the sample ID twice and it's corresponding value from the dictionary. If it is not a key, we record the sample ID twice and set the value to 0. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7030d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Single sample comparison \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# Validity check \n",
    "# For the single samples, the length should be consistent across dict, mt sample list, and final list\n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549b8ef",
   "metadata": {},
   "source": [
    "If a sample pair is a key in the sample-pair-key dictionary, we record the two sample IDs and the corresponding value from the dictionary. If that is not the case, we record the two sample IDs and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c8bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Sample pair comparison\n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# Validity check \n",
    "# Length of final list should be equal to the length of the sample list obtained from the mt \n",
    "print(len(sample_pair_final_list) == len(mt_sample_pairs)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedb06e",
   "metadata": {},
   "source": [
    "Last step is to combine the two lists obtained from the comparisons, convert that into a pandas table, format it as needed, and write it out as a csv so the values can be plotted as a heat map in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8525fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# Validity check \n",
    "len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list) # True\n",
    "\n",
    "# Format list as pandas table \n",
    "df = pd.DataFrame(final_list)\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) # rename column names "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4de5b0",
   "metadata": {},
   "source": [
    "- Write out table to the Cloud so it can be plotted in R \n",
    "\n",
    "```python3\n",
    "df.to_csv(final_doubleton_count_path, index=False, sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552c562",
   "metadata": {},
   "source": [
    "### The sample-level *f*<sub>2</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/F2_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67f89d",
   "metadata": {},
   "source": [
    "<a id='4.-F_ST'></a>\n",
    "# 4. F<sub>ST</sub>\n",
    "\n",
    "F<sub>ST</sub> detects genetic divergence from common variance allowing us to understand past deep history. Similar to the *f*<sub>2</sub> analysis, we are running F<sub>ST</sub> on unrelated samples only. However, here we are starting with a filtered and pruned dataset instead of the pre-QC dataset (without outliers).\n",
    "\n",
    "**Something to note**: Since the mt we are starting with is filtered and pruned, running <code>hl.variant_qc</code> and filtering to variants with <code>call rate > 0.05 </code> (similar to what we did for the *f*<sub>2</sub> analysis) doesn't make a difference to the number of variants.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "943d0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples after filtering (unrelated samples) = 3378\n"
     ]
    }
   ],
   "source": [
    "# Read-in the filtered and pruned mt with outliers - 199974 variants and 4120 samples\n",
    "mt_FST_initial = hl.read_matrix_table(fst_input_path) \n",
    "\n",
    "# Filter to only unrelated samples - 3378 samples (also excludes outliers)\n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_FST_unrel = mt_FST_initial.filter_cols(unrel_samples.contains(mt_FST_initial['s'])) # filter mt\n",
    "print(f'Num of samples after filtering (unrelated samples) = {mt_FST_unrel.count_cols()}') # 3378 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56acd171",
   "metadata": {},
   "source": [
    "<a id='4.a.-F_ST-with-PLINK'></a>\n",
    "## 4.a. F<sub>ST</sub> with PLINK\n",
    "\n",
    "In order to calculate F<sub>ST</sub> using PLINK, we first need to export the mt as PLINK files.\n",
    "\n",
    "After exporting the files to PLINK format, the rest of the analysis is done using shell commands within the notebook. \n",
    "\n",
    "**Place <code>!</code> before the command you want to run and proceed as if you are running codes in a terminal.** You can use <code>! ls</code> after each run to check for ouputs in the directory and see if commands have run correctly. \n",
    " \n",
    "**Every time you start a new cluster, you will need to download PLINK to run the F<sub>ST</sub> analysis since downloads and files are discarded when a cluster is stopped.**  \n",
    "\n",
    "**Something to note**: when running the notebook on the Cloud, the shell commands still run even if we didn't use <code>!</code>. The same is true for when running the notebook locally. If you are having issues with the shell commands, we recommend trying both ways. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686377d",
   "metadata": {},
   "source": [
    "- Export mt as PLINK2 BED, BIM and FAM files - store on Google Cloud \n",
    "\n",
    "```python3\n",
    "hl.export_plink(mt_FST_unrel, plink_files_path, fam_id=mt_FST_unrel.hgdp_tgp_meta.population)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0997abb",
   "metadata": {},
   "source": [
    "### 4.a.1. PLINK Set up \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef02509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-10 19:48:36--  https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.230.96, 52.217.236.96, 52.216.57.176, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.230.96|:443... failed: Operation timed out.\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.236.96|:443... failed: Operation timed out.\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.57.176|:443... failed: Operation timed out.\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.235.96|:443... failed: Operation timed out.\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.200.181|:443... ^C\n",
      "unzip:  cannot find or open plink_linux_x86_64_20210606.zip, plink_linux_x86_64_20210606.zip.zip or plink_linux_x86_64_20210606.zip.ZIP.\n",
      "zsh:1: no such file or directory: ./plink\n"
     ]
    }
   ],
   "source": [
    "# Download PLINK using a link from the PLINK website (linux - recent version - 64 bit - stable) \n",
    "! wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
    "    \n",
    "# Unzip the \".gz\" file: \n",
    "! unzip plink_linux_x86_64_20210606.zip\n",
    "\n",
    "# A documentation output when you run this command indicates that PLINK has been installed properly \n",
    "! ./plink "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e29377",
   "metadata": {},
   "source": [
    "### 4.a.2. Files Set up \n",
    "\n",
    "Because F<sub>ST</sub> is computed among groups, we need to create a list of all pairs of populations.\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c083cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/hgdp_tgp.fam...\n",
      "/ [1 files][ 74.8 KiB/ 74.8 KiB]                                                \n",
      "Operation completed over 1 objects/74.8 KiB.                                     \n",
      "Copying gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/hgdp_tgp.bim...\n",
      "/ [1 files][  7.9 MiB/  7.9 MiB]                                                \n",
      "Operation completed over 1 objects/7.9 MiB.                                      \n",
      "Copying gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg/f2_fst/hgdp_tgp.bed...\n",
      "| [1 files][161.2 MiB/161.2 MiB]                                                \n",
      "Operation completed over 1 objects/161.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Copy the PLINK files that are stored on the Cloud to the current session directory\n",
    "! gsutil cp {plink_files_path}.fam . # fam \n",
    "! gsutil cp {plink_files_path}.bim . # bim\n",
    "! gsutil cp {plink_files_path}.bed . # bed\n",
    "\n",
    "# Obtain FID - in this case, it is the 78 populations in the first column of the FAM file\n",
    "! awk '{print $1}' hgdp_tgp.fam | sort -u > pop.codes\n",
    "\n",
    "# Make all possible combinations of pairs using the 78 populations \n",
    "! for i in `seq 78`; do for j in `seq 78`; do if [ $i -lt $j ]; then VAR1=`sed \"${i}q;d\" pop.codes`; VAR2=`sed \"${j}q;d\" pop.codes`; echo $VAR1 $VAR2; fi; done; done > pop.combos\n",
    "\n",
    "# Validity check \n",
    "! wc -l pop.combos # 3003\n",
    "\n",
    "# Create directories for intermediate files and F_ST results \n",
    "! mkdir within_files # intermediate files\n",
    "! mkdir FST_results # F_ST results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fc858",
   "metadata": {},
   "source": [
    "### 4.a.3. Scripts Set up \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1574a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 1 ####\n",
    "\n",
    "# For each population pair, set up a bash script to create a \"within\" file and run F_ST \n",
    "# Files to be produced: \n",
    "#    [pop_pairs].within will be saved in the \"within_files\" directory\n",
    "#    [pop_pairs].fst, [pop_pairs].log, and [pop_pairs].nosex will be saved in \"FST_results\" directory\n",
    "\n",
    "fst_script = '''    \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "\n",
    "# create \"within\" files for each population pair using the FAM file (columns 1,2 and 1 again)\n",
    "    awk -v r1=$POP1 -v r2=$POP2 '$1 == r1 || $1 == r2' hgdp_tgp.fam | awk '{ print $1, $2, $1 }' > within_files/${POP1}_${POP2}.within\n",
    "\n",
    "# run F_st\n",
    "    ./plink --bfile hgdp_tgp --fst --within within_files/${POP1}_${POP2}.within --out FST_results/${POP1}_${POP2}\n",
    "done'''\n",
    "\n",
    "with open('run_fst.py', mode='w') as file:\n",
    "    file.write(fst_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dede6f7",
   "metadata": {},
   "source": [
    "**Script 1 has to be run for script 2 to run** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a1167be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 2 ### \n",
    "\n",
    "# Use the \"[pop_pairs].log\" file produced from script 1 above (located in \"FST_results\" directory) to get the \"Mean F_st estimate\" for each population pair \n",
    "# Then compile all of the values in a single file for F_st heat map generation\n",
    "\n",
    "extract_mean_script = ''' \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "    mean_FST=$(tail -n4 FST_results/${POP1}_${POP2}.log | head -n 1 | awk -F: '{print $2}' | awk '{$1=$1};1')\n",
    "    printf \"%-20s\\t%-20s\\t%-20s\\n \" ${POP1} ${POP2} $mean_FST >> mean_fst_sum.txt\n",
    "done'''\n",
    "\n",
    "with open('extract_mean.py', mode='w') as file:\n",
    "    file.write(extract_mean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f5da",
   "metadata": {},
   "source": [
    "### 4.a.4. Run Scripts  \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f96c122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3003\n",
      "9009\n"
     ]
    }
   ],
   "source": [
    "# Run script 1 and direct the run log into a file (~20min to run) \n",
    "! sh run_fst.py > fst_script.log\n",
    "\n",
    "# Validity check \n",
    "! cd within_files/; ls | wc -l # 3003\n",
    "! cd FST_results/; ls | wc -l # 3003 * 3 = 9009 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a0503",
   "metadata": {},
   "source": [
    "**Script 2 requires script 1 to be run first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 2 (~1min to run)\n",
    "! sh extract_mean.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc7c52",
   "metadata": {},
   "source": [
    "- Copy the output from script 2 to the Cloud for heat map plotting in R \n",
    "\n",
    "```python3\n",
    "! gsutil cp mean_fst_sum.txt {final_mean_fst_path}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc1932",
   "metadata": {},
   "source": [
    "### The population-level F<sub>ST</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/FST_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
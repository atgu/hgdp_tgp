{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77298100",
   "metadata": {},
   "source": [
    "# Computing Population Genetics Statistics (*f*<sub>2</sub> and *F*<sub>ST</sub>)\n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16fb799",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Setting Default Paths](#1.-Set-Default-Paths)\n",
    "2. [Read in Pre-QC Dataset and Apply Quality Control Filters](#2.-Read-in-Pre-QC-Dataset-and-Apply-Quality-Control-Filters)\n",
    "3. [*f*<sub>2</sub> Analysis](#3.-f2-Analysis)\n",
    "4. [*F*<sub>ST</sub>](#4.-F_ST)\n",
    "    1. [*F*<sub>ST</sub> with PLINK](#4.a.-F_ST-with-PLINK)\n",
    "        1. [PLINK Set up](#4.a.1.-PLINK-Set-up)\n",
    "        2. [Files Set up](#4.a.2.-Files-Set-up)\n",
    "        3. [Scripts Set up](#4.a.3.-Scripts-Set-up) \n",
    "        4. [Run Scripts](#4.a.4.-Run-Scripts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ed1e3",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "\n",
    "The purpose of this notebook is to show two common population genetics analyses (*f*<sub>2</sub> and *F*<sub>ST</sub>) which are used to understand recent and deep history. \n",
    "\n",
    "*f*<sub>2</sub> analysis computes the number of SNVs that appear twice in a dataset and compares how often they are shared among individuals. Since doubletons are rare variants, they tend to have arisen relatively recently giving us information about recent population history. In contrast, *F*<sub>ST</sub> is a “fixation index” which calculates the extent of variation within versus between populations using SNVs of many frequencies. Because common variants are used in *F*<sub>ST</sub> analyses which arose a long time ago, this gives us information about older population history.\n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in a matrix table (mt) and filter using sample IDs that were obtained from another matrix table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a matrix table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain mt/table fields\n",
    "- Download a tool such as PLINK using a link and shell command \n",
    "- Run shell commands, set up a script & run it from inside a code block \n",
    "- Calculate *F*<sub>st</sub> using PLINK \n",
    "- Go through a log file and extract needed information\n",
    "- Write out results onto the Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd16205a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a8fb1882-db8c-497a-b48f-cfb812410b06\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"a8fb1882-db8c-497a-b48f-cfb812410b06\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"a8fb1882-db8c-497a-b48f-cfb812410b06\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"a8fb1882-db8c-497a-b48f-cfb812410b06\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a8fb1882-db8c-497a-b48f-cfb812410b06\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "\n",
    "# Functions from gnomAD library to apply genotype filters   \n",
    "from gnomad.utils.filtering import filter_to_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fbbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Hail \n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee48f2d",
   "metadata": {},
   "source": [
    "# 1. Set Default Paths\n",
    "\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets.\n",
    "\n",
    "**By default, all of the dataset write out sections are shown as markdown cells. If you would like to write out your own dataset, you can copy the code and paste it into a new code cell. Don't forget to change the paths in the following cell accordingly.** \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797918f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file path for f2 analysis - HGDP+1kGP dataset prior to applying gnomAD QC filters\n",
    "pre_qc_path = 'gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_dense.mt'\n",
    "\n",
    "# Path for gnomAD's HGDP+1kGP metadata for plotting \n",
    "metadata_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/metadata_and_qc/gnomad_meta_updated.tsv'\n",
    "\n",
    "# Path for unrelated samples mt without outliers -  for subsetting purposes\n",
    "unrelateds_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/pca_results/unrelateds_without_outliers.mt'\n",
    "\n",
    "# Path for final count table for the f2 analysis \n",
    "final_doubleton_count_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/f2_fst/doubleton_count.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd27421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning input file path for F_ST analysis - mt generated in Notebook 2: PCA and Ancestry Analyses after LD pruning\n",
    "fst_input_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/pca_preprocessing/ld_pruned.mt'\n",
    "\n",
    "# Path for exporting the PLINK files \n",
    "# Include file prefix at the end of the path - here the prefix is 'hgdp_tgp'\n",
    "plink_files_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/f2_fst/hgdp_tgp'\n",
    "    \n",
    "final_mean_fst_path = 'gs://gcp-public-data--gnomad/release/3.1/secondary_analyses/hgdp_1kg_v2/f2_fst/mean_fst.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f33eb4",
   "metadata": {},
   "source": [
    "# 2. Read in Pre-QC Dataset and Apply Quality Control Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28179ad7",
   "metadata": {},
   "source": [
    "Since the post-QC mt was not written out, we run the same function as the previous tutorial notebooks to apply the quality control filters to the pre-QC dataset.\n",
    "\n",
    "**To avoid errors, make sure to run the next two cells before running any code that includes the post-QC dataset.**\n",
    "\n",
    "**If running the cell below results in an error, double check that you used the  `--packages gnomad` argument when starting your cluster.**  \n",
    "\n",
    "- See the tutorials [README](https://github.com/atgu/hgdp_tgp/tree/master/tutorials#readme) for more information on how to start a cluster.\n",
    "\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/impex.html#hail.methods.read_matrix_table\"> More on  <i> read_matrix_table() </i></a></li>\n",
    "        \n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.describe\"> More on  <i> describe() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.count\"> More on  <i> count() </i></a></li>\n",
    "    \n",
    "<li><a href=\"https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.filter_cols\"> More on  <i> filter_cols() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.filter_rows\"> More on  <i> filter_rows() </i></a></li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4367a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function to:\n",
    "# apply gnomAD's sample, variant and genotype QC filters\n",
    "# remove two contaminated samples identified using CHARR - https://pubmed.ncbi.nlm.nih.gov/37425834/\n",
    "# remove the gnomAD sample that's added for QC purposes\n",
    "# only keep the variants which are found in the samples that are left \n",
    "# add gnomAD's HGDP+1kGP metadata with the updated population labels as a column field \n",
    "\n",
    "def run_qc(mt):\n",
    "    \n",
    "    ## Apply sample QC filters to dataset \n",
    "    # This filters to only samples that passed gnomAD's sample QC hard filters  \n",
    "    mt = mt.filter_cols(~mt.gnomad_sample_filters.hard_filtered) # removed 31 samples\n",
    "    \n",
    "    ## Apply variant QC filters to dataset\n",
    "    # This subsets to only PASS variants - those which passed gnomAD's variant QC\n",
    "    # PASS variants have an entry in the filters field \n",
    "    mt = mt.filter_rows(hl.len(mt.filters) != 0, keep=False)\n",
    "    \n",
    "    # Remove the two contaminated samples identified by CHARR and 'CHMI_CHMI3_WGS2'\n",
    "    contaminated_samples = {'HGDP01371', 'LP6005441-DNA_A09'}\n",
    "    contaminated_samples_list = hl.literal(contaminated_samples)\n",
    "    mt = mt.filter_cols(~contaminated_samples_list.contains(mt['s']))\n",
    "    \n",
    "    # CHMI_CHMI3_WGS2 is a sample added by gnomAD for QC purposes and has no metadata info \n",
    "    mt = mt.filter_cols(mt.s == 'CHMI_CHMI3_WGS2', keep = False)\n",
    "\n",
    "    # Only keep the variants which are found in the samples that are left \n",
    "    mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref()))\n",
    "    \n",
    "    # Read in and add the metadata with the updated population labels as a column field \n",
    "    metadata = hl.import_table(metadata_path, impute = True, key = 's') \n",
    "    mt = mt.annotate_cols(meta_updated = metadata[mt.s])\n",
    "    \n",
    "    ## Apply genotype QC filters to the dataset\n",
    "    # This is done using a function imported from gnomAD and is the last step in the QC process\n",
    "    mt = filter_to_adj(mt)\n",
    "\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the HGDP+1kGP pre-QC mt\n",
    "pre_qc_mt = hl.read_matrix_table(pre_qc_path)\n",
    "\n",
    "# Run QC \n",
    "mt_filt = run_qc(pre_qc_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37091c2e",
   "metadata": {},
   "source": [
    "<a id='3.-f2-Analysis'></a>\n",
    "\n",
    "# 3. *f*<sub>2</sub> Analysis\n",
    "\n",
    "\n",
    "We are running *f*<sub>2</sub> on unrelated samples only so for subsetting, we are using the unrelateds mt which was generated after removing outliers and rerunning PCA in <a href=\"https://nbviewer.org/github/atgu/hgdp_tgp/blob/master/tutorials/nb4.ipynb\">Notebook 2</a>. After obtaining the desired samples, we run Hail's common variant statistics so we can separate out doubletons*. Once we have the doubletons filtered, we then remove variants with a call rate less than 0.05 (no more than 5% missingness/low missingness).\n",
    "\n",
    "*Doubletons are variants that show up twice and only twice in a dataset and useful in detecting rare variance & understanding recent history.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53fc4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took ~20min to run \n",
    "\n",
    "# Filter to only unrelated samples - 3400 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt \n",
    "\n",
    "# Only keep the variants which are found in the samples that are left \n",
    "mt_filt_unrel = mt_filt_unrel.filter_rows(hl.agg.any(mt_filt_unrel.GT.is_non_ref()))\n",
    "#print(f'Num of SNVs and samples after filtering (unrelated samples) = {mt_filt_unrel.count()}') # 151908201 variants and 3400 samples\n",
    "\n",
    "# Run common variant statistics (quality control metrics)  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# Separate the AC array into individual fields and extract the doubletons  \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "#print(f'Num of variants that are doubletons = {mt_unrel_only2.count_rows()}') # 17385333 variants\n",
    "\n",
    "# Remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "#print(f'Num of variants > 0.05 = {mt_unrel_only2_filtered.count_rows()}') # 17337272 variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444448cd",
   "metadata": {},
   "source": [
    "The next step is to check which allele, reference (ref) or alternate (alt), is the the doubleton. If the first element of the array in the allele frequency field (AF[0]) is less than the second elelement (AF[1]), then the doubleton is the 1st allele (ref). If the first element (AF[0]) is greater than the second elelement (AF[1]), then the doubleton is the 2nd allele (alt).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c135fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>(49999 + 1) / 50000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of variants where the 1st allele (ref) is the doubleton = 2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================================>(49999 + 1) / 50000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of variants where the 2nd allele (alt) is the doubleton = 17334314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                    (0 + 156) / 50000]\r"
     ]
    }
   ],
   "source": [
    "# This code chunk took 44min to run because of the print commands which we've commented out here\n",
    "\n",
    "# Check allele frequency (AF) to see if the doubleton is the ref or alt allele \n",
    "\n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print(f'Num of variants where the 1st allele (ref) is the doubleton = {mt_doubl_ref.count_rows()}') # 2958 variants\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print(f'Num of variants where the 2nd allele (alt) is the doubleton = {mt_doubl_alt.count_rows()}') # 17226764 variants\n",
    "\n",
    "# Validity check: should print True\n",
    "mt_doubl_ref.count_rows() + mt_doubl_alt.count_rows() == mt_unrel_only2_filtered.count_rows() # True\n",
    "print(2958 + 17334314 == 17337272) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad8d3a",
   "metadata": {},
   "source": [
    "Once we've figured out which allele is the doubleton and divided the doubleton matrix table accordingly, the next step is to find the samples that have doubletons, compile them in a set, and annotate that onto the mt as a new row field. This done for each mt separately and can be achieved by looking at the genotype call (GT) field. When the doubleton is the 1st allele (ref), the genotype call would be 0|1 & 0|0. When the doubleton is the 2nd allele (alt), the genotype call would be 0|1 & 1|1. \n",
    "\n",
    "We chose a set for the results instead of a list because a list isn't hashable and the next step wouldn't run. After the annotation of the new row field in each mt, we then count how many times a sample or a sample pair appears within that field and store the results in a dictionary. Once we have the two dictionaries (one for the ref and one for the alt), we merge them into one and add up the values for identical keys.\n",
    "\n",
    "If you want to do a validity check at this point, you can add up the count of the two dictionaries and then subtract the number of keys that intersect between the two. The value that you get should be equal to the length of the combined dictionary.  \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58216cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.>                             (19644 + 168) / 50000]\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[Stage 8:======================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary = 3015536\n"
     ]
    }
   ],
   "source": [
    "# This code chunk took 21min to run\n",
    "\n",
    "# For each mt, find the samples that have doubletons, compile them in a set, and add as a new row field \n",
    "\n",
    "# Doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# If there is one sample in the new column field then it's 0|0\n",
    "# If there are two samples, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons=hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1)) | (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "# Doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# If there is one sample in the new column field then it's 1|1\n",
    "# If there are two samples, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons=hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1)) | (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n",
    "\n",
    "# Count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field - returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))\n",
    "\n",
    "# Combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "print(f'Length of dictionary = {len(all_doubl_count)}') # 3015536\n",
    "\n",
    "# Validity check \n",
    "#len(all_doubl_count) == (len(ref_doubl_count) + len(alt_doubl_count)) - # of keys that intersect b/n the two dictionaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19bfc8",
   "metadata": {},
   "source": [
    "For the next step, we get a list of samples that are in the doubleton mt and also create sample pairs out of them. We also divide the combined dictionary into two: one for when a sample is a key by itself (len(key) == 1) and the other for when the dictionary key is a pair of samples (len(key) != 1). \n",
    "\n",
    "We then go through the lists of samples obtained from the mt and see if any of them are keys in their respective doubleton dictionaries:\n",
    "- list of samples by themselves is compared against the dictionary that has a single sample as a key \n",
    "- the list with sample pairs is compared against the dictionary where the key is a pair of samples \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5600b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Get list of samples from mt - 3400 samples \n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# Make pairs from sample list: n(n-1)/2 - 5778300 pairs \n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(mt_sample_list) for j,y in enumerate(mt_sample_list) if i<j]\n",
    "\n",
    "# Subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "# subset dict to keys with sample pairs (not just 1)\n",
    "dict_pair_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) != 1}\n",
    "\n",
    "# Validity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True\n",
    "\n",
    "# Are the samples in the list the same as the dict keys?\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True \n",
    "\n",
    "# Are the sample pairs obtained from the mt equal to what's in the pair dict? \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from the mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3e2c1",
   "metadata": {},
   "source": [
    "If a single sample is a key in the single-sample-key dictionary, we record the sample ID twice and it's corresponding value from the dictionary. If it is not a key, we record the sample ID twice and set the value to 0. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25f3679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Single sample comparison \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# Validity check \n",
    "# For the single samples, the length should be consistent across dict, mt sample list, and final list\n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef886c67",
   "metadata": {},
   "source": [
    "If a sample pair is a key in the sample-pair-key dictionary, we record the two sample IDs and the corresponding value from the dictionary. If that is not the case, we record the two sample IDs and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16f335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Sample pair comparison\n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# Validity check \n",
    "# Length of final list should be equal to the length of the sample list obtained from the mt \n",
    "print(len(sample_pair_final_list) == len(mt_sample_pairs)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a6141",
   "metadata": {},
   "source": [
    "Last step is to combine the two lists obtained from the comparisons, convert that into a pandas table, format it as needed, and write it out as a csv so the values can be plotted as a heat map in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b7dccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# Validity check \n",
    "print(len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list)) # True\n",
    "\n",
    "# Format list as pandas table \n",
    "df = pd.DataFrame(final_list)\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) # rename column names "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743196b",
   "metadata": {},
   "source": [
    "- Write out table to the Cloud so it can be plotted in R \n",
    "\n",
    "```python3\n",
    "df.to_csv(final_doubleton_count_path, index=False, sep='\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f0c99",
   "metadata": {},
   "source": [
    "### The sample-level *f*<sub>2</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/figure_generation/F2_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f47da",
   "metadata": {},
   "source": [
    "<a id='4.-F_ST'></a>\n",
    "# 4. *F*<sub>ST</sub>\n",
    "\n",
    "*F*<sub>ST</sub> detects genetic divergence from common variance allowing us to understand past deep history. Similar to the *f*<sub>2</sub> analysis, we are running *F*<sub>ST</sub> on unrelated samples only. However, here we are starting with a filtered and pruned dataset instead of the pre-QC dataset (without outliers).\n",
    "\n",
    "**Something to note**: Since the mt we are starting with is filtered and pruned, running <code>hl.variant_qc</code> and filtering to variants with <code>call rate > 0.05 </code> (similar to what we did for the *f*<sub>2</sub> analysis) doesn't make a difference to the number of variants.\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e48cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the filtered and pruned mt with outliers - 200403 variants and 4120 samples\n",
    "mt_FST_initial = hl.read_matrix_table(fst_input_path) \n",
    "\n",
    "# Filter to only unrelated samples - 3400 samples (also excludes outliers)\n",
    "mt_unrel = hl.read_matrix_table(unrelateds_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_FST_unrel = mt_FST_initial.filter_cols(unrel_samples.contains(mt_FST_initial['s'])) # filter mt\n",
    "\n",
    "# Only keep the variants which are found in the samples that are left \n",
    "mt_FST_unrel = mt_FST_unrel.filter_rows(hl.agg.any(mt_FST_unrel.GT.is_non_ref()))\n",
    "#print(f'Num of SNVs and samples after filtering (unrelated samples) = {mt_FST_unrel.count()}') # 200403 variants and 3400 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b0da1",
   "metadata": {},
   "source": [
    "<a id='4.a.-F_ST-with-PLINK'></a>\n",
    "## 4.a. *F*<sub>ST</sub> with PLINK\n",
    "\n",
    "In order to calculate *F*<sub>ST</sub> using PLINK, we first need to export the mt as PLINK files.\n",
    "\n",
    "After exporting the files to PLINK format, the rest of the analysis is done using shell commands within the notebook. \n",
    "\n",
    "**Place <code>!</code> before the command you want to run and proceed as if you are running codes in a terminal.** You can use <code>! ls</code> after each run to check for ouputs in the directory and see if commands have run correctly. \n",
    " \n",
    "**Every time you start a new cluster, you will need to download PLINK to run the *F*<sub>ST</sub> analysis since downloads and files are discarded when a cluster is stopped.**  \n",
    "\n",
    "**Something to note**: when running the notebook on the Cloud, the shell commands still run even if we didn't use <code>!</code>. The same is true for when running the notebook locally. If you are having issues with the shell commands, we recommend trying both ways. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697a9f7",
   "metadata": {},
   "source": [
    "- Export mt as PLINK2 BED, BIM and FAM files - store on Google Cloud \n",
    "\n",
    "```python3\n",
    "hl.export_plink(mt_FST_unrel, plink_files_path, fam_id=mt_FST_unrel.meta_updated.population)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c5dd6",
   "metadata": {},
   "source": [
    "### 4.a.1. PLINK Set up \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cc80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-24 15:52:35--  https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.174.88, 52.217.95.104, 54.231.132.192, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.174.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8917076 (8.5M) [application/zip]\n",
      "Saving to: ‘plink_linux_x86_64_20210606.zip’\n",
      "\n",
      "plink_linux_x86_64_ 100%[===================>]   8.50M  27.6MB/s    in 0.3s    \n",
      "\n",
      "2024-01-24 15:52:35 (27.6 MB/s) - ‘plink_linux_x86_64_20210606.zip’ saved [8917076/8917076]\n",
      "\n",
      "Archive:  plink_linux_x86_64_20210606.zip\n",
      "  inflating: plink                   \n",
      "  inflating: LICENSE                 \n",
      "  inflating: toy.ped                 \n",
      "  inflating: toy.map                 \n",
      "  inflating: prettify                \n",
      "PLINK v1.90b6.24 64-bit (6 Jun 2021)           www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2021 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "\n",
      "  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n",
      "  plink --help [flag name(s)...]\n",
      "\n",
      "Commands include --make-bed, --recode, --flip-scan, --merge-list,\n",
      "--write-snplist, --list-duplicate-vars, --freqx, --missing, --test-mishap,\n",
      "--hardy, --mendel, --ibc, --impute-sex, --indep-pairphase, --r2, --show-tags,\n",
      "--blocks, --distance, --genome, --homozyg, --make-rel, --make-grm-gz,\n",
      "--rel-cutoff, --cluster, --pca, --neighbour, --ibs-test, --regress-distance,\n",
      "--model, --bd, --gxe, --logistic, --dosage, --lasso, --test-missing,\n",
      "--make-perm-pheno, --tdt, --qfam, --annotate, --clump, --gene-report,\n",
      "--meta-analysis, --epistasis, --fast-epistasis, and --score.\n",
      "\n",
      "\"plink --help | more\" describes all functions (warning: long).\n"
     ]
    }
   ],
   "source": [
    "# Download PLINK using a link from the PLINK website (linux - recent version - 64 bit - stable) \n",
    "! wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
    "    \n",
    "# Unzip the \".gz\" file: \n",
    "! unzip plink_linux_x86_64_20210606.zip\n",
    "\n",
    "# A documentation output when you run this command indicates that PLINK has been installed properly \n",
    "! ./plink "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbad07a",
   "metadata": {},
   "source": [
    "### 4.a.2. Files Set up \n",
    "\n",
    "Because *F*<sub>ST</sub> is computed among groups, we need to create a list of all pairs of populations.\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2493f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://hgdp-1kg/tutorial_datasets/f2_fst/hgdp_tgp.fam...\n",
      "/ [1 files][ 75.4 KiB/ 75.4 KiB]                                                \n",
      "Operation completed over 1 objects/75.4 KiB.                                     \n",
      "Copying gs://hgdp-1kg/tutorial_datasets/f2_fst/hgdp_tgp.bim...\n",
      "/ [1 files][  8.0 MiB/  8.0 MiB]                                                \n",
      "Operation completed over 1 objects/8.0 MiB.                                      \n",
      "Copying gs://hgdp-1kg/tutorial_datasets/f2_fst/hgdp_tgp.bed...\n",
      "- [1 files][162.4 MiB/162.4 MiB]                                                \n",
      "Operation completed over 1 objects/162.4 MiB.                                    \n",
      "3160 pop.combos\n"
     ]
    }
   ],
   "source": [
    "# Copy the PLINK files that are stored on the Cloud to the current session directory\n",
    "! gsutil cp {plink_files_path}.fam . # fam \n",
    "! gsutil cp {plink_files_path}.bim . # bim\n",
    "! gsutil cp {plink_files_path}.bed . # bed\n",
    "\n",
    "# Obtain FID - in this case, it is the 80 populations in the first column of the FAM file\n",
    "! awk '{print $1}' hgdp_tgp.fam | sort -u > pop.codes\n",
    "\n",
    "# Make all possible combinations of pairs using the 80 populations \n",
    "! for i in `seq 80`; do for j in `seq 80`; do if [ $i -lt $j ]; then VAR1=`sed \"${i}q;d\" pop.codes`; VAR2=`sed \"${j}q;d\" pop.codes`; echo $VAR1 $VAR2; fi; done; done > pop.combos\n",
    "\n",
    "# Validity check \n",
    "! wc -l pop.combos # 3160 combinations - ((80-1)*80)/2\n",
    "\n",
    "# Create directories for intermediate files and F_ST results \n",
    "! mkdir within_files # intermediate files\n",
    "! mkdir FST_results # F_ST results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2774045",
   "metadata": {},
   "source": [
    "### 4.a.3. Scripts Set up \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3525562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 1 ####\n",
    "\n",
    "# For each population pair, set up a bash script to create a \"within\" file and run F_ST \n",
    "# Files to be produced: \n",
    "#    [pop_pairs].within will be saved in the \"within_files\" directory\n",
    "#    [pop_pairs].fst, [pop_pairs].log, and [pop_pairs].nosex will be saved in \"FST_results\" directory\n",
    "\n",
    "fst_script = '''    \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3160`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "\n",
    "# create \"within\" files for each population pair using the FAM file (columns 1,2 and 1 again)\n",
    "    awk -v r1=$POP1 -v r2=$POP2 '$1 == r1 || $1 == r2' hgdp_tgp.fam | awk '{ print $1, $2, $1 }' > within_files/${POP1}_${POP2}.within\n",
    "\n",
    "# run F_st\n",
    "    ./plink --bfile hgdp_tgp --fst --within within_files/${POP1}_${POP2}.within --out FST_results/${POP1}_${POP2}\n",
    "done'''\n",
    "\n",
    "with open('run_fst.py', mode='w') as file:\n",
    "    file.write(fst_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d479c",
   "metadata": {},
   "source": [
    "**Script 1 has to be run for script 2 to run** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f6c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 2 ### \n",
    "\n",
    "# Use the \"[pop_pairs].log\" file produced from script 1 above (located in \"FST_results\" directory) to get the \"Mean F_st estimate\" for each population pair \n",
    "# Then compile all of the values in a single file for F_st heat map generation\n",
    "\n",
    "extract_mean_script = ''' \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3160`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "    mean_FST=$(tail -n4 FST_results/${POP1}_${POP2}.log | head -n 1 | awk -F: '{print $2}' | awk '{$1=$1};1')\n",
    "    printf \"%-20s\\t%-20s\\t%-20s\\n \" ${POP1} ${POP2} $mean_FST >> mean_fst_sum.txt\n",
    "done'''\n",
    "\n",
    "with open('extract_mean.py', mode='w') as file:\n",
    "    file.write(extract_mean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a868279",
   "metadata": {},
   "source": [
    "### 4.a.4. Run Scripts  \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c06bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3160\n",
      "9480\n"
     ]
    }
   ],
   "source": [
    "# Run script 1 and direct the run log into a file (~20min to run) \n",
    "! sh run_fst.py > fst_script.log\n",
    "\n",
    "# Validity check \n",
    "! cd within_files/; ls | wc -l # 3160\n",
    "! cd FST_results/; ls | wc -l # 3160 * 3 = 9480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa4f49",
   "metadata": {},
   "source": [
    "**Script 2 requires script 1 to be run first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab697cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 2 (~1min to run)\n",
    "! sh extract_mean.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fd34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://mean_fst_sum.txt [Content-Type=text/plain]...\n",
      "/ [1 files][197.5 KiB/197.5 KiB]                                                \n",
      "Operation completed over 1 objects/197.5 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp mean_fst_sum.txt {final_mean_fst_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0ac5f",
   "metadata": {},
   "source": [
    "- Copy the output from script 2 to the Cloud for heat map plotting in R \n",
    "\n",
    "```python3\n",
    "! gsutil cp mean_fst_sum.txt {final_mean_fst_path}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b148d",
   "metadata": {},
   "source": [
    "### The population-level *F*<sub>ST</sub> heatmap was plotted in R using this [code](https://github.com/atgu/hgdp_tgp/blob/master/figure_generation/FST_heatmap.Rmd).\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

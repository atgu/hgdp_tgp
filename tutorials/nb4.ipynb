{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ee735e",
   "metadata": {},
   "source": [
    "Notebook 5: F2 and F_st\n",
    "\n",
    "1. F_st in Hail if there is time\n",
    "2. Heat maps for this analysis were plotted using R - will be kept that way\n",
    "    - once you edit and organize the R codes, link them here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac40b0",
   "metadata": {},
   "source": [
    "# Computing Population Genetics Statistics (F2 and F<sub>st</sub>)\n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921c368",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Setting Default Paths](#1.-Set-Default-Paths)\n",
    "2. [F2 analysis](#2.-F2-analysis)\n",
    "3. [F<sub>ST</sub>](#3.-F_st)\n",
    "    1. [F<sub>ST</sub> with PLINK](#3a.-F_st-with-PLINK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df87f6e",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "\n",
    "The purpose of this notebook is to show two common population genetics analyses (F2 and F<sub>ST</sub>) used to understand recent and deep history. F2 analyses compute the number of SNVs that appear twice in a dataset and compares how often they are shared among individuals. Since doubletons are rare variants, they tend to have arisen relatively recently, so give us information about recent population history. In contrast, F<sub>st</sub> is a “fixation index” which calculates the extent of variation within versus between populations using SNVs of many frequencies. Because common variants are used in F<sub>st</sub> analyses which arose a long time ago, this gives us information about older population history.\n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in a Matrix Table (mt) and filter using sample IDs that were obtained from another Matrix Table \n",
    "- Separate a field into multiple fields\n",
    "- Filter using the call rate field \n",
    "- Extract doubletons and check if they are the reference or alternate allele\n",
    "- Count how many times a sample or a sample pair appears in a field \n",
    "- Combine two dictionaries and add up the values for identical keys\n",
    "- Format list as pandas table \n",
    "- Export a Matrix Table as PLINK2 BED, BIM and FAM files \n",
    "- Set up a pair-wise comparison\n",
    "- Drop certain fields\n",
    "- Download a tool such as PLINK using a link and shell command \n",
    "- Run shell commands, and set up a script & run it from inside a code block \n",
    "- Calculate F<sub>st</sub> in PLINK (once there is progress on this, I will elaborate more) \n",
    "- Go through a log file and extract needed information\n",
    "- Write out results onto the Cloud \n",
    "- Plot certain fields from the Matrix Table:\n",
    "    - Heterozygosity (plot distribution to show how it doesn’t work with diverse populations as expected (e.g. high for AFR, low for FIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ea66",
   "metadata": {},
   "source": [
    "# 1. Set Default Paths\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets. \n",
    "\n",
    "By default all of the write sections are shown as markdown cells. If you would like to write out your own datasets, you can copy the code and paste it into a new code cell. \n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate file - beginning input file for F2 analysis \n",
    "f2_input_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt'\n",
    "\n",
    "# Unrelated samples mt - for subsetting purposes \n",
    "unrelated_path = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt'\n",
    "\n",
    "# Final F2 output \n",
    "f2_final_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton_sample_pair_count_tbl.csv'\n",
    "\n",
    "# Annotation file for F2 heat map \n",
    "f2_annotation_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/sampleID_pop_reg.txt'\n",
    "\n",
    "# Beginning input file for F_st analysis \n",
    "fst_input_path = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/whole_dataset.mt'\n",
    "\n",
    "# Path for exporting the PLINK files \n",
    "# Include file prefix at the end of the path - here the prefix is 'hgdp_tgp'\n",
    "plink_files_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/FST/PLINK/unrel/hgdp_tgp'\n",
    "\n",
    "# Final F_st output  \n",
    "fst_final_path = 'gs://hgdp-1kg/hgdp_tgp/FST_F2/FST/PLINK/mean_fst_sum_UPDATED.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e9546",
   "metadata": {},
   "source": [
    "# 2. F2 analysis\n",
    "\n",
    "<details><summary> For more information on the datasets we are using for F2 analysis click <u><span style=\"color:blue\">here</span></u>.</summary>\n",
    "\n",
    "The file being imported here has:\n",
    "<ul>\n",
    "<li>run through gnomAD's sample QC filters</li> \n",
    "<li>run through gnomAD's variant QC filters</li>\n",
    "<li>had PCA outliers removed</li>\n",
    "<li>not been LD pruned.</li>\n",
    "</ul>\n",
    "\n",
    "We are running F2 on unrelated samples only and using a dataset generated after removing outliers and rerunning PCA (<a href=\"https://nbviewer.org/github/atgu/hgdp_tgp/blob/master/tutorials/nb4.ipynb\">nb4</a>) for subsetting. After obtaining the desired samples, we run Hail's common variant statistics so we can separate out doubletons*. Once we have the doubletons filtered, we then remove variants with a call rate less than 0.05 (no more than 5% missingness/low missingness).\n",
    "\n",
    "*Doubletons are variants that show up twice and only twice in a data set and useful in detecting rare variance & understanding recent history.\n",
    "\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in intermediate file \n",
    "mt_filt = apply_qc(post_qc=True)\n",
    "\n",
    "# Filter to only unrelated samples - 3380 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelated_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_filt_unrel = mt_filt.filter_cols(unrel_samples.contains(mt_filt['s'])) # filter mt \n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_filt_unrel.count()[1])) # 3380 samples\n",
    "\n",
    "# Run common variant statistics (quality control metrics)  \n",
    "mt_unrel_varqc = hl.variant_qc(mt_filt_unrel)\n",
    "\n",
    "# Separate the AC array into individual fields and extract the doubletons  \n",
    "mt_unrel_interm = mt_unrel_varqc.annotate_rows(AC1 = mt_unrel_varqc.variant_qc.AC[0], AC2 = mt_unrel_varqc.variant_qc.AC[1])\n",
    "mt_unrel_only2 = mt_unrel_interm.filter_rows((mt_unrel_interm.AC1 == 2) | (mt_unrel_interm.AC2 == 2))\n",
    "print('Num of variants that are doubletons = ' + str(mt_unrel_only2.count()[0])) # 18018978 variants \n",
    "\n",
    "# Write out an intermediate file and read it back in (saving took ~23 min) - MIGHT NOT NEED THIS LATER (DECIDE ONCE RUN WITH THE NEW DS)\n",
    "#mt_unrel_only2.write('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt', overwrite=False)\n",
    "#mt_unrel_only2 = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/FST_F2/F2/doubleton.mt')\n",
    "\n",
    "# remove variants with call rate < 0.05 (no more than 5% missingness/low missingness)  \n",
    "mt_unrel_only2_filtered = mt_unrel_only2.filter_rows(mt_unrel_only2.variant_qc.call_rate > 0.05)\n",
    "print('Num of variants > 0.05 = ' + str(mt_unrel_only2_filtered.count()[0])) # 17997741 variants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20fd2e",
   "metadata": {},
   "source": [
    "The next step is to check which allele, reference (ref) or alternate (alt), is the the doubleton. If the first element of the array in the allele frequency field (AF[0]) is less than the second elelement (AF[1]), then the doubleton is 1st allele (ref). If the first element (AF[0]) is greater than the second elelement (AF[1]), then the doubleton is 2nd allele (alt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a229812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check allele frequency (AF) to see if the doubleton is the ref or alt allele \n",
    "\n",
    "# AF[0] < AF[1] - doubleton is 1st allele (ref)\n",
    "mt_doubl_ref = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] < mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print('Num of variants where the 1st allele (ref) is the doubleton = ' + str(mt_doubl_ref.count()[0])) # 3159 variants\n",
    "\n",
    "\n",
    "# AF[0] > AF[1] - doubleton is 2nd allele (alt)\n",
    "mt_doubl_alt = mt_unrel_only2_filtered.filter_rows((mt_unrel_only2_filtered.variant_qc.AF[0] > mt_unrel_only2_filtered.variant_qc.AF[1]))\n",
    "print('Num of variants where the 2nd allele (alt) is the doubleton = ' + str(mt_doubl_alt.count()[0])) # 17994582 variants\n",
    "\n",
    "# Validity check, should print True\n",
    "mt_doubl_ref.count()[0] + mt_doubl_alt.count()[0] == mt_unrel_only2_filtered.count()[0]\n",
    "print(3159 + 17994582 == 17997741) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85cb8c",
   "metadata": {},
   "source": [
    "Once we have figured out which allele is the doubleton and divided the doubleton Matrix Table accordingly, the next step is to find the samples that have doubletons, compile them in a set, and annotate that onto the mt as a new row field. This done for each mt separately and can be achieved by looking at the genotype call (GT) field. When the doubleton is 1st allele (ref), the genotype call would be 0|1 & 0|0. When the doubleton is 2nd allele (alt), the genotype call would be 0|1 & 1|1. We chose a set for the results instead of a list because a list isn't hashable and the next step wouldn't have run. After the annotation of the new row field in each mt, we then count how many times a sample or a sample pair appears within that field and store the results in a dictionary. Once we have the two dictionaries (one for the ref and one for the alt), we merge them into one and add up the values for identical keys.\n",
    "\n",
    "If you want to do a validity check at this point, you can add up the count of the two dictionaries and then subtract the number of keys that intersect between the two. The value that you get should be equal to the length of the combined dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each mt, find the samples that have doubletons, compile them in a set, and add as a new row field \n",
    "\n",
    "# Doubleton is 1st allele (ref) - 0|1 & 0|0\n",
    "# If there is one sample in the new column field then it's 0|0. If there are two samples, then it's 0|1\n",
    "mt_ref_collected = mt_doubl_ref.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_ref.GT == hl.call(0, 1))| (mt_doubl_ref.GT == hl.call(0, 0)), hl.agg.collect_as_set(mt_doubl_ref.s)))\n",
    "\n",
    "# Doubleton is 2nd allele (alt) - 0|1 & 1|1\n",
    "# If there is one sample in the new column field then it's 1|1. If there are two samples, then it's 0|1\n",
    "mt_alt_collected = mt_doubl_alt.annotate_rows(\n",
    "    samples_with_doubletons = hl.agg.filter(\n",
    "        (mt_doubl_alt.GT == hl.call(0, 1))| (mt_doubl_alt.GT == hl.call(1, 1)), hl.agg.collect_as_set(mt_doubl_alt.s)))\n",
    "\n",
    "# Count how many times a sample or a sample pair appears in the \"samples_with_doubletons\" field - returns a dictionary\n",
    "ref_doubl_count = mt_ref_collected.aggregate_rows(hl.agg.counter(mt_ref_collected.samples_with_doubletons))\n",
    "alt_doubl_count = mt_alt_collected.aggregate_rows(hl.agg.counter(mt_alt_collected.samples_with_doubletons))\n",
    "\n",
    "# Combine the two dictionaries and add up the values for identical keys  \n",
    "all_doubl_count = {k: ref_doubl_count.get(k, 0) + alt_doubl_count.get(k, 0) for k in set(ref_doubl_count) | set(alt_doubl_count)}\n",
    "print('Length of dictionary = ' + str(len(all_doubl_count))) # 3183039 \n",
    "\n",
    "# Validity check \n",
    "## len(all_doubl_count) == (len(ref_doubl_count) + len(alt_doubl_count)) - # of keys that intersect b/n the two dictionaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a20180",
   "metadata": {},
   "source": [
    "For the next step, we get a list of samples that are in the doubleton mt and also create sample pairs out of them. We also divide the combined dictionary into two: one for when a sample is a key by itself (len(key) == 1) and the other for when the dictionary key is a pair of samples (len(key) != 1). We then go through the lists of samples obtained from the mt and see if any of them are keys in their respective doubleton dictionaries - list of samples by themselves is compared against the dictionary that has a single sample as a key and the list with sample pairs is compared against the dictionary where the key is a pair of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of samples from mt\n",
    "mt_sample_list = mt_unrel_only2_filtered.s.collect()\n",
    "\n",
    "# Make pairs from sample list: n(n-1)/2 - 5710510 pairs \n",
    "mt_sample_pairs = [{x,y} for i, x in enumerate(sample_list) for j,y in enumerate(sample_list) if i<j]\n",
    "\n",
    "# Subset dict to only keys with length of 1 - one sample \n",
    "dict_single_samples = {x:all_doubl_count[x] for x in all_doubl_count if len(x) == 1}\n",
    "\n",
    "# Validity check \n",
    "print(len(dict_single_samples) + len(dict_pair_samples) == len(all_doubl_count)) # True\n",
    "# Are the samples in the list the same as the dict keys?\n",
    "print(len(mt_sample_list) == len(dict_single_samples)) # True \n",
    "# Are the sample pairs obtained from the mt equal to what's in the pair dict? \n",
    "print(len(mt_sample_pairs) == len(dict_pair_samples)) # False - there are more sample pairs obtained from the mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d77b3",
   "metadata": {},
   "source": [
    "If a single sample is a key in the single-sample-key dictionary, we record the sample ID twice and it's corresponding value from the dictionary. If it is not a key, we record the sample ID twice and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample comparison \n",
    "single_sample_final_list = [[s, s, 0] if dict_single_samples.get(frozenset([s])) is None else [s, s, dict_single_samples[frozenset([s])]] for s in mt_sample_list]\n",
    "\n",
    "# Validity check \n",
    "# For the single samples, the length should be consistent across dict, mt sample list, and final list\n",
    "print(len(single_sample_final_list) == len(mt_sample_list) == len(dict_single_samples)) # True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b81c4",
   "metadata": {},
   "source": [
    "If a sample pair is a key in the sample-pair-key dictionary, we record the two sample IDs and the corresponding value from the dictionary. If that is not the case, we record the two sample IDs and set the value to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pair comparison\n",
    "sample_pair_final_list = [[list(s)[0], list(s)[1], 0] if dict_pair_samples.get(frozenset(list(s))) is None else [list(s)[0], list(s)[1], dict_pair_samples[frozenset(list(s))]] for s in mt_sample_pairs]\n",
    "\n",
    "# Validity check \n",
    "# Length of final list should be equal to the length of the sample list obtained from the mt \n",
    "print(len(sample_pair_final_list) == len(mt_sample_pairs)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21564e",
   "metadata": {},
   "source": [
    "Last step is to combine the two lists obtained from the comparisons, convert that into a pandas table, format it as needed, and write it out as a csv so that the values can be plotted as a heat map in R. For plot annotation purposes, we also export the sample IDs and their respective populations & genetic regions.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = single_sample_final_list + sample_pair_final_list\n",
    "\n",
    "# Validity check \n",
    "len(final_list) == len(single_sample_final_list) + len(sample_pair_final_list) # True\n",
    "\n",
    "# Format list as pandas table \n",
    "df = pd.DataFrame(final_list)\n",
    "df.rename({0:'sample1', 1:'sample2', 2:'count'}, axis=1, inplace=True) # rename column names \n",
    "\n",
    "# Save table to the Cloud so it can be plotted in R \n",
    "df.to_csv(f2_final_path, index=False, sep='\\t')\n",
    "\n",
    "# Save sample IDs and their respective populations & genetic regions for heat map annotation \n",
    "sampleID_pop_reg = (mt_unrel_only2_filtered.select_cols(mt_unrel_only2_filtered['hgdp_tgp_meta']['Population'], mt_unrel_only2_filtered['hgdp_tgp_meta']['Genetic']['region'])).cols()\n",
    "sampleID_pop_reg.export(f2_annotation_path, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc78ebc",
   "metadata": {},
   "source": [
    "# 3. F<sub>st</sub>\n",
    "\n",
    "\n",
    "F<sub>st</sub> detects genetic divergence from common variance allowing us to understand past deep history. \n",
    "\n",
    "In this tutorial we are running F<sub>st</sub> on only unrelated samples, unlike the F2 analysis.\n",
    "To do this we are using a post-QC dataset which has gone through LD pruning and does not include the 22 PCA outliers (whole_dataset.mt = *filtered_n_pruned_output_updated.mt* - 22 outliers). \n",
    "\n",
    "Something to note: Since the mt we are starting with is filtered and pruned, running *hl.variant_qc* and filtering to variants with call rate > 0.05 (similar to what we did for the F2 analysis) doesn't make a difference to the number of variants.\n",
    "\n",
    "\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00496151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the mt before running pc_relate but without outliers - 248634 variants and 4097 samples\n",
    "mt_FST_initial = hl.read_matrix_table(fst_input_path) \n",
    "\n",
    "# Filter to only unrelated samples - 3380 samples \n",
    "mt_unrel = hl.read_matrix_table(unrelated_path) \n",
    "unrel_samples = mt_unrel.s.collect() # collect sample IDs as a list \n",
    "unrel_samples = hl.literal(unrel_samples) # capture and broadcast the list as an expression \n",
    "mt_FST_unrel = mt_FST_initial.filter_cols(unrel_samples.contains(mt_FST_initial['s'])) # filter mt\n",
    "print('Num of samples after filtering (unrelated samples) = ' + str(mt_FST_unrel.count()[1])) # 3380 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcd857",
   "metadata": {},
   "source": [
    "### 3a. F_st with PLINK\n",
    "\n",
    "In order to calculate F_st using PLINK, we first need to export mt as PLINK files.\n",
    "\n",
    "After exporting the files to plink format, the rest of the analysis is done using shell commands within the notebook. \n",
    "\n",
    "**Place *\"!\"* before the command you want to run and proceed as if you are running codes in a terminal.** You can use *\"! ls\"* after each run to check for ouputs in the directory and see if commands have run correctly. \n",
    " \n",
    "**Every time you start a new cluster, you will need to download PLINK to run the F_st analysis since downloads and files are discarded when a cluster is stopped.** \n",
    "\n",
    "Something to note: when running the notebook on the Cloud, the shell commands still run even if we didn't use \"!\". Not sure why but will check if that is also the case when running the notebook locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export mt as PLINK2 BED, BIM and FAM files - store on the Cloud \n",
    "hl.export_plink(mt_FST_unrel, plink_files_path, fam_id=mt_FST_unrel.hgdp_tgp_meta.Population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5592a6f",
   "metadata": {},
   "source": [
    "3a.1. PLINK Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04399829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PLINK using a link from the PLINK website (linux - recent version - 64 bit - stable) \n",
    "! wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20210606.zip\n",
    "    \n",
    "# Unzip the \".gz\" file: \n",
    "! unzip plink_linux_x86_64_20210606.zip\n",
    "\n",
    "# A documentation output when you run this command indicates that PLINK has been installed properly \n",
    "! ./plink "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8555f",
   "metadata": {},
   "source": [
    "3a.2. Files Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the PLINK files that are stored on the Cloud into the current session directory\n",
    "! gsutil cp {plink_files_path}.fam . # fam \n",
    "! gsutil cp {plink_files_path}.bim . # bim\n",
    "! gsutil cp {plink_files_path}.bed . # bed\n",
    "\n",
    "# Obtain FID - in this case, it is the 78 populations in the first column of the FAM file\n",
    "! awk '{print $1}' hgdp_tgp.fam | sort -u > pop.codes\n",
    "\n",
    "# Make all possible combinations of pairs using the 78 populations \n",
    "! for i in `seq 78`; do for j in `seq 78`; do if [ $i -lt $j ]; then VAR1=`sed \"${i}q;d\" pop.codes`; VAR2=`sed \"${j}q;d\" pop.codes`; echo $VAR1 $VAR2; fi; done; done > pop.combos\n",
    "\n",
    "# Validity check \n",
    "! wc -l pop.combos # 3003\n",
    "\n",
    "# Create directories for intermediate files and F_Sst results \n",
    "! mkdir within_files # intermediate files\n",
    "! mkdir FST_results # F_st results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2445280",
   "metadata": {},
   "source": [
    "3a.3. Scripts Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009f50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 1 ####\n",
    "\n",
    "# For each population pair, set up a bash script to create a \"within\" file and run F_st \n",
    "# Files to be produced: \n",
    "#    [pop_pairs].within will be saved in the \"within_files\" directory\n",
    "#    [pop_pairs].fst, [pop_pairs].log, and [pop_pairs].nosex will be save in \"FST_results\" directory\n",
    "\n",
    "fst_script = '''    \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "\n",
    "# create \"within\" files for each population pair using the FAM file (columns 1,2 and 1 again)\n",
    "    awk -v r1=$POP1 -v r2=$POP2 '$1 == r1 || $1 == r2' hgdp_tgp.fam | awk '{ print $1, $2, $1 }' > within_files/${POP1}_${POP2}.within\n",
    "\n",
    "# run F_st\n",
    "    ./plink --bfile hgdp_tgp --fst --within within_files/${POP1}_${POP2}.within --out FST_results/${POP1}_${POP2}\n",
    "done'''\n",
    "\n",
    "with open('run_fst.py', mode='w') as file:\n",
    "    file.write(fst_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2383c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script 2 ### - script 1 has to be run for this script to run \n",
    "\n",
    "# Use the \"[pop_pairs].log\" file produced from script 1 above (located in \"FST_results\" directory) to get the \"Mean F_st estimate\" for each population pair \n",
    "# And compile all of the values in a single file for F_st heat map generation\n",
    "\n",
    "extract_mean_script = ''' \n",
    "#!/bin/bash\n",
    "\n",
    "# set variables\n",
    "for i in `seq 3003`\n",
    "do\n",
    "    POP1=`sed \"${i}q;d\" pop.combos | awk '{ print $1 }'`\n",
    "    POP2=`sed \"${i}q;d\" pop.combos | awk '{ print $2 }'`\n",
    "    mean_FST=$(tail -n4 FST_results/${POP1}_${POP2}.log | head -n 1 | awk -F: '{print $2}' | awk '{$1=$1};1')\n",
    "    printf \"%-20s\\t%-20s\\t%-20s\\n \" ${POP1} ${POP2} $mean_FST >> mean_fst_sum.txt\n",
    "done'''\n",
    "\n",
    "with open('extract_mean.py', mode='w') as file:\n",
    "    file.write(extract_mean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd0192",
   "metadata": {},
   "source": [
    "3a.4. Run Scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4550e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 1 and direct the run log into a file (~20min to run) \n",
    "! sh run_fst.py > fst_script.log\n",
    "\n",
    "# Validity check \n",
    "! cd within_files/; ls | wc -l # 3003\n",
    "! cd FST_results/; ls | wc -l # 3003 * 3 = 9009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00aa4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script 2; requires script 1 to be run first (~ 1min to run)\n",
    "! sh extract_mean.py \n",
    "\n",
    "# Copy script 2 output to the Cloud for heat map plotting in R \n",
    "! gsutil cp mean_fst_sum.txt {fst_final_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfaec93",
   "metadata": {},
   "source": [
    "PCA plots - Ally - *PENDING*\n",
    "    - already implemented in R, just need to plot it in Hail\n",
    "    \n",
    "----------------------------------------\n",
    "Further edits needed in this nb: \n",
    "- Add the path to the PCA plotting Rmarkdown (in section 5) once available  \n",
    "- Complete the table in section 5 - need Alicia's help with that \n",
    "- Add Ally's code for plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a416c24",
   "metadata": {},
   "source": [
    "# Index\n",
    "1. [Setting Default Output Paths](#1.-Set-Default-Output-Paths)\n",
    "2. [Variant Filtering and LD Pruning](#2.-Variant-Filtering-and-LD-Pruning)\n",
    "3. [Run PC Relate](#3.-Run-PC-Relate)\n",
    "4. [PCA](#4.-PCA)\n",
    "    1. [Function to Run PCA on Unrelated Individuals](#4a.-Function-to-Run-PCA-on-Unrelated-Individuals)\n",
    "    2. [Function to Project Related Individuals](#4b.-Function-to-Project-Related-Individuals)\n",
    "    3. [Global PCA](#4c.-Global-PCA)\n",
    "    4. [Subcontinental PCA](#4d.-Subcontinental-PCA)\n",
    "5. [Outlier Removal](#5.-Outlier-Removal)\n",
    "6. [Rerun PCA](#6.-Rerun-PCA)\n",
    "    1. [Global PCA](#6a.-Global-PCA)\n",
    "    2. [Subcontinental PCA](#6b.-Subcontinental-PCA)\n",
    "7. [Writing out Matrix Table](#7.-Write-Out-Matrix-Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ed110",
   "metadata": {},
   "source": [
    "# General Overview \n",
    "The purpose of this notebook is to further filter the postQC matrix table to prepare it for LD pruning, compute relatedness and run Principal Component Analysis (PCA).\n",
    "\n",
    "**This script contains information on how to:**\n",
    "- Read in the a matrix table and run Hail common variant statistics  \n",
    "- Filter using allele frequency and call rate\n",
    "- Run LD pruning \n",
    "- Run relatedness and separate related and unrelated individuals\n",
    "- Calculate PC scores and project samples on to a PC space  \n",
    "- Run global and Subcontinental PCA and plot them \n",
    "- Remove PCA outliers (filter using sample IDs)\n",
    "- rerun global and subcontinental PCA\n",
    "- Write out a matrix table \n",
    "\n",
    "Author: Mary T. Yohannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hail\n",
    "import hail as hl\n",
    "\n",
    "# import the read_qc function\n",
    "# tmp: this is commented out as the function will continue to change\n",
    "#from read_qc_function import read_qc\n",
    "\n",
    "# importing methods from gnomAD needed to project individuals\n",
    "from gnomad.sample_qc.ancestry import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea827452",
   "metadata": {},
   "source": [
    "## Set Requester Pays Bucket\n",
    "Running through these tutorials, users must specify which project is to be billed. To change which project is billed, set the `GCP_PROJECT_NAME` variable to your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3317fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting requester pays bucket to use throughout tutorial\n",
    "GCP_PROJECT_NAME = \"diverse-pop-seq-ref\" # change this to your project name\n",
    "hl.init(spark_conf={\n",
    "    'spark.hadoop.fs.gs.requester.pays.mode': 'CUSTOM',\n",
    "    'spark.hadoop.fs.gs.requester.pays.buckets': 'hgdp_tgp,gcp-public-data--gnomad',\n",
    "    'spark.hadoop.fs.gs.requester.pays.project.id': GCP_PROJECT_NAME\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4c71f",
   "metadata": {},
   "source": [
    "### tmp read_qc function\n",
    "to be removed once tutorials & function are complete and we can troubleshoot importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_qc(\n",
    "        default: bool = False,\n",
    "        post_qc:bool = False,\n",
    "        sample_qc: bool = False,\n",
    "        variant_qc: bool = False,\n",
    "        duplicate: bool = False,\n",
    "        outlier_removal: bool = False,\n",
    "        ld_pruning: bool = False,\n",
    "        rel_unrel: str = 'default') -> hl.MatrixTable:\n",
    "    \"\"\"\n",
    "    Wrapper function to get HGDP+1kGP data as Matrix Table at different stages of QC/filtering.\n",
    "    By default, returns pre QC MatrixTable with qc filters annotated but not filtered.\n",
    "\n",
    "    :param bool default: if True will preQC version of the dataset\n",
    "    :param bool post_qc: if True will return a post QC matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - outlier removal\n",
    "    :param bool sample_qc: if True will return a post sample QC matrix table\n",
    "    :param bool variant_qc: if True will return a post variant QC matrix table\n",
    "    :param bool duplicate: if True will return a matrix table with duplicate samples removed\n",
    "    :param bool outlier_removal: if True will return a matrix table with PCA outliers and duplicate samples removed\n",
    "    :param bool ld_pruning: if True will return a matrix table that has gone through:\n",
    "        - sample QC\n",
    "        - variant QC\n",
    "        - duplicate removal\n",
    "        - LD pruning\n",
    "        - additional variant filtering\n",
    "    :param bool rel_unrel: default will return same mt as ld pruned above\n",
    "        if 'all' will return the same matrix table as if ld_pruning is True\n",
    "        if 'related_pre_outlier' will return a matrix table with only related samples pre pca outlier removal\n",
    "        if 'unrelated_pre_outlier' will return a matrix table with only unrelated samples pre pca outlier removal\n",
    "        if 'related_post_outlier' will return a matrix table with only related samples post pca outlier removal\n",
    "        if 'unrelated_post_outlier' wil return a matrix table with only unrelated samples post pca outlier removal\n",
    "    \"\"\"\n",
    "    # Reading in all the tables and matrix tables needed to generate the pre_qc matrix table\n",
    "    sample_meta = hl.import_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv')\n",
    "    sample_qc_meta = hl.read_table('gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht')\n",
    "    dense_mt = hl.read_matrix_table(\n",
    "        'gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_dense.mt')\n",
    "    \n",
    "    dense_mt = dense_mt.naive_coalesce(5000)\n",
    "\n",
    "\n",
    "    # Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "    def dict_to_struct(d):\n",
    "        fields = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                v = dict_to_struct(v)\n",
    "            fields[k] = v\n",
    "        return hl.struct(**fields)\n",
    "\n",
    "    # un-flattening a hail table with nested structure\n",
    "    # dict to hold struct names as well as nested field names\n",
    "    d = {}\n",
    "\n",
    "    # Getting the row field names\n",
    "    row = sample_meta.row_value\n",
    "\n",
    "    # returns a dict with the struct names as keys and their inner field names as values\n",
    "    for name in row:\n",
    "        def recur(dict_ref, split_name):\n",
    "            if len(split_name) == 1:\n",
    "                dict_ref[split_name[0]] = row[name]\n",
    "                return\n",
    "            existing = dict_ref.get(split_name[0])\n",
    "            if existing is not None:\n",
    "                assert isinstance(existing, dict), existing\n",
    "                recur(existing, split_name[1:])\n",
    "            else:\n",
    "                existing = {}\n",
    "                dict_ref[split_name[0]] = existing\n",
    "                recur(existing, split_name[1:])\n",
    "        recur(d, name.split('.'))\n",
    "\n",
    "    # using the dict created from flattened struct, creating new structs now un-flattened\n",
    "    sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "    sample_meta = sample_meta.key_by('s')\n",
    "\n",
    "    # grabbing the columns needed from HGDP metadata\n",
    "    new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "    # creating a table with gnomAD sample metadata and HGDP metadata\n",
    "    ht = sample_qc_meta.annotate(**new_meta[sample_qc_meta.s])\n",
    "\n",
    "    # stripping 'v3.1::' from the names to match with the densified MT\n",
    "    ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))\n",
    "\n",
    "    # Using hl.annotate_cols() method to annotate the gnomAD variant QC metadata onto the matrix table\n",
    "    mt = dense_mt.annotate_cols(**ht[dense_mt.s])\n",
    "    \n",
    "\n",
    "    print(f\"sample_qc: {sample_qc}\\nvariant_qc: {variant_qc}\\nduplicate: {duplicate}\" \\\n",
    "          f\"\\noutlier_removal: { outlier_removal}\\nld_pruning: {ld_pruning}\\nrel_unrel: {rel_unrel}\")\n",
    "    \n",
    "    if default:\n",
    "        print(\"Returning default preQC matrix table\")\n",
    "        # returns preQC dataset\n",
    "        return mt\n",
    "    \n",
    "    if post_qc:\n",
    "        print(\"Returning post sample and variant QC matrix table with duplicates and PCA outliers removed\")\n",
    "        sample_qc = True\n",
    "        variant_qc = True\n",
    "        duplicate = True\n",
    "        outlier_removal = True\n",
    "    \n",
    "    if sample_qc:\n",
    "        print(\"Running sample QC\")\n",
    "        # run data through sample QC\n",
    "        # filtering samples to those who should pass gnomADs sample QC\n",
    "        # this filters to only samples that passed gnomad sample QC hard filters\n",
    "        mt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "        # annotating partially filtered dataset with variant metadata\n",
    "        mt = mt.annotate_rows(**var_meta[mt.locus, mt.alleles])\n",
    "\n",
    "    if variant_qc:\n",
    "        print(\"Running variant QC\")\n",
    "        # run data through variant QC\n",
    "        # Subsetting the variants in the dataset to only PASS variants (those which passed gnomAD's variant QC)\n",
    "        # PASS variants are variants which have an entry in the filters field.\n",
    "        # This field contains an array which contains a bool if any variant qc filter was failed\n",
    "        # This is the last step in the QC process\n",
    "        mt = mt.filter_rows(hl.len(mt.filters) != 0, keep=False)\n",
    "\n",
    "    if duplicate:\n",
    "        print(\"Removing any duplicate samples\")\n",
    "        # Removing any duplicates in the dataset using hl.distinct_by_col() which removes\n",
    "        # columns with a duplicate column key. It keeps one column for each unique key.\n",
    "        # after updating to the new dense_mt, this step is no longer necessary to run\n",
    "        mt = mt.distinct_by_col()\n",
    "\n",
    "    if outlier_removal:\n",
    "        print(\"Removing PCA outliers\")\n",
    "        # remove PCA outliers and duplicates\n",
    "        # reading in the PCA outlier list\n",
    "        # To read in the PCA outlier list, first need to read the file in as a list\n",
    "        # using hl.hadoop_open here which allows one to read in files into hail from Google cloud storage\n",
    "        pca_outlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "        with hl.utils.hadoop_open(pca_outlier_path) as file:\n",
    "            outliers = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "        # Using hl.literal here to convert the list from a python object to a hail expression so that it can be used\n",
    "        # to filter out samples\n",
    "        outliers_list = hl.literal(outliers)\n",
    "\n",
    "        # Using the list of PCA outliers, using the ~ operator which is a negation operator and obtains the compliment\n",
    "        # In this case the compliment is samples which are not contained in the pca outlier list\n",
    "        mt = mt.filter_cols(~outliers_list.contains(mt['s']))\n",
    "\n",
    "    if ld_pruning:\n",
    "        print(\"Returning ld pruned post variant and sample QC matrix table pre PCA outlier removal \")\n",
    "        # read in dataset which has additional variant filtering and ld pruning run\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt')\n",
    "\n",
    "    if rel_unrel == \"default\":\n",
    "        # do nothing\n",
    "        # created a default value because there are multiple options for rel/unrel datasets\n",
    "        mt = mt\n",
    "\n",
    "    elif rel_unrel == 'related_pre_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only related individuals\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/rel_updated.mt')\n",
    "\n",
    "        \n",
    "    elif rel_unrel == 'unrelated_pre_outlier':\n",
    "        print(\"Returning post QC matrix table with only unrelated individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only unrelated individuals\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt')\n",
    "\n",
    "\n",
    "    elif rel_unrel == 'related_post_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only related individuals\n",
    "        #   - PCA outlier removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt')\n",
    "\n",
    "\n",
    "    elif rel_unrel == 'unrelated_pst_outlier':\n",
    "        print(\"Returning post sample and variant QC matrix table \" \\\n",
    "              \"pre PCA outlier removal with only related individuals\")\n",
    "        # data has gone through:\n",
    "        #   - sample QC\n",
    "        #   - variant QC\n",
    "        #   - duplicate removal\n",
    "        #   - LD pruning\n",
    "        #   - pc_relate - filter to only unrelated individuals\n",
    "        #   - PCA outlier removal\n",
    "        mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt')\n",
    "        \n",
    "    # Calculating both variant and sample_qc metrics on the mt before returning\n",
    "    # so the stats are up to date with the version being written out\n",
    "    mt = hl.sample_qc(mt)\n",
    "    mt = hl.variant_qc(mt)\n",
    "    \n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2f6eb",
   "metadata": {},
   "source": [
    "# 1. Set Default Output Paths\n",
    "These default paths can be edited by users as needed. It is recommended to run these tutorials without writing out datasets. The read_qc() function is intended to take the place of needing to write out and read in datasets by the user. \n",
    "\n",
    "By default we have commented out all of the write steps of the tutorials, if you would like to write out your own datasets, uncomment those sections and replace the paths with your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file \n",
    "input_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/pre_running_varqc.mt'\n",
    "\n",
    "# save the filtered and LD pruned mt as an intermediate file since LD pruning takes a while to rerun\n",
    "intermediate_file_path = 'gs://hgdp-1kg/hgdp_tgp/intermediate_files/filtered_n_pruned_output_updated.mt'\n",
    "\n",
    "# paths for unrelated and related samples (prior to outlier identification and removal) \n",
    "unrel_preoutlier_path = 'gs://hgdp-1kg/hgdp_tgp/unrel_updated.mt'\n",
    "rel_preoutlier_path = 'gs://hgdp-1kg/hgdp_tgp/rel_updated.mt' \n",
    "\n",
    "# path for pre-outlier PCA results - global & subcontinental PCA \n",
    "pca_preoutlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_preoutlier/'\n",
    "\n",
    "# outliers file \n",
    "outliers_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'\n",
    "\n",
    "# path for post-outlier PCA results - global & subcontinental PCA \n",
    "pca_postoutlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_postoutlier/'\n",
    "\n",
    "# final output paths for unrelated and related samples (post-outlier)\n",
    "unrel_final_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt'\n",
    "rel_final_output = 'gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298ce96",
   "metadata": {},
   "source": [
    "# 2. Variant Filtering and LD Pruning\n",
    "<br>\n",
    "<details><summary> Click <u><span style=\"color:blue\">here</span></u> to learn why we are doing this. </summary>\n",
    "    \n",
    "> At this point, we have 155,648,020 SNPs and since we need fewer number of variants (~100-300k) for PCA, we filter on:\n",
    "> - AF - allele frequency \n",
    "> - call rate - fraction of calls neither missing nor filtered\n",
    ">\n",
    "> and then run LD pruning.     \n",
    ">    \n",
    "> Linkage disequilibrium (LD) is the correlation between nearby variants such that the alleles at neighboring polymorphisms (observed on the same chromosome) are associated within a population more often than if they were unlinked.\n",
    "<br>    \n",
    "For more information on LD pruning click <a href=\"\"> here </a>\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc\"> More on  <i> variant_qc() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/genetics.html#hail.methods.ld_prune\"> More on  <i> ld_prune() </i></a></li>\n",
    "    </ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc518019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the input file using the read_qc function\n",
    "mt_filt = read_qc(post_qc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72287ed5",
   "metadata": {},
   "source": [
    "#### 2a. Variant Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b329fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Hail's common variant statistics (QC metrics) \n",
    "mt_var = hl.variant_qc(mt_filt) \n",
    "\n",
    "# filter to variants with AF between 0.05 & 0.95, and call rate greater than 0.999    \n",
    "mt_var_filt = mt_var.filter_rows((mt_var.variant_qc.AF[0] > 0.05) & \n",
    "                                 (mt_var.variant_qc.AF[0] < 0.95) & (mt_var.variant_qc.call_rate > 0.999))\n",
    "\n",
    "# Should print 6787034 snps; this line take ~20min to run \n",
    "print('Num of variants after filtering = ' + str(mt_var_filt.count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031578b",
   "metadata": {},
   "source": [
    "#### 2b. LD Pruning\n",
    "Since the number of variants after this step is now in the ~100-300k range, we proceed to the PCA analysis without any more adjustments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa26718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated variants \n",
    "pruned = hl.ld_prune(mt_var_filt.GT, r2=0.1, bp_window_size=500000) # ~113 min to run  \n",
    "mt_var_pru_filt = mt_var_filt.filter_rows(hl.is_defined(pruned[mt_var_filt.row_key])) \n",
    "print('Num of variants after LD pruning = ' + str(mt_var_pru_filt.count()[0])) # 248634 snps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb607ef",
   "metadata": {},
   "source": [
    "#### 2c. Write out an intermediate file\n",
    "The LD pruning step takes a non negligble time to run so to ensure that the downstream analyses steps don't take a very long time we write out an intermediate file. This write out step should take around 23 minutes to run. \n",
    "\n",
    "Due to the use of the read_qc function however, you do not need to run through the write out step. Instead, the function will automatically read in the version of the dataset we wrote out when creating these tutorials. \n",
    "\n",
    "If the user wishes to export their own intermediate file, they can do so by changing the intermediate path and then replacing the read_qc() function call with `hl.read_matrix_table(intermediate_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf49568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this step will take ~23 min\n",
    "# # writing out an intermediate file to speed up subsequent analyses\n",
    "# mt_var_pru_filt.write(intermediate_file_path, overwrite=False)\n",
    "\n",
    "# read the intermediate file back in for subsequent analyses\n",
    "mt_var_pru_filt = read_qc(ld_prune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79623b3e",
   "metadata": {},
   "source": [
    "# 3. Run PC Relate   \n",
    "<br>\n",
    "<details><summary>Click <u><span style=\"color:blue\">here</span></u> to learn why we are doing this. </summary>\n",
    "<br>\n",
    "When doing Principal Component Analysis (PCA), we need to separate the related and unrelated samples before computing the PC scores and ploting them. This is because if we compute PCA with the related samples in the data set, the population structure and clustering will be affected by the relatedness that exists among those samples. Thus, we first have to identify the related individuals by computing relatedness estimates (kinship statistic in this case) using a variant of the PC-Relate method in Hail. We used a minimum minor allele frequency (MAF) filter of 0.05, excluded sample pairs with kinship less than 0.05, and used 20 principal components (PC) to control for population structure. After getting the sample ID pairs for the related samples, we then separate the filtered and pruned mt into relateds and unrelateds.\n",
    "   \n",
    "    \n",
    "For more information on relatedness click <a href=\"https://hail.is/docs/0.2/methods/relatedness.html#relatedness\"> here</a>\n",
    "    \n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<details><summary> Click <u><span style=\"color:blue\">here</span></u> to learn what metrics we used for pc_relate. \n",
    " </summary>\n",
    "    \n",
    "<br>\n",
    "We computed the kinship statistic using:\n",
    "<ul>\n",
    "<li>a minimum minor allele frequency filter of 0.05</li>\n",
    "<li>excluding sample-pairs with kinship less than 0.05</li>\n",
    "<li>20 principal components to control for population structure</li>\n",
    "</ul>\n",
    "    \n",
    "For more information on the pc_relate method click <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/\">here</a>\n",
    "    \n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "    \n",
    "<ul>\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate\"> More on  <i> pc_relate() </i></a></li>\n",
    "\n",
    "<li><a href=\"https://hail.is/docs/0.2/methods/misc.html#hail.methods.maximal_independent_set\"> More on  <i> maximal_independent_set() </i></a></li>\n",
    "</ul>\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute kinship statistic\n",
    "# takes ~4min to run\n",
    "relatedness_ht = hl.pc_relate(\n",
    "    mt_var_pru_filt.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by() \n",
    "\n",
    "# identify closely related individuals in pairs (list of sample IDs) \n",
    "# takes ~2hr & 22min to run\n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False) \n",
    "\n",
    "# subset the filtered and pruned mt to samples that are NOT present in the list of related individuals  \n",
    "mt_unrel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=False) \n",
    "\n",
    "# do the same as above but this time subset to samples that are present in the related-individuals list   \n",
    "mt_rel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write out the unrelated and related mts since they are used beyond this notebook in other analyses     \n",
    "# # unrelated mt\n",
    "# mt_unrel.write(unrel_path, overwrite=False) \n",
    "\n",
    "# # related mt \n",
    "# mt_rel.write(rel_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the related and unrelated mts back in using read_qc\n",
    "# unrelated mt\n",
    "mt_unrel = read_qc(rel_unrel='unrelated_pre_outlier') \n",
    "\n",
    "# related mt \n",
    "mt_rel = read_qc(rel_unrel='related_pre_outlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd32ee8",
   "metadata": {},
   "source": [
    "# 4. PCA\n",
    "<br>\n",
    "<details><summary>Click <u><span style=\"color:blue\">here</span></u> to learn why we are doing this. </summary>\n",
    "<br>\n",
    "PCA is ran on the unrelated samples first. Then, the related samples are projected onto the PC space of the unrelated samples to get their PC scores. This way the population structure and clustering is not affected by the relatedness among samples.  \n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08301c08",
   "metadata": {},
   "source": [
    "### 4a. Function to Run PCA on Unrelated Individuals\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_path: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a data set\n",
    "    :param mt: data set to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_path: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_path + reg_name + '_scores.txt.bgz')  # save individual-level-genetic-region PCs\n",
    "    pca_loadings.write(out_path + reg_name + '_loadings.ht', overwrite)  # save PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da90d17",
   "metadata": {},
   "source": [
    "### 4b. Function to Project Related Individuals\n",
    "<br>\n",
    "<details><summary> For troubleshooting information click <u><span style=\"color:blue\">here</span></u>. </summary>\n",
    "\n",
    "> If this function is not working, make sure you used the  <code>--packages gnomad</code> argument when starting your cluster\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc1497",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gnomad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#if running on GCS, need to add \"--packages gnomad\" when starting a cluster in order for the import to work  \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgnomad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample_qc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mancestry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_individuals\u001b[39m(pca_loadings, project_mt, reg_name:\u001b[38;5;28mstr\u001b[39m, out_prefix: \u001b[38;5;28mstr\u001b[39m, overwrite: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Project samples into predefined PCA space\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    :param pca_loadings: existing PCA space - unrelated samples \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gnomad'"
     ]
    }
   ],
   "source": [
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_path: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space of unrelated samples \n",
    "    :param project_mt: matrix table of related samples to project  \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_path: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_path + reg_name + '_projected_scores.txt.bgz') # save output   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ade90",
   "metadata": {},
   "source": [
    "### 4c. Global PCA\n",
    "\n",
    "<br>\n",
    "<details><summary> Click <u><span style=\"color:blue\">here</span></u> to learn why are we doing this.</summary>\n",
    "<br>\n",
    "    \n",
    "> To see the population structure and clustering on a continental level and contextualize the data globally.    \n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the unrelated samples\n",
    "run_pca(mt_unrel, 'global', pca_preoutlier_path, False)  \n",
    "\n",
    "# read in the PCA loadings of the unrelated samples\n",
    "loadings = hl.read_table(pca_preoutlier_path+'global_loadings.ht') \n",
    "\n",
    "# project the related samples onto the unrelated-samples' PC space \n",
    "project_individuals(loadings, mt_rel, 'global', pca_preoutlier_path, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f86f5",
   "metadata": {},
   "source": [
    "### 4d. Subcontinental PCA \n",
    "<br>\n",
    "\n",
    "When running the following section, the notebook might freeze after printing the log for EUR, AFR and AMR. If this happens, do not restart it. Let it run and follow the progress with the outputs being generated.  \n",
    "\n",
    "When complete, check for the following in your specified output path:\n",
    "- 21 total output files (3 for each region)\n",
    "\n",
    "Once you have confirmed you have the desired output do the following:\n",
    "1. Save close and halt the current notebook\n",
    "2. Open a new session\n",
    "3. Proceed to the next step (run project_relateds function)\n",
    "\n",
    "\n",
    "<br>\n",
    "<details><summary> Click <u><span style=\"color:blue\">here</span></u> to learn why are we doing this. </summary>\n",
    "<br>\n",
    "    \n",
    "> To see the population structure and clustering on a subcontinental level and contextualize data within continental regions. This also helped us identify outliers which were removed later on.     \n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<details><summary> For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "\n",
    "<ul>\n",
    "<li><a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca\"> More on <i> hwe_normalized_pca() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.annotate_rows\"> More on <i> annotate_rows() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.annotate\"> More on <i> annotate() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.transmute\"> More on <i> transmute() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.export\"> More on <i> export() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/experimental/index.html#hail.experimental.pc_project\"> More on <i> pc_project() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.collect\"> More on <i> collect() </i></a></li>\n",
    "</ul>\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the continental regions in the data set (used the unrelated data set since it had more samples) \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # convert into a list\n",
    "# There are 7 regions: EUR, AFR, AMR, EAS, CSA, OCE, and MID\n",
    "\n",
    "# set argument values for PCA \n",
    "subcont_pca_prefix = pca_preoutlier_path+'subcont_pca/' # path for outputs \n",
    "overwrite = False\n",
    "\n",
    "# for each region, run PCA on the unrelated samples (~27min to run)\n",
    "for i in regions:  \n",
    "    # filter the unrelateds per region\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  \n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eae548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each region, project the related samples onto the unrelated-samples' PC space (~2min to run)\n",
    "for i in regions:\n",
    "    # read in the PCA loadings of the unrelated samples for each region \n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') \n",
    "    \n",
    "    # filter the related mt per region \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  \n",
    "    \n",
    "    # project \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517116b",
   "metadata": {},
   "source": [
    "# 5. Outlier Removal\n",
    "\n",
    "After plotting the PCs using R (link_the_plotting_Rmarkdown_here), 22 outliers were identified (complete_the_table)\n",
    "\n",
    "| s | Genetic region | Population | Note |\n",
    "| --- | --- | --- | -- |\n",
    "| HG01880 | AFR | ACB | - |\n",
    "| HG01881 | AFR | ACB | - |\n",
    "| NA20274 | AFR | ASW | - |\n",
    "| NA20299 | AFR | ASW | - |\n",
    "| NA20314 | AFR | ASW | Clusters with AMR in global PCA | \n",
    "| HGDP00013 | CSA | Brahui | - |\n",
    "| HGDP00029 | CSA | Brahui | - |\n",
    "| HGDP00057 | CSA | Balochi | - | \n",
    "| HGDP00130 | CSA | Makrani | Closer to AFR than most CSA |\n",
    "| HGDP00150 | CSA | Makrani | - |\n",
    "| HGDP01298 | EAS | Uygur | - |\n",
    "| HGDP01303 | EAS | Uygur | - |\n",
    "| HGDP01300 | EAS | Uygur | - |\n",
    "| LP6005443-DNA_B02 | EAS | Uygur | - |\n",
    "| HG01628 | EUR | IBS | - |\n",
    "| HG01629 | EUR | IBS | - |\n",
    "| HG01630 | EUR | IBS | - |\n",
    "| HG01694 | EUR | IBS | - |\n",
    "| HG01696 | EUR | IBS | - |\n",
    "| HGDP00621 | MID | Bedouin | Closer to AFR than most MID |\n",
    "| HGDP01270 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP01271 | MID | Mozabite | Closer to AFR than most MID |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the unrelated and related mts to remove outliers and rerun pca  \n",
    "mt_unrel_unfiltered = read_qc(rel_unrel='unrelated_pre_outlier') # unrelated mt\n",
    "mt_rel_unfiltered = read_qc(rel_unrel='related_pre_outlier') # related mt\n",
    "\n",
    "# read the outliers file into a list\n",
    "with hl.utils.hadoop_open(outliers_path) as file: \n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "# capture and broadcast the list as an expression\n",
    "outliers_list = hl.literal(outliers)\n",
    "\n",
    "# remove the 22 outliers from both mts\n",
    "mt_unrel = mt_unrel_unfiltered.filter_cols(~outliers_list.contains(mt_unrel_unfiltered['s']))\n",
    "mt_rel = mt_rel_unfiltered.filter_cols(~outliers_list.contains(mt_rel_unfiltered['s']))\n",
    "\n",
    "# sanity check \n",
    "print('Unrelated: Before outlier removal ' + \n",
    "      str(mt_unrel_unfiltered.count()[1]) + ' | After outlier removal ' + \n",
    "      str(mt_unrel.count()[1]))\n",
    "\n",
    "print('Related: Before outlier removal: ' + \n",
    "      str(mt_rel_unfiltered.count()[1]) + ' | After outlier removal ' + \n",
    "      str(mt_rel.count()[1])) num_outliers = (mt_unrel_unfiltered.count()[1] - \n",
    "                                              mt_unrel.count()[1]) + (mt_rel_unfiltered.count()[1] - \n",
    "                                                                      mt_rel.count()[1])\n",
    "print('Total samples removed = ' + str(num_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a3b1a",
   "metadata": {},
   "source": [
    "# 6. Rerun PCA\n",
    "\n",
    "**Before running the sections below make sure you have run sections 4a (PCA) and 4b (projection) above.**\n",
    "\n",
    "<br>\n",
    "<details><summary> To learn what is different from the prior PCA run click <u><span style=\"color:blue\">here</span></u>.</summary>\n",
    "<ul>\n",
    "<li>updated unrelated and related mts (outliers removed)</li>\n",
    "<li>new paths for the outputs</li>  \n",
    "    </ul>\n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a09ac3",
   "metadata": {},
   "source": [
    "### 6a. Global PCA (without outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b29d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the unrelated samples  \n",
    "run_pca(mt_unrel, 'global', pca_postoutlier_path, False)\n",
    "\n",
    "# read in the PCA loadings of the unrelated samples  \n",
    "loadings = hl.read_table(pca_postoutlier_path+'global_loadings.ht') \n",
    "\n",
    "# project the related samples onto the unrelated-samples' PC space \n",
    "project_individuals(loadings, mt_rel, 'global', pca_postoutlier_path, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc260642",
   "metadata": {},
   "source": [
    "### 6b. Subcontinental PCA (without outliers)\n",
    "\n",
    "> When running the following section, the notebook might freeze after printing the log for EUR, AFR and AMR. If this happens, do not restart it. Let it run and follow the progress with the outputs being generated.  \n",
    ">\n",
    "> When complete, check for the following in your specified output path:\n",
    "> - 21 total output files (3 for each region)\n",
    ">\n",
    "> Once you have confirmed you have the desired output do the following:\n",
    "> 1. Save close and halt the current notebook\n",
    "> 2. Open a new session\n",
    "> 3. Proceed to the next step (run project_relateds function)\n",
    ">\n",
    "<br>\n",
    "<details><summary>For more information on Hail methods and expressions click <u><span style=\"color:blue\">here</span></u>.</summary> \n",
    "\n",
    "<ul>\n",
    "<li><a href=\"more info https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca\"> More on <i> hwe_normalized_pca() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.annotate_rows\"> More on <i> annotate_rows() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.annotate\"> More on <i> annotate() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.transmute\"> More on <i> transmute() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.Table.html#hail.Table.export\"> More on <i> export() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/experimental/index.html#hail.experimental.pc_project\"> More on <i> pc_project() </i></a></li>\n",
    "    \n",
    "<li><a href=\"more info https://hail.is/docs/0.2/hail.expr.Expression.html#hail.expr.Expression.collect\"> More on <i> collect() </i></a></li>\n",
    "    </ul>\n",
    "    \n",
    "</details>\n",
    "\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values for PCA \n",
    "subcont_pca_prefix = pca_postoutlier_path+'subcont_pca/' # path for outputs \n",
    "overwrite = False \n",
    "\n",
    "\n",
    "# for each region, run PCA on the unrelated samples (~26 min to run) \n",
    "# \"regions\" is a list containing the 7 continental regions in the data set from section 4d\n",
    "for i in regions: \n",
    "    # filter the unrelateds per region\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  \n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each region, project the related samples onto the unrelated-samples' PC space (~3min to run)\n",
    "for i in regions:\n",
    "    # read in the PCA loadings of the unrelated samples for each region\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') \n",
    "    \n",
    "    # filter the relateds per region \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  \n",
    "    \n",
    "    # project \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b2bb2",
   "metadata": {},
   "source": [
    "# 7. Write Out Matrix Table \n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89958a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write out mts of unrelated and related samples separately (post-outlier removal) \n",
    "# #unrelated mt\n",
    "# mt_unrel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/unrelated.mt',\n",
    "#                overwrite=False)\n",
    "# #related mt\n",
    "# mt_rel.write('gs://hgdp-1kg/hgdp_tgp/datasets_for_others/lindo/ds_without_outliers/related.mt',\n",
    "#              overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

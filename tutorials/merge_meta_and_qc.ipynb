{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "great-jenny",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Functions](#Functions)\n",
    "- [Reading in datasets](#Reading-in-datasets)\n",
    "- [Combining metadata](#Combining-metadata)\n",
    "- [Annotating metadata onto matrix table](#Annotating-merged-metadata-onto-matrix-table)\n",
    "- [Sample QC filtering](#Sample-QC-filtering)\n",
    "- [PCA outlier removal](#PCA-outlier-removal)\n",
    "- [Variant QC filtering](#Variant-QC-filtering)\n",
    "- [Exporting datasets post QC](#Exporting-final-dataset-post-QC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-football",
   "metadata": {},
   "source": [
    "### The purpose of this script is to merge metadata components needed for the HGDP+1kGP dataset and then run  QC filters on that resulting dataset. The QC filters were run using sample/variant information from the metadata datasets. \n",
    "\n",
    "**This script contains information on how to**: \n",
    "- annotate metadata onto a matrix table\n",
    "- combine multiple tables and matrix tables\n",
    "- harmonize datasets to prevent merge conflicts\n",
    "- filter matrix tables using a field within the matrix table\n",
    "- filter samples using a hardcoded list of samples to remove\n",
    "- write out a matrix table in vcf format\n",
    "\n",
    "**Datasets merged are**: \n",
    "    - gnomad sample metadata (sample_meta)\n",
    "    - gnomad v3.1 sample qc metadata from Julia for the hgdp_1kg subset (jul_meta)\n",
    "    - gnomad v3.1 variant qc metadata information (var_meta)\n",
    "    - densified hgdp_1kg matrix table (dense_mt)   \n",
    "    \n",
    "**Author: Zan Koenig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apart-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.5\n",
      "SparkUI available at http://znk-plink-m.c.diverse-pop-seq-ref.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.64-1ef70187dc78\n",
      "LOGGING: writing to /home/hail/hail-20211202-1706-0.2.64-1ef70187dc78.log\n"
     ]
    }
   ],
   "source": [
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-dispatch",
   "metadata": {},
   "source": [
    "# Functions\n",
    "For interactive analyses scripts such as this, defining functions at the top of the script allow for ease of use. There are other ways to organize the definition of functions in python and the best method depends on the indended usage of the script as well as the writers personal preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "def dict_to_struct(d):\n",
    "    fields = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = dict_to_struct(v)\n",
    "        fields[k] = v\n",
    "    return hl.struct(**fields)\n",
    "\n",
    "# Formats the output of using hl.count in a more user friendly format\n",
    "def print_count(mt):\n",
    "    '''\n",
    "    Prints out total sample/variant count for an mt\n",
    "    :param mt: hail matrix table\n",
    "    :return: print statement with number of samples and variants\n",
    "    '''\n",
    "    n = mt.count()\n",
    "    print('Number of Samples: {}\\nNumber of Variants: {}'.format(n[1], n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-plane",
   "metadata": {},
   "source": [
    "# Reading in datasets\n",
    "Setting separate variables for paths before the dataset are read in makes it easier to update paths if datasets move in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "skilled-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for Alicia's sample metadata file\n",
    "sample_metadata_path = 'gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv'\n",
    "\n",
    "# path for Julia's sample metadata file\n",
    "jul_metadata_path = ('gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht')\n",
    "\n",
    "# path for variant qc info\n",
    "var_metadata_path = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'\n",
    "\n",
    "# path for Konrad's densified matrix table\n",
    "dense_mt_path = 'gs://hgdp_tgp/output/tgp_hgdp.mt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:02:28 Hail: INFO: Reading table to impute column types\n",
      "2021-11-02 17:02:35 Hail: INFO: Loading 184 fields. Counts by type:\n",
      "  str: 80\n",
      "  bool: 44\n",
      "  float64: 40\n",
      "  int32: 20\n"
     ]
    }
   ],
   "source": [
    "# reading in Alicia's sample metadata file (Note: this file uses the 'v3.1::' prefix as done in gnomAD)\n",
    "sample_meta = hl.import_table(sample_metadata_path, impute=True)\n",
    "\n",
    "# reading in Julia's sample metadata file\n",
    "jul_meta = hl.read_table(jul_metadata_path)\n",
    "\n",
    "# reading in variant qc information\n",
    "var_meta = hl.read_table(var_metadata_path)\n",
    "\n",
    "# reading in densified matrix table\n",
    "dense_mt = hl.read_matrix_table(dense_mt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-output",
   "metadata": {},
   "source": [
    "# Combining metadata\n",
    "Before conducting QC, the different metadata datasets must be merged together. The first cell below is an example of having to alter the structure of a dataset before being able to merge with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bits below were written by Tim Poterba to help troubleshoot unflattening a ht with nested structure\n",
    "# dict to hold struct names as well as nested field names\n",
    "d = {}\n",
    "\n",
    "# Getting just the row field names \n",
    "row = sample_meta.row_value\n",
    "\n",
    "# returns a dict with the struct names as keys and their inner field names as values\n",
    "for name in row:\n",
    "    def recur(dict_ref, split_name):\n",
    "        if (len(split_name) == 1):\n",
    "            dict_ref[split_name[0]] = row[name]\n",
    "            return\n",
    "        existing = dict_ref.get(split_name[0])\n",
    "        if existing is not None:\n",
    "            assert isinstance(existing, dict), existing  # fails on foo.bar and foo.bar.baz\n",
    "            recur(existing, split_name[1:])\n",
    "        else:\n",
    "            existing = {}\n",
    "            dict_ref[split_name[0]] = existing\n",
    "            recur(existing, split_name[1:])\n",
    "    recur(d, name.split('.'))\n",
    "\n",
    "\n",
    "# using the dict created from flattened struct, creating new structs now unflattened\n",
    "sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "sample_meta = sample_meta.key_by('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "formed-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the columns needed from Alicia's metadata\n",
    "new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "# creating a table with Julia's metadata and Alicia's metadata\n",
    "ht = jul_meta.annotate(**new_meta[jul_meta.s])\n",
    "\n",
    "# stripping 'v3.1::' from the names to match with Konrad's MT\n",
    "ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 18:31:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:31:55 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:32:25 Hail: INFO: wrote table with 4150 rows in 155 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_sample_metadata.ht\n",
      "    Total size: 1.69 MiB\n",
      "    * Rows: 1.68 MiB\n",
      "    * Globals: 6.51 KiB\n",
      "    * Smallest partition: 1 rows (758.00 B)\n",
      "    * Largest partition:  173 rows (68.18 KiB)\n"
     ]
    }
   ],
   "source": [
    "# When writing out any dataset, you want to make sure the path is as intended and the resulting name is descriptive\n",
    "# hl.write() takes the entire output path as an argument as well as the name of the resulting table or matrix table\n",
    "ht.write('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-division",
   "metadata": {},
   "source": [
    "# Annotating merged metadata onto matrix table\n",
    "Now that the two metadata datasets are merged together and in the proper format, the next step is to annotate the dense matrix table with all of the samples and variants preQC with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in table annotated with Alicia and Julia's respective metadata\n",
    "ht = hl.read_table('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "center-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hl.count() returns the counts of samples and variants within a matrix table or table.\n",
    "# In this case since it is a hail table, it only returns the count of the number of samples\n",
    "# The number of samples is equal to the number of rows\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "million-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'sex_imputation_ploidy_cutoffs': struct {\n",
      "        x_ploidy_cutoffs: struct {\n",
      "            upper_cutoff_X: float64, \n",
      "            lower_cutoff_XX: float64, \n",
      "            upper_cutoff_XX: float64, \n",
      "            lower_cutoff_XXX: float64\n",
      "        }, \n",
      "        y_ploidy_cutoffs: struct {\n",
      "            lower_cutoff_Y: float64, \n",
      "            upper_cutoff_Y: float64, \n",
      "            lower_cutoff_YY: float64\n",
      "        }, \n",
      "        f_stat_cutoff: float64\n",
      "    } \n",
      "    'population_inference_pca_metrics': struct {\n",
      "        min_prob: float64, \n",
      "        include_unreleasable_samples: bool, \n",
      "        max_mislabeled_training_samples: int32, \n",
      "        known_pop_removal_iterations: int32, \n",
      "        n_pcs: int32\n",
      "    } \n",
      "    'relatedness_inference_cutoffs': struct {\n",
      "        min_individual_maf: float64, \n",
      "        min_emission_kinship: float64, \n",
      "        ibd0_0_max: float64, \n",
      "        second_degree_kin_cutoff: float64, \n",
      "        first_degree_kin_thresholds: tuple (\n",
      "            float64, \n",
      "            float64\n",
      "        )\n",
      "    } \n",
      "    'outlier_detection_metrics': struct {\n",
      "        lms: struct {\n",
      "            n_snp: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_singleton: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_ti_tv: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_insertion_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_insertion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_het_hom_var: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transition: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transversion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }\n",
      "        }, \n",
      "        qc_metrics_stats: struct {\n",
      "            n_snp_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_singleton_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_ti_tv_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_insertion_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_insertion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_het_hom_var_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transition_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transversion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }\n",
      "        }, \n",
      "        n_pcs: int32, \n",
      "        used_regressed_metrics: bool\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    'project_meta': struct {\n",
      "        sample_id: str, \n",
      "        research_project_key: str, \n",
      "        seq_project: str, \n",
      "        ccdg_alternate_sample_id: str, \n",
      "        ccdg_gender: str, \n",
      "        ccdg_center: str, \n",
      "        ccdg_study: str, \n",
      "        cram_path: str, \n",
      "        project_id: str, \n",
      "        v2_age: float64, \n",
      "        v2_sex: str, \n",
      "        v2_hard_filters: str, \n",
      "        v2_perm_filters: str, \n",
      "        v2_pop_platform_filters: str, \n",
      "        v2_related: bool, \n",
      "        v2_data_type: str, \n",
      "        v2_product: str, \n",
      "        v2_product_simplified: str, \n",
      "        v2_qc_platform: str, \n",
      "        v2_project_id: str, \n",
      "        v2_project_description: str, \n",
      "        v2_internal: bool, \n",
      "        v2_investigator: str, \n",
      "        v2_known_pop: str, \n",
      "        v2_known_subpop: str, \n",
      "        v2_pop: str, \n",
      "        v2_subpop: str, \n",
      "        v2_neuro: bool, \n",
      "        v2_control: bool, \n",
      "        v2_topmed: bool, \n",
      "        v2_high_quality: bool, \n",
      "        v2_release: bool, \n",
      "        v2_pcr_free: bool, \n",
      "        v2_project_name: str, \n",
      "        v2_release_2_0_2: bool, \n",
      "        project_ancestry: str, \n",
      "        project_pop: str, \n",
      "        project_subpop: str, \n",
      "        pdo_owner: str, \n",
      "        category: str, \n",
      "        contact_pi: str, \n",
      "        sample_pi: str, \n",
      "        pm: str, \n",
      "        research_project: str, \n",
      "        pdo: str, \n",
      "        title: str, \n",
      "        product: str, \n",
      "        probably_releasable: str, \n",
      "        releasable: bool, \n",
      "        broad_external: str, \n",
      "        sex: str, \n",
      "        subpop_description: str, \n",
      "        exclude: bool, \n",
      "        exclude_reason: str, \n",
      "        case_control: str, \n",
      "        age: int32, \n",
      "        age_bin: str, \n",
      "        tcga_tumor: bool, \n",
      "        age_alt: int32, \n",
      "        v2_s_match: str, \n",
      "        topmed: bool, \n",
      "        neuro_cohort: bool, \n",
      "        neuro_case: bool\n",
      "    } \n",
      "    'subsets': struct {\n",
      "        non_topmed: bool, \n",
      "        controls_and_biobanks: bool, \n",
      "        non_neuro: bool, \n",
      "        non_v2: bool, \n",
      "        non_cancer: bool, \n",
      "        tgp: bool, \n",
      "        hgdp: bool\n",
      "    } \n",
      "    'bam_metrics': struct {\n",
      "        pct_bases_20x: float64, \n",
      "        pct_chimeras: float64, \n",
      "        freemix: float64, \n",
      "        mean_coverage: float64, \n",
      "        median_coverage: float64, \n",
      "        mean_insert_size: float64, \n",
      "        median_insert_size: float64, \n",
      "        pct_bases_10x: float64\n",
      "    } \n",
      "    'sex_imputation': struct {\n",
      "        is_female: bool, \n",
      "        chr20_mean_dp: float32, \n",
      "        chrX_mean_dp: float32, \n",
      "        chrY_mean_dp: float32, \n",
      "        chrX_ploidy: float32, \n",
      "        chrY_ploidy: float32, \n",
      "        X_karyotype: str, \n",
      "        Y_karyotype: str, \n",
      "        sex_karyotype: str, \n",
      "        impute_sex_stats: struct {\n",
      "            f_stat: float64, \n",
      "            n_called: int64, \n",
      "            expected_homs: float64, \n",
      "            observed_homs: int64\n",
      "        }\n",
      "    } \n",
      "    'sample_qc': struct {\n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64, \n",
      "        n_snp_residual: float64, \n",
      "        n_singleton_residual: float64, \n",
      "        r_ti_tv_residual: float64, \n",
      "        r_insertion_deletion_residual: float64, \n",
      "        n_insertion_residual: float64, \n",
      "        n_deletion_residual: float64, \n",
      "        r_het_hom_var_residual: float64, \n",
      "        n_transition_residual: float64, \n",
      "        n_transversion_residual: float64\n",
      "    } \n",
      "    'population_inference': struct {\n",
      "        training_pop: str, \n",
      "        pca_scores: array<float64>, \n",
      "        pop: str, \n",
      "        prob_afr: float64, \n",
      "        prob_ami: float64, \n",
      "        prob_amr: float64, \n",
      "        prob_asj: float64, \n",
      "        prob_eas: float64, \n",
      "        prob_fin: float64, \n",
      "        prob_mid: float64, \n",
      "        prob_nfe: float64, \n",
      "        prob_oth: float64, \n",
      "        prob_sas: float64, \n",
      "        training_pop_all: str\n",
      "    } \n",
      "    'sample_filters': struct {\n",
      "        sex_aneuploidy: bool, \n",
      "        insert_size: bool, \n",
      "        chimera: bool, \n",
      "        contamination: bool, \n",
      "        bad_qc_metrics: bool, \n",
      "        low_coverage: bool, \n",
      "        ambiguous_sex: bool, \n",
      "        failed_fingerprinting: bool, \n",
      "        TCGA_tumor_sample: bool, \n",
      "        hard_filters: set<str>, \n",
      "        hard_filtered: bool, \n",
      "        release_related: bool, \n",
      "        release_duplicate: bool, \n",
      "        release_parent_child: bool, \n",
      "        release_sibling: bool, \n",
      "        all_samples_related: bool, \n",
      "        all_samples_duplicate: bool, \n",
      "        all_samples_parent_child: bool, \n",
      "        all_samples_sibling: bool, \n",
      "        fail_n_snp_residual: bool, \n",
      "        fail_n_singleton_residual: bool, \n",
      "        fail_r_ti_tv_residual: bool, \n",
      "        fail_r_insertion_deletion_residual: bool, \n",
      "        fail_n_insertion_residual: bool, \n",
      "        fail_n_deletion_residual: bool, \n",
      "        fail_r_het_hom_var_residual: bool, \n",
      "        fail_n_transition_residual: bool, \n",
      "        fail_n_transversion_residual: bool, \n",
      "        qc_metrics_filters: set<str>\n",
      "    } \n",
      "    'relatedness_inference': struct {\n",
      "        relationships: set<str>\n",
      "    } \n",
      "    'high_quality': bool \n",
      "    'release': bool \n",
      "    'hgdp_tgp_meta': struct {\n",
      "        Project: str, \n",
      "        Study: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Population: str, \n",
      "        Genetic: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Latitude: float64, \n",
      "        Longitude: float64, \n",
      "        Continent: struct {\n",
      "            colors: str\n",
      "        }, \n",
      "        n: int32, \n",
      "        rownum: int32, \n",
      "        Pop: struct {\n",
      "            colors: str, \n",
      "            shapes: int32\n",
      "        }\n",
      "    } \n",
      "    'bergstrom': struct {\n",
      "        hgdp: str, \n",
      "        lp: str, \n",
      "        source: str, \n",
      "        library_type: str, \n",
      "        region: str, \n",
      "        sex: str, \n",
      "        coverage: float64, \n",
      "        freemix: float64, \n",
      "        capmq: int32, \n",
      "        insert_size_average: float64, \n",
      "        array_non_reference_discordance: float64, \n",
      "        sample: str\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['s']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# hl.describe() gives you an overview of all of the fields in a matrix table or table\n",
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36669a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hl.annotate_cols() method to annotate the metadata onto the matrix table\n",
    "# Using hl.annotate_cols() in this way is essentially merging dense_mt with ht\n",
    "# In order for this hl.annotate_cols() to work, both of the datasets to merge need to share the same key\n",
    "# In this case that key is 's'\n",
    "# When using hl.annotate_cols() the table is being indexed by the equivalent key in  the dense_mt\n",
    "mt = dense_mt.annotate_cols(**ht[dense_mt.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "british-visibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4151)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since hl.count() here is being used on a matrix table, the result has two numbers in output\n",
    "# When using hl.count() on a matrix table the first number is the number of rows, equivalent to the number of variants\n",
    "# The second number is the number of columns, equivalent to the number of samples\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "capital-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-08 15:38:02 Hail: INFO: wrote matrix table with 211358784 rows and 4151 columns in 5000 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_dense_meta_preQC.mt\n",
      "    Total size: 3.32 TiB\n",
      "    * Rows/entries: 3.32 TiB\n",
      "    * Columns: 1.71 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (32.13 MiB)\n",
      "    * Largest partition:  183321 rows (4.39 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out a pre-qc version of the dataset for Mary's PCA analyses\n",
    "mt.write(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_preQC.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-beatles",
   "metadata": {},
   "source": [
    "# Sample QC filtering\n",
    "As previously mentioned, sample QC filtering for this dataset was conducted using metadata which was annotated onto the main matrix table. Sample QC was run using gnomAD's QC pipeline and the fields used to filter below contain information on whether samples passed or failed gnomAD QC. \n",
    "\n",
    "For more information on how sample QC filters were developed see [INSERT ADDITIONAL TUTORIAL NAME HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rational-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the preQC dataset \n",
    "# This is a merged version of the metadata from different sources and the sample/variant dense dataset\n",
    "mt = hl.read_matrix_table(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_preQC.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8769dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a preliminary count before filtering on the dataset\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering samples to those who should pass QC\n",
    "# this filters to only samples that passed gnomad sample QC hard filters\n",
    "mt_filt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "# annotating partially filtered dataset with variant metadata\n",
    "mt_filt = mt_filt.annotate_rows(**var_meta[mt_filt.locus, mt_filt.alleles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the counts of samples/filters after filtering to those who passed sample QC\n",
    "mt_filt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlikely-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 19:18:12 Hail: INFO: wrote matrix table with 211358784 rows and 4120 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt\n",
      "    Total size: 3.82 TiB\n",
      "    * Rows/entries: 3.82 TiB\n",
      "    * Columns: 1.70 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (38.69 MiB)\n",
      "    * Largest partition:  183321 rows (4.82 GiB)\n"
     ]
    }
   ],
   "source": [
    "mt_filt.write('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-glance",
   "metadata": {},
   "source": [
    "# PCA outlier removal\n",
    "For information on how PCA outliers were found see [INSERT TUTORIAL NAME HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "persistent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the annotated & partially filtered dataset\n",
    "mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecological-bundle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sample and variant count before removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "minimal-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a set of outliers found by Mary during PCA analyses as well as one duplicate sample\n",
    "outlier_set = {\"NA20314\",\"NA20299\",\"HG01880\",\"HG01881\",\"HGDP00130\",\"HGDP00013\",\n",
    "               \"HGDP00150\",\"HGDP00029\",\"HGDP01298\",\"HGDP01303\",\"LP6005443-DNA_B02\",\n",
    "               \"HGDP01300\",\"HG01628\",\"HG01629\",\"HG01630\",\"HG01694\",\"HG01696\",\n",
    "               \"HGDP00621\",\"HGDP01270\",\"HGDP01271\",\"NA20274\",\"HGDP00057\"}\n",
    "# Creating a set of the samples to remove \n",
    "# Using hl.filter_cols() to remove the samples in the outlier set\n",
    "set_to_remove = hl.literal(outlier_set)\n",
    "mt = mt.filter_cols(~set_to_remove.contains(mt['s']))\n",
    "mt = mt.distinct_by_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "technological-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4097)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a count of samples/variants after removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-funeral",
   "metadata": {},
   "source": [
    "# Variant QC filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "disabled-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the variants in the dataset to only PASS variants (those which passed variant QC)\n",
    "# PASS variants are variants which have an entry in the filters field. This field contains an array which contains a bool if any variant qc filter was failed\n",
    "# This is the last step in the QC process\n",
    "mt = mt.filter_rows(hl.len(mt.filters) !=0  ,keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "honest-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155648020, 4097)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the final count of the dataset before writing out the dataset to different formats\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-progress",
   "metadata": {},
   "source": [
    "# Exporting final dataset post QC\n",
    "### Before exporting, the dataset fields must be formatted so it can be written out as a vcf\n",
    "This is another example of how datasets need to be altered in order to be written out in a specific format. In this case, the types of certain fields within the matrix table need to be changed before it can be written out into vcf format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing types to float64 so they can be written out to vcf\n",
    "mt_vcf = mt.annotate_rows(info = mt.info.annotate(\n",
    "    QUALapprox=hl.float64(mt.info.QUALapprox),\n",
    "    SB= mt.info.SB.map(lambda x: hl.float64(x)),\n",
    "    MQ=hl.float64(mt.info.MQ),\n",
    "    MQRankSum=hl.float64(mt.info.MQRankSum),\n",
    "    VarDP=hl.float64(mt.info.VarDP),\n",
    "    AS_ReadPosRankSum=hl.float64(mt.info.AS_ReadPosRankSum), \n",
    "    AS_pab_max=hl.float64(mt.info.AS_pab_max), \n",
    "    AS_QD=hl.float64(mt.info.AS_QD), \n",
    "    AS_MQ=hl.float64(mt.info.AS_MQ), \n",
    "    QD=hl.float64(mt.info.QD), \n",
    "    AS_MQRankSum=hl.float64(mt.info.AS_MQRankSum), \n",
    "    FS=hl.float64(mt.info.FS), \n",
    "    AS_FS=hl.float64(mt.info.AS_FS), \n",
    "    ReadPosRankSum=hl.float64(mt.info.ReadPosRankSum), \n",
    "    AS_QUALapprox=hl.float64(mt.info.AS_QUALapprox), \n",
    "    AS_SB_TABLE=mt.info.AS_SB_TABLE.map(lambda x: hl.float64(x)), \n",
    "    AS_VarDP=hl.float64(mt.info.AS_VarDP), \n",
    "    AS_SOR=hl.float64(mt.info.AS_SOR), \n",
    "    SOR=hl.float64(mt.info.SOR), \n",
    "    singleton=hl.bool(mt.info.singleton), \n",
    "    transmitted_singleton=hl.bool(mt.info.transmitted_singleton), \n",
    "    omni=hl.bool(mt.info.omni), \n",
    "    mills=hl.bool(mt.info.omni), \n",
    "    monoallelic=hl.bool(mt.info.monoallelic), \n",
    "    AS_VQSLOD=hl.float64(mt.info.AS_VQSLOD), \n",
    "    InbreedingCoeff=hl.float64(mt.info.InbreedingCoeff)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a field from the matrix table which is not needed in the final written out version\n",
    "mt_vcf = mt_vcf.drop('gvcf_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-louisville",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 13:56:41 Hail: WARN: export_vcf: ignored the following fields:\n",
      "    'project_meta' (column)\n",
      "    'subsets' (column)\n",
      "    'bam_metrics' (column)\n",
      "    'sex_imputation' (column)\n",
      "    'sample_qc' (column)\n",
      "    'population_inference' (column)\n",
      "    'sample_filters' (column)\n",
      "    'relatedness_inference' (column)\n",
      "    'high_quality' (column)\n",
      "    'release' (column)\n",
      "    'hgdp_tgp_meta' (column)\n",
      "    'bergstrom' (column)\n",
      "    'a_index' (row)\n",
      "    'was_split' (row)\n",
      "    'freq' (row)\n",
      "    'raw_qual_hists' (row)\n",
      "    'popmax' (row)\n",
      "    'qual_hists' (row)\n",
      "    'faf' (row)\n",
      "    'vep' (row)\n",
      "    'vqsr' (row)\n",
      "    'region_flag' (row)\n",
      "    'allele_info' (row)\n",
      "    'age_hist_het' (row)\n",
      "    'age_hist_hom' (row)\n",
      "    'cadd' (row)\n",
      "    'revel' (row)\n",
      "    'splice_ai' (row)\n",
      "    'primate_ai' (row)\n",
      "2021-08-06 13:56:44 Hail: WARN: export_vcf found row field rsid with type 'set<str>', but expected type str. Emitting missing ID.\n"
     ]
    }
   ],
   "source": [
    "# Writing out the dense, postQC dataset in vcf format\n",
    "hl.export_vcf(mt_vcf, 'gs://african-seq-data/hgdp_tgp/hgdp_tgp_postqc.vcf.bgz', parallel='separate_header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olympic-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 21:19:22 Hail: INFO: wrote matrix table with 155648020 rows and 4097 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/new_hgdp_tgp_postQC.mt\n",
      "    Total size: 3.09 TiB\n",
      "    * Rows/entries: 3.09 TiB\n",
      "    * Columns: 1.69 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (20.00 B)\n",
      "    * Largest partition:  96270 rows (2.23 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out the postQC dataset with PCA sample outliers removed and subset to PASS variants\n",
    "mt.write('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/new_hgdp_tgp_postQC.mt', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

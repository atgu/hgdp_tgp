{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "great-jenny",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Functions](#Functions)\n",
    "- [Reading in datasets](#Reading-in-datasets)\n",
    "- [Combining metadata](#Combining-metadata)\n",
    "- [Annotating metadata onto matrix table](#Annotating-merged-metadata-onto-matrix-table)\n",
    "- [Sample QC filtering](#Sample-QC-filtering)\n",
    "- [PCA outlier removal](#PCA-outlier-removal)\n",
    "- [Variant QC filtering](#Variant-QC-filtering)\n",
    "- [Exporting datasets post QC](#Exporting-final-dataset-post-QC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-football",
   "metadata": {},
   "source": [
    "### The purpose of this script is to merge metadata components needed for the HGDP+1kGP dataset and then run  QC filters on that resulting dataset. The QC filters were run using sample/variant information from the metadata datasets. \n",
    "\n",
    "**This script contains information on how to**: \n",
    "- annotate metadata onto a matrix table\n",
    "- combine multiple tables and matrix tables\n",
    "- harmonize datasets to prevent merge conflicts\n",
    "- filter matrix tables using a field within the matrix table\n",
    "- filter samples using a hardcoded list of samples to remove\n",
    "- write out a matrix table in vcf format\n",
    "\n",
    "**Datasets merged are**: \n",
    "    - sample metadata table which contains harmonized metadata for the HGDP_1kGP dataset (sample_meta)\n",
    "    - gnomad v3.1 sample qc metadata from for the hgdp_1kg subset which contains flags to denote which samples failed gnomAD QC(sample_qc_meta)\n",
    "    - gnomad v3.1 variant metadata information which contains flags to denote which variants passed gnomAD QC (var_meta)\n",
    "    - densified hgdp_1kg matrix table (dense_mt)   \n",
    "    \n",
    "**Author: Zan Koenig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apart-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.5\n",
      "SparkUI available at http://znk-plink-m.c.diverse-pop-seq-ref.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.64-1ef70187dc78\n",
      "LOGGING: writing to /home/hail/hail-20211202-1706-0.2.64-1ef70187dc78.log\n"
     ]
    }
   ],
   "source": [
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-dispatch",
   "metadata": {},
   "source": [
    "# Functions\n",
    "For interactive analyses scripts such as this, defining functions at the top of the script allow for ease of use. There are other ways to organize the definition of functions in python and the best method depends on the intended usage of the script as well as the writers personal preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of dicts and converts it to a struct format (works with nested structs too)\n",
    "def dict_to_struct(d):\n",
    "    fields = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = dict_to_struct(v)\n",
    "        fields[k] = v\n",
    "    return hl.struct(**fields)\n",
    "\n",
    "# Formats the output of using hl.count in a more user-friendly format\n",
    "def print_count(mt):\n",
    "    '''\n",
    "    Prints out total sample/variant count for a mt\n",
    "    :param mt: hail matrix table\n",
    "    :return: print statement with number of samples and variants\n",
    "    '''\n",
    "    # Since hl.count() is being used on a matrix table, the result has two numbers in output\n",
    "    # When using hl.count() on a matrix table the first number is the number of rows, equivalent to the number of variants\n",
    "    # The second number is the number of columns, equivalent to the number of samples\n",
    "    n = mt.count()\n",
    "    print('Number of Samples: {}\\nNumber of Variants: {}'.format(n[1], n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-plane",
   "metadata": {},
   "source": [
    "# Reading in datasets\n",
    "Setting separate variables for paths before the datasets are read in makes it easier to update paths if datasets move in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "skilled-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for sample metadata file which contains metadata for the HDGP dataset\n",
    "sample_meta_path = 'gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/gnomad_meta_v1.tsv'\n",
    "\n",
    "# path for hail table which contains information on which samples passed gnomAD QC filters\n",
    "sample_qc_meta_path = 'gs://hgdp_tgp/output/gnomad_v3.1_sample_qc_metadata_hgdp_tgp_subset.ht'\n",
    "\n",
    "# path for table of the most recent gnomAD release which contains information on which variants passed gnomAD QC filters\n",
    "var_meta_path = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'\n",
    "\n",
    "# path for densfied pre-qc matrix table which was generated from the original pre-qc sparse version of the hgdp_1kgp dataset\n",
    "dense_mt_path = 'gs://hgdp_tgp/output/tgp_hgdp.mt'\n",
    "\n",
    "# Path for a txt file which contains the latest list of PCA outliers\n",
    "pca_outlier_path = 'gs://hgdp-1kg/hgdp_tgp/pca_outliers_v2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:02:28 Hail: INFO: Reading table to impute column types\n",
      "2021-11-02 17:02:35 Hail: INFO: Loading 184 fields. Counts by type:\n",
      "  str: 80\n",
      "  bool: 44\n",
      "  float64: 40\n",
      "  int32: 20\n"
     ]
    }
   ],
   "source": [
    "# reading in Alicia's sample metadata file (Note: this file uses the 'v3.1::' prefix as done in gnomAD)\n",
    "sample_meta = hl.import_table(sample_meta_path, impute=True)\n",
    "\n",
    "# reading in Julia's sample metadata file\n",
    "sample_qc_meta = hl.read_table(sample_qc_meta_path)\n",
    "\n",
    "# reading in variant qc information\n",
    "var_meta = hl.read_table(var_meta_path)\n",
    "\n",
    "# reading in densified pre-qc matrix table\n",
    "dense_mt = hl.read_matrix_table(dense_mt_path)\n",
    "\n",
    "# To read in the PCA outlier list, first need to read the file in as a list\n",
    "# using hl.hadoop_open here which allows one to read in files into hail from Google cloud storage\n",
    "with hl.utils.hadoop_open('gs://african-seq-data/hgdp_tgp/pca_outliers_v2.txt') as file:\n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# Using hl.literal here to convert the list from a python object to a hail expression so that it can be used to filter out samples\n",
    "outliers_list = hl.literal(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-output",
   "metadata": {},
   "source": [
    "# Combining metadata\n",
    "Before conducting QC, the different metadata datasets must be merged together. The first cell below is an example of having to alter the structure of a dataset before being able to merge with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bits below were written by Tim Poterba to help troubleshoot unflattening a ht with nested structure\n",
    "# dict to hold struct names as well as nested field names\n",
    "d = {}\n",
    "\n",
    "# Getting just the row field names \n",
    "row = sample_meta.row_value\n",
    "\n",
    "# returns a dict with the struct names as keys and their inner field names as values\n",
    "for name in row:\n",
    "    def recur(dict_ref, split_name):\n",
    "        if len(split_name) == 1:\n",
    "            dict_ref[split_name[0]] = row[name]\n",
    "            return\n",
    "        existing = dict_ref.get(split_name[0])\n",
    "        if existing is not None:\n",
    "            assert isinstance(existing, dict), existing  # fails on foo.bar and foo.bar.baz\n",
    "            recur(existing, split_name[1:])\n",
    "        else:\n",
    "            existing = {}\n",
    "            dict_ref[split_name[0]] = existing\n",
    "            recur(existing, split_name[1:])\n",
    "    recur(d, name.split('.'))\n",
    "\n",
    "\n",
    "# using the dict created from flattened struct, creating new structs now unflattened\n",
    "sample_meta = sample_meta.select(**dict_to_struct(d))\n",
    "sample_meta = sample_meta.key_by('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "formed-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the columns needed from Alicia's metadata\n",
    "new_meta = sample_meta.select(sample_meta.hgdp_tgp_meta, sample_meta.bergstrom)\n",
    "\n",
    "# creating a table with Julia's metadata and Alicia's metadata\n",
    "ht = sample_qc_meta.annotate(**new_meta[sample_qc_meta.s])\n",
    "\n",
    "# stripping 'v3.1::' from the names to match with Konrad's MT\n",
    "ht = ht.key_by(s=ht.s.replace(\"v3.1::\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 18:31:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:31:55 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-06-22 18:32:25 Hail: INFO: wrote table with 4150 rows in 155 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_sample_metadata.ht\n",
      "    Total size: 1.69 MiB\n",
      "    * Rows: 1.68 MiB\n",
      "    * Globals: 6.51 KiB\n",
      "    * Smallest partition: 1 rows (758.00 B)\n",
      "    * Largest partition:  173 rows (68.18 KiB)\n"
     ]
    }
   ],
   "source": [
    "# When writing out any dataset, you want to make sure the path is as intended and the resulting name is descriptive\n",
    "# hl.write() takes the entire output path as an argument as well as the name of the resulting table or matrix table\n",
    "ht.write('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-division",
   "metadata": {},
   "source": [
    "# Annotating merged metadata onto matrix table\n",
    "Now that the two metadata datasets are merged together and in the proper format, the next step is to annotate the dense matrix table with all of the samples and variants preQC with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in table annotated with Alicia and Julia's respective metadata\n",
    "ht = hl.read_table('gs://hgdp-1kg/hgdp_tgp/hgdp_tgp_sample_metadata.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "center-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hl.count() returns the counts of samples and variants within a matrix table or table.\n",
    "# In this case since it is a hail table, it only returns the count of the number of samples\n",
    "# The number of samples is equal to the number of rows\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "million-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'sex_imputation_ploidy_cutoffs': struct {\n",
      "        x_ploidy_cutoffs: struct {\n",
      "            upper_cutoff_X: float64, \n",
      "            lower_cutoff_XX: float64, \n",
      "            upper_cutoff_XX: float64, \n",
      "            lower_cutoff_XXX: float64\n",
      "        }, \n",
      "        y_ploidy_cutoffs: struct {\n",
      "            lower_cutoff_Y: float64, \n",
      "            upper_cutoff_Y: float64, \n",
      "            lower_cutoff_YY: float64\n",
      "        }, \n",
      "        f_stat_cutoff: float64\n",
      "    } \n",
      "    'population_inference_pca_metrics': struct {\n",
      "        min_prob: float64, \n",
      "        include_unreleasable_samples: bool, \n",
      "        max_mislabeled_training_samples: int32, \n",
      "        known_pop_removal_iterations: int32, \n",
      "        n_pcs: int32\n",
      "    } \n",
      "    'relatedness_inference_cutoffs': struct {\n",
      "        min_individual_maf: float64, \n",
      "        min_emission_kinship: float64, \n",
      "        ibd0_0_max: float64, \n",
      "        second_degree_kin_cutoff: float64, \n",
      "        first_degree_kin_thresholds: tuple (\n",
      "            float64, \n",
      "            float64\n",
      "        )\n",
      "    } \n",
      "    'outlier_detection_metrics': struct {\n",
      "        lms: struct {\n",
      "            n_snp: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_singleton: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_ti_tv: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_insertion_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_insertion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_deletion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            r_het_hom_var: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transition: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }, \n",
      "            n_transversion: struct {\n",
      "                beta: array<float64>, \n",
      "                standard_error: array<float64>, \n",
      "                t_stat: array<float64>, \n",
      "                p_value: array<float64>, \n",
      "                multiple_standard_error: float64, \n",
      "                multiple_r_squared: float64, \n",
      "                adjusted_r_squared: float64, \n",
      "                f_stat: float64, \n",
      "                multiple_p_value: float64, \n",
      "                n: int32\n",
      "            }\n",
      "        }, \n",
      "        qc_metrics_stats: struct {\n",
      "            n_snp_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_singleton_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_ti_tv_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_insertion_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_insertion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_deletion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            r_het_hom_var_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transition_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }, \n",
      "            n_transversion_residual: struct {\n",
      "                median: float64, \n",
      "                mad: float64, \n",
      "                lower: float64, \n",
      "                upper: float64\n",
      "            }\n",
      "        }, \n",
      "        n_pcs: int32, \n",
      "        used_regressed_metrics: bool\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    'project_meta': struct {\n",
      "        sample_id: str, \n",
      "        research_project_key: str, \n",
      "        seq_project: str, \n",
      "        ccdg_alternate_sample_id: str, \n",
      "        ccdg_gender: str, \n",
      "        ccdg_center: str, \n",
      "        ccdg_study: str, \n",
      "        cram_path: str, \n",
      "        project_id: str, \n",
      "        v2_age: float64, \n",
      "        v2_sex: str, \n",
      "        v2_hard_filters: str, \n",
      "        v2_perm_filters: str, \n",
      "        v2_pop_platform_filters: str, \n",
      "        v2_related: bool, \n",
      "        v2_data_type: str, \n",
      "        v2_product: str, \n",
      "        v2_product_simplified: str, \n",
      "        v2_qc_platform: str, \n",
      "        v2_project_id: str, \n",
      "        v2_project_description: str, \n",
      "        v2_internal: bool, \n",
      "        v2_investigator: str, \n",
      "        v2_known_pop: str, \n",
      "        v2_known_subpop: str, \n",
      "        v2_pop: str, \n",
      "        v2_subpop: str, \n",
      "        v2_neuro: bool, \n",
      "        v2_control: bool, \n",
      "        v2_topmed: bool, \n",
      "        v2_high_quality: bool, \n",
      "        v2_release: bool, \n",
      "        v2_pcr_free: bool, \n",
      "        v2_project_name: str, \n",
      "        v2_release_2_0_2: bool, \n",
      "        project_ancestry: str, \n",
      "        project_pop: str, \n",
      "        project_subpop: str, \n",
      "        pdo_owner: str, \n",
      "        category: str, \n",
      "        contact_pi: str, \n",
      "        sample_pi: str, \n",
      "        pm: str, \n",
      "        research_project: str, \n",
      "        pdo: str, \n",
      "        title: str, \n",
      "        product: str, \n",
      "        probably_releasable: str, \n",
      "        releasable: bool, \n",
      "        broad_external: str, \n",
      "        sex: str, \n",
      "        subpop_description: str, \n",
      "        exclude: bool, \n",
      "        exclude_reason: str, \n",
      "        case_control: str, \n",
      "        age: int32, \n",
      "        age_bin: str, \n",
      "        tcga_tumor: bool, \n",
      "        age_alt: int32, \n",
      "        v2_s_match: str, \n",
      "        topmed: bool, \n",
      "        neuro_cohort: bool, \n",
      "        neuro_case: bool\n",
      "    } \n",
      "    'subsets': struct {\n",
      "        non_topmed: bool, \n",
      "        controls_and_biobanks: bool, \n",
      "        non_neuro: bool, \n",
      "        non_v2: bool, \n",
      "        non_cancer: bool, \n",
      "        tgp: bool, \n",
      "        hgdp: bool\n",
      "    } \n",
      "    'bam_metrics': struct {\n",
      "        pct_bases_20x: float64, \n",
      "        pct_chimeras: float64, \n",
      "        freemix: float64, \n",
      "        mean_coverage: float64, \n",
      "        median_coverage: float64, \n",
      "        mean_insert_size: float64, \n",
      "        median_insert_size: float64, \n",
      "        pct_bases_10x: float64\n",
      "    } \n",
      "    'sex_imputation': struct {\n",
      "        is_female: bool, \n",
      "        chr20_mean_dp: float32, \n",
      "        chrX_mean_dp: float32, \n",
      "        chrY_mean_dp: float32, \n",
      "        chrX_ploidy: float32, \n",
      "        chrY_ploidy: float32, \n",
      "        X_karyotype: str, \n",
      "        Y_karyotype: str, \n",
      "        sex_karyotype: str, \n",
      "        impute_sex_stats: struct {\n",
      "            f_stat: float64, \n",
      "            n_called: int64, \n",
      "            expected_homs: float64, \n",
      "            observed_homs: int64\n",
      "        }\n",
      "    } \n",
      "    'sample_qc': struct {\n",
      "        n_hom_ref: int64, \n",
      "        n_het: int64, \n",
      "        n_hom_var: int64, \n",
      "        n_non_ref: int64, \n",
      "        n_singleton: int64, \n",
      "        n_snp: int64, \n",
      "        n_insertion: int64, \n",
      "        n_deletion: int64, \n",
      "        n_transition: int64, \n",
      "        n_transversion: int64, \n",
      "        n_star: int64, \n",
      "        r_ti_tv: float64, \n",
      "        r_het_hom_var: float64, \n",
      "        r_insertion_deletion: float64, \n",
      "        n_snp_residual: float64, \n",
      "        n_singleton_residual: float64, \n",
      "        r_ti_tv_residual: float64, \n",
      "        r_insertion_deletion_residual: float64, \n",
      "        n_insertion_residual: float64, \n",
      "        n_deletion_residual: float64, \n",
      "        r_het_hom_var_residual: float64, \n",
      "        n_transition_residual: float64, \n",
      "        n_transversion_residual: float64\n",
      "    } \n",
      "    'population_inference': struct {\n",
      "        training_pop: str, \n",
      "        pca_scores: array<float64>, \n",
      "        pop: str, \n",
      "        prob_afr: float64, \n",
      "        prob_ami: float64, \n",
      "        prob_amr: float64, \n",
      "        prob_asj: float64, \n",
      "        prob_eas: float64, \n",
      "        prob_fin: float64, \n",
      "        prob_mid: float64, \n",
      "        prob_nfe: float64, \n",
      "        prob_oth: float64, \n",
      "        prob_sas: float64, \n",
      "        training_pop_all: str\n",
      "    } \n",
      "    'sample_filters': struct {\n",
      "        sex_aneuploidy: bool, \n",
      "        insert_size: bool, \n",
      "        chimera: bool, \n",
      "        contamination: bool, \n",
      "        bad_qc_metrics: bool, \n",
      "        low_coverage: bool, \n",
      "        ambiguous_sex: bool, \n",
      "        failed_fingerprinting: bool, \n",
      "        TCGA_tumor_sample: bool, \n",
      "        hard_filters: set<str>, \n",
      "        hard_filtered: bool, \n",
      "        release_related: bool, \n",
      "        release_duplicate: bool, \n",
      "        release_parent_child: bool, \n",
      "        release_sibling: bool, \n",
      "        all_samples_related: bool, \n",
      "        all_samples_duplicate: bool, \n",
      "        all_samples_parent_child: bool, \n",
      "        all_samples_sibling: bool, \n",
      "        fail_n_snp_residual: bool, \n",
      "        fail_n_singleton_residual: bool, \n",
      "        fail_r_ti_tv_residual: bool, \n",
      "        fail_r_insertion_deletion_residual: bool, \n",
      "        fail_n_insertion_residual: bool, \n",
      "        fail_n_deletion_residual: bool, \n",
      "        fail_r_het_hom_var_residual: bool, \n",
      "        fail_n_transition_residual: bool, \n",
      "        fail_n_transversion_residual: bool, \n",
      "        qc_metrics_filters: set<str>\n",
      "    } \n",
      "    'relatedness_inference': struct {\n",
      "        relationships: set<str>\n",
      "    } \n",
      "    'high_quality': bool \n",
      "    'release': bool \n",
      "    'hgdp_tgp_meta': struct {\n",
      "        Project: str, \n",
      "        Study: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Population: str, \n",
      "        Genetic: struct {\n",
      "            region: str\n",
      "        }, \n",
      "        Latitude: float64, \n",
      "        Longitude: float64, \n",
      "        Continent: struct {\n",
      "            colors: str\n",
      "        }, \n",
      "        n: int32, \n",
      "        rownum: int32, \n",
      "        Pop: struct {\n",
      "            colors: str, \n",
      "            shapes: int32\n",
      "        }\n",
      "    } \n",
      "    'bergstrom': struct {\n",
      "        hgdp: str, \n",
      "        lp: str, \n",
      "        source: str, \n",
      "        library_type: str, \n",
      "        region: str, \n",
      "        sex: str, \n",
      "        coverage: float64, \n",
      "        freemix: float64, \n",
      "        capmq: int32, \n",
      "        insert_size_average: float64, \n",
      "        array_non_reference_discordance: float64, \n",
      "        sample: str\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['s']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# hl.describe() gives you an overview of all the fields in a matrix table or table\n",
    "ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36669a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hl.annotate_cols() method to annotate the metadata onto the matrix table\n",
    "# Using hl.annotate_cols() in this way is essentially merging dense_mt with ht\n",
    "# In order for this hl.annotate_cols() to work, both of the datasets to merge need to share the same key\n",
    "# In this case that key is 's'\n",
    "# When using hl.annotate_cols() the table is being indexed by the equivalent key in  the dense_mt\n",
    "mt = dense_mt.annotate_cols(**ht[dense_mt.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Harmonizing the geographic region labels in the merged dataset\n",
    "# First step is to create a dictionary which contains the current region name as keys and the desired region name\n",
    "# as values.\n",
    "region_dict = {'Africa': 'AFR', 'AFR':'AFR', 'America': 'AMR', 'AMR': 'AMR',\n",
    "               'Central_South_Asia': 'CSA', 'SAS':'CSA', 'East_Asia': 'EAS',\n",
    "               'EAS': 'EAS', 'Europe': 'EUR', 'EUR':'EUR','Middle_East': 'MID', 'Oceania': 'OCE'}\n",
    "\n",
    "# Next step is to convert the type of the dictionary from a python dict to a hail dict\n",
    "region_dict=hl.literal(region_dict)\n",
    "\n",
    "# Final step is to annotate a new field onto the matrix table using the dictionary to map the current region labels\n",
    "# to the new harmonized region labels\n",
    "mt = mt.annotate_cols(study_geo_region = region_dict[mt.hgdp_tgp_meta.Study.region])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "british-visibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4151)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_count(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "capital-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-08 15:38:02 Hail: INFO: wrote matrix table with 211358784 rows and 4151 columns in 5000 partitions to gs://african-seq-data/hgdp_tgp/hgdp_tgp_dense_meta_preQC.mt\n",
      "    Total size: 3.32 TiB\n",
      "    * Rows/entries: 3.32 TiB\n",
      "    * Columns: 1.71 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (32.13 MiB)\n",
      "    * Largest partition:  183321 rows (4.39 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out a pre-qc version of the dataset for Mary's PCA analyses\n",
    "### FIX: NEED TO UPDATE THIS FILE WITH THE NEWLY FIXED GLOBAL REGION LABELS\n",
    "mt.write(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_preQC.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-beatles",
   "metadata": {},
   "source": [
    "# Sample QC filtering\n",
    "As previously mentioned, sample QC filtering for this dataset was conducted using metadata which was annotated onto the main matrix table. Sample QC was run using gnomAD's QC pipeline and the fields used to filter below contain information on whether samples passed or failed gnomAD QC.\n",
    "[INSERT DETAILS ON GNOMAD QC STEPS WHICH WERE CONDUCED]\n",
    "\n",
    "For more information on how sample QC filters were developed see [INSERT ADDITIONAL TUTORIAL NAME HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rational-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the preQC dataset \n",
    "# This is a merged version of the metadata from different sources and the sample/variant dense dataset\n",
    "mt = hl.read_matrix_table(\"gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_preQC.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8769dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a preliminary count before filtering on the dataset\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering samples to those who should pass QC\n",
    "# this filters to only samples that passed gnomad sample QC hard filters\n",
    "mt_filt = mt.filter_cols(~mt.sample_filters.hard_filtered)\n",
    "\n",
    "# annotating partially filtered dataset with variant metadata\n",
    "mt_filt = mt_filt.annotate_rows(**var_meta[mt_filt.locus, mt_filt.alleles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the counts of samples/filters after filtering to those who passed sample QC\n",
    "mt_filt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlikely-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 19:18:12 Hail: INFO: wrote matrix table with 211358784 rows and 4120 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt\n",
      "    Total size: 3.82 TiB\n",
      "    * Rows/entries: 3.82 TiB\n",
      "    * Columns: 1.70 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 10589 rows (38.69 MiB)\n",
      "    * Largest partition:  183321 rows (4.82 GiB)\n"
     ]
    }
   ],
   "source": [
    "mt_filt.write('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-glance",
   "metadata": {},
   "source": [
    "# PCA outlier removal\n",
    "For information on how PCA outliers were found see [INSERT TUTORIAL NAME HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "persistent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the annotated & partially filtered dataset\n",
    "mt = hl.read_matrix_table('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/hgdp_tgp_dense_meta_filt.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecological-bundle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sample and variant count before removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the list of PCA outliers, using the ~ operator which is a negation operator and obtains the compliment\n",
    "# In this case the compliment is samples which are not contained in the pca outlier list\n",
    "mt = mt.filter_cols(~outliers_list.contains(mt['s']))\n",
    "# Removing any duplicates in the dataset using hl.distinct_by_col() which removes columns with a duplicate column key. It keeps one column for each unique key.\n",
    "mt = mt.distinct_by_col()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "technological-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211358784, 4097)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a count of samples/variants after removing PCA outliers\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-funeral",
   "metadata": {},
   "source": [
    "# Variant QC filtering\n",
    "[INSERT INFORMATION ON DETAILS OF GNOMAD VARIANT QC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "disabled-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the variants in the dataset to only PASS variants (those which passed variant QC)\n",
    "# PASS variants are variants which have an entry in the filters field. This field contains an array which contains a bool if any variant qc filter was failed\n",
    "# This is the last step in the QC process\n",
    "mt = mt.filter_rows(hl.len(mt.filters) !=0  ,keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "honest-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155648020, 4097)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the final count of the dataset before writing out the dataset to different formats\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-progress",
   "metadata": {},
   "source": [
    "# Exporting final dataset post QC\n",
    "In order to write out matrix tables in hail, you use the mt.write() method. As a string inside that method you put the path where you want your matrix table to be written out to. Keep in mind a matrix table is a directory format and is a large size. Writing out this dataset will take some time until complete. On your Google cloud cluster, you can switch to worker nodes instead of secondary worker nodes in order to shorten the time it takes to write out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olympic-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 21:19:22 Hail: INFO: wrote matrix table with 155648020 rows and 4097 columns in 5000 partitions to gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/new_hgdp_tgp_postQC.mt\n",
      "    Total size: 3.09 TiB\n",
      "    * Rows/entries: 3.09 TiB\n",
      "    * Columns: 1.69 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (20.00 B)\n",
      "    * Largest partition:  96270 rows (2.23 GiB)\n"
     ]
    }
   ],
   "source": [
    "# writing out the postQC dataset with PCA sample outliers removed and subset to PASS variants\n",
    "mt.write('gs://hgdp-1kg/hgdp_tgp/qc_and_figure_generation/new_hgdp_tgp_postQC.mt', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}